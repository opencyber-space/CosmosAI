This FAQ document provides guidance to Large Language Models (LLMs) on resolving conflicts and ambiguities during ScaleLayout analysis and resource estimation.  It addresses scenarios like missing data (advising against hallucination and suggesting alternatives), FPS determination (prioritizing ScaleLayout's `UsecaseVsNumberOfInputFrames.md`), resource contention (recommending splitting workloads or scaling nodes), fluctuating resource usage (using expected, average, or 75th percentile values), conflicting CPU/memory needs (prioritizing the more restrictive resource), affinity/anti-affinity rules (prioritizing anti-affinity), missing pod metrics (using interpolation or conservative estimates), unspecified event rates (using default values based on historical data), and conflicting user constraints (explaining the conflict and offering the best possible solution).  The overall goal is to enable the LLM to make robust, context-aware decisions and improve retrieval accuracy.