{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "06ff3d90",
   "metadata": {},
   "source": [
    "# AutoAI: Expert system for Automated AI design & scaling - powered by knowledge graphs and RAG.\n",
    "\n",
    "- **Author: Shridhar Kini** ([Profile](https://www.linkedin.com/in/shridhar-kini-79911249?utm_source=share&utm_campaign=share_via&utm_content=profile&utm_medium=android_app))\n",
    "- **Note**: This notebook is designed to run in a Jupyter environment with access to the necessary libraries of AIOS v1 like `aios_instance`, `aios_transformers` and `Weaviate vector database`.\n",
    "- **To Securely Run**: `jupyter notebook password` to generate onetime password for secure access\n",
    "- **To Run**: `jupyter notebook --allow-root  --port 9999 --ip=0.0.0.0`\n",
    "- **To Clear Outputs**: Use `jupyter nbconvert --clear-output --inplace RAG_ScaleLayout_AppLayout_Demo.ipynb`\n",
    "\n",
    "\n",
    "This notebook demonstrates how **Retrieval-Augmented Generation (RAG)** is used to power two critical components of our AI vision pipeline system:\n",
    "\n",
    "---\n",
    "\n",
    "## üé≠ **Introduction to AppLayout and ScaleLayout**\n",
    "\n",
    "---\n",
    "\n",
    "### üèóÔ∏è **AppLayout: The Logical Blueprint of ensemble AI** üí°\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```\n",
    "üéØ VISION ‚Üí üß© DECOMPOSITION ‚Üí üìê BLUEPRINT ‚Üí ‚öôÔ∏è EXECUTION\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**AppLayout** is the **logical blueprint** and **architectural DNA** of an ensemble AI and AI application. It's the mastermind that meticulously deconstructs complex, high level AI use cases into composable AI workflows of smaller, interconnected, and reusable AI blocks. To know more about AppLayout, visit ([AppLayout](https://aiosdocs.pages.dev/getting-started/concepts/#app-layout))\n",
    "\n",
    "#### üç≥ **Think of it as a Master Chef's Recipe:**\n",
    "- üìù Each **unit/block** = Individual intelligent or non intelligent module (AI model, algorithm, policy, code block)\n",
    "- üîó **DAG Structure** = Compositon recipe of an ensemble - can be used to create permutation and combination of AIs \n",
    "- ‚ö° **Flow** = sequence or order of execution - sequential or async or conditional or cyclic or acyclic etc.\n",
    "- üéì **Domain Expertise** = Years of expertise & experience distilled into automation - a miniature AI twin of an AI expert.\n",
    "\n",
    "> üí´ *\"From high-level specifications to automated AI magic!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ **ScaleLayout: The Strategic Deployment Commander** ‚öñÔ∏è\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "```\n",
    "üèóÔ∏è BLUEPRINT ‚Üí üåê INFRASTRUCTURE ‚Üí üéØ OPTIMIZATION ‚Üí üìà DEPLOYMENT\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "\n",
    "**ScaleLayout** is the **intelligent deployment strategist** and **operational mastermind** for AppLayout execution. If AppLayout answers *\"what and how\"*, ScaleLayout conquers *\"where and how efficiently\"*. To know more about AppLayout, visit ([ScaleLayout](https://aiosdocs.pages.dev/getting-started/concepts/#scale-layout))\n",
    "\n",
    "#### üéØ **The Master Deployment Plan:**\n",
    "- üó∫Ô∏è **Strategic Mapping** = Map of Processing units to Hardware infrastructure\n",
    "- ‚ö° **Performance Goals** = Minimize network traffic, eliminate bottlenecks\n",
    "- üí∞ **Resource Efficiency** = Zero CPU/GPU cycles or memory bytes wasted\n",
    "- ü§ñ **Smart Scheduling** = efficient deployment\n",
    "\n",
    "> üåü *\"Convert AI designs into real-world scale, high-performing reality!\"*\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ù **The Perfect Partnership**\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| üèóÔ∏è **AppLayout** | ‚öñÔ∏è **ScaleLayout** |\n",
    "|:---:|:---:|\n",
    "| üß† **WHAT** to process | üåê **WHERE** to deploy |\n",
    "| ‚öôÔ∏è **HOW** to structure | üìà **HOW** to optimize |\n",
    "| üìê Logical Architecture | üöÄ Physical Execution |\n",
    "| üéØ Functional Design | ‚ö° Performance Reality |\n",
    "\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Scope of AppLayout and ScaleLayout Automation\n",
    "- Traditionally, the creation higher level AI or ensemble AI meant handcrafting the ensembles and large high skills teams to meticulously plan, operate and mange deployments - a completely manual process - that doesnt scale. AppLayouts and ScaleLayouts allowed automation of most parts of the process - making it scalable and efficient. What about automating the app layout and scale layout itself 100%, that puts us in the territory of skynet - an AI that can compose itself and scale and adapt infintely to available resources. \n",
    "- Without autoAI \n",
    "  - It would require a domain expert to first architect the application's logical blueprint and then devise an end to end optimal deployment strategy - simpler for small scale and beyond complexity for large to planetary scale. \n",
    "  - These plans would then be handed off to the AIOS for execution. \n",
    "- In this demonstration, we unveil a revolutionary shift. \n",
    "  - We will showcase how an expert system powered by Retrieval-Augmented Generation (RAG) system can completely automate this workflow, dynamically generating and deploying the entire application structure in direct response to a user's query. \n",
    "\n",
    "### **AppLayout**: DAG Structure Creation for Use Cases\n",
    "- **Purpose**: Creates Directed Acyclic Graph (DAG) structures for user-queried use cases. Applayout automatically designs processing pipelines based on requirements with the help of knowledge from our Deployment Knowledge Base.\n",
    "- **Function**: Automatically designs processing pipelines based on requirements\n",
    "- **RAG Role**: Retrieves relevant pipeline configurations, block specifications, and architectural patterns\n",
    "\n",
    "### **ScaleLayout**: Deployment Planning & Hardware Optimization\n",
    "- **Purpose**: Plans deployment across servers with intelligent hardware allocation\n",
    "- **Function**: Optimizes resource distribution, handles multi-camera scenarios\n",
    "- **RAG Role**: Retrieves deployment patterns, resource requirements, and optimization strategies\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ How RAG Enhances Both Systems\n",
    "\n",
    "When multiple cameras are provided simultaneously, our RAG system enables:\n",
    "- **Unified Planning**: All use case data available at once for optimal resource allocation\n",
    "- **Context-Aware Decisions**: Historical deployment patterns inform current planning\n",
    "- **Intelligent Optimization**: Cross-camera resource sharing and load balancing\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf937a",
   "metadata": {},
   "source": [
    "## üìö Knowledge Base Structure\n",
    "\n",
    "Our RAG system indexes various types of documentation:\n",
    "\n",
    "### AppLayout Specific Knowledge:\n",
    "- **üìã Model Cards**: AI model specifications and requirements\n",
    "- **üîß Policy Cards**: Deployment and resource management policies\n",
    "- **üìä Parameter Tuning**: Optimization guidelines and best practices\n",
    "- **üèóÔ∏è Pipeline Compositions**: Pre-built pipeline architectures\n",
    "- **üíæ Input/Output Format Cards**: IO Data format specifications\n",
    "- **üéØ Use Case Cards**: Specific application scenarios and configurations\n",
    "- **üîó Chaining Blueprints**: Inter-block communication patterns\n",
    "\n",
    "### ScaleLayout Specific Knowledge:\n",
    "- **üíª Hardware specifications and limits**\n",
    "- **üéØ Resource allocation strategies**\n",
    "- **üìπ Multi-camera deployment patterns**\n",
    "- **‚ö° Performance optimization techniques**\n",
    "- **üìä Streaming resource estimation**\n",
    "- **üîß Resource estimation for each component in pipeline deployment**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2b485f",
   "metadata": {},
   "source": [
    "##  Prerequisites\n",
    "\n",
    "Build the docker image for the RAG system with all necessary components:\n",
    "- **Note**: Run the below two scripts outside of the notebook environment to ensure proper execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a576ea",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "%%bash\n",
    "# Install required packages\n",
    "pip install weaviate-client llama-index llama-index-embeddings-openai llama-index-llms-openai google-generativeai\n",
    "\n",
    "echo \"‚úÖ Packages installed successfully!\"\n",
    "```\n",
    "\n",
    "```bash\n",
    "docker build -t rag-scale-layout-app-layout .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b632e620",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Setting Up the RAG Indexer docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edbf56f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "```bash\n",
    "CUR_DIR=$(dirname \"$(realpath \"$0\")\")\n",
    "\n",
    "dockertransformerimagename=\"rag-scale-layout-app-layout\"\n",
    "container_name=\"rag_weaviate_test_container\"\n",
    "\n",
    "\n",
    "docker run -d\\\n",
    " --network=host \\\n",
    " -v /home/ubuntu/models:/home/ubuntu/models \\\n",
    " -v $CUR_DIR:$CUR_DIR \\\n",
    " --gpus='\"device=3\"' \\\n",
    " --env=\"BLOCK_ID=hello-001\" \\\n",
    " --env=\"BLOCKS_DB_URI=http://MANAGEMENTMASTER:30100\" \\\n",
    " --name=$container_name \\\n",
    " --entrypoint /bin/bash \\\n",
    "  $dockertransformerimagename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd93143",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üõ†Ô∏è Setting Up the RAG Indexer\n",
    "\n",
    "Let's start by importing the necessary components and setting up our test environment.\n",
    "### **Process Flow**:\n",
    "1. Setup Weaviate vector database for storing indexed data\n",
    "2. Phase-1: Index the knowledge base with all relevant documentation of AppLayout and ScaleLayout.\n",
    "3. Initialize Indexing with proper settings and configurations like embedding models, chunks size, chunk overlap, embedding dimensions, similarity threshold etc.\n",
    "4. Start the indexing process to populate the Weaviate database with the knowledge base content.\n",
    "5. Phase-2: Retrieve the indexed data using RAG to power AppLayout and ScaleLayout systems by providing suitable queries as chat messages.\n",
    "   - Retrieve relevant pipeline configurations, block specifications, and architectural patterns for AppLayout first\n",
    "    - With the retrieved pipeline for AppLayout, retrieve the deployment planning of cameras vs usecase matrix for ScaleLayout iteratively if not satisfied with LLM response at any point.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234234e",
   "metadata": {},
   "source": [
    "## Block Diagram\n",
    "![RAG System Block Diagram](RAG_system_overview_s_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef8fb8d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚öôÔ∏è Setup Weaviate\n",
    "We will use Weaviate as our vector database to store and retrieve the indexed knowledge base content. \n",
    "- To Set up the Weaviate vector database, we will use the docker-compose file provided in the `weaveateSetup` directory.\n",
    "- Run the script `create_folders.sh` to create the necessary directories for Weaviate setup.\n",
    "- Run the script `run_docker_compose.sh` to start the Weaviate server. \n",
    "- To down the Weaviate server, we will use the script `down_weaveate.sh` to stop the Weaviate server. \n",
    "- To clean up the weaviate server, down the docker and clean the `weaveateSetup/weaviate_data` directory so that you can recreate database from scratch if needed.\n",
    "- **Note**: Run these scripts outside of the notebook environment to ensure proper execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888c12bd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéØ Phase-1: Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5baaf6e",
   "metadata": {},
   "source": [
    "### üîê API Keys Setup\n",
    "\n",
    "To use the RAG system, you'll need API keys for the language models:\n",
    "\n",
    "#### **ü§ñ OpenAI API Key** (Required)\n",
    "- **Purpose**: Used for text embeddings and GPT chat completions\n",
    "- **Get it from**: [OpenAI Platform](https://platform.openai.com/api-keys)\n",
    "- **Format**: Starts with `sk-`\n",
    "\n",
    "#### **üíé Gemini API Key** (Required)  \n",
    "- **Purpose**: Alternative LLM for chat completions and model diversity\n",
    "- **Get it from**: [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
    "- **Format**: Usually a long alphanumeric string\n",
    "\n",
    "#### **‚öôÔ∏è Setup Methods**\n",
    "Choose one of the following methods to configure your API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56ca1f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#!/bin/bash\n",
    "\n",
    "# Function to safely prompt for API key\n",
    "get_api_key() {\n",
    "    local service_name=$1\n",
    "    local env_var_name=$2\n",
    "    \n",
    "    echo \"üîë Setting up $service_name API key...\"\n",
    "    echo \"Please enter your $service_name API key:\"\n",
    "    read -s api_key\n",
    "    \n",
    "    if [ -z \"$api_key\" ]; then\n",
    "        echo \"‚ùå No API key entered for $service_name\"\n",
    "        return 1\n",
    "    fi\n",
    "    \n",
    "    export $env_var_name=\"$api_key\"\n",
    "    echo \"‚úÖ $service_name API key set successfully\"\n",
    "    return 0\n",
    "}\n",
    "\n",
    "# Create persistent environment setup script\n",
    "cat > setup_env.sh << 'EOF'\n",
    "#!/bin/bash\n",
    "# RAG System Environment Setup\n",
    "# Replace the placeholder values below with your actual API keys\n",
    "\n",
    "# OpenAI API Key (required for embeddings and chat)\n",
    "export OPENAI_API_KEY=\"your-openai-api-key-here\"\n",
    "\n",
    "# Gemini API Key (alternative LLM option)\n",
    "export GEMINI_API_KEY=\"your-gemini-api-key-here\"\n",
    "\n",
    "# Verify the keys are set\n",
    "if [ ! -z \"$OPENAI_API_KEY\" ] && [ \"$OPENAI_API_KEY\" != \"your-openai-api-key-here\" ]; then\n",
    "    echo \"‚úÖ OPENAI_API_KEY: ${OPENAI_API_KEY:0:10}...\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  Please set your actual OPENAI_API_KEY in setup_env.sh\"\n",
    "fi\n",
    "\n",
    "if [ ! -z \"$GEMINI_API_KEY\" ] && [ \"$GEMINI_API_KEY\" != \"your-gemini-api-key-here\" ]; then\n",
    "    echo \"‚úÖ GEMINI_API_KEY: ${GEMINI_API_KEY:0:10}...\"\n",
    "else\n",
    "    echo \"‚ö†Ô∏è  Please set your actual GEMINI_API_KEY in setup_env.sh\"\n",
    "fi\n",
    "EOF\n",
    "\n",
    "chmod +x setup_env.sh\n",
    "\n",
    "echo \"üìù Created setup_env.sh file with template API keys\"\n",
    "echo \"\"\n",
    "echo \"üîß SETUP OPTIONS:\"\n",
    "echo \"1. Interactive setup (recommended for first time):\"\n",
    "echo \"   Run the cells below to enter your API keys interactively\"\n",
    "echo \"\"\n",
    "echo \"2. Manual setup:\"\n",
    "echo \"   Edit setup_env.sh file and replace the placeholder keys with your actual keys\"\n",
    "echo \"   Then run: source setup_env.sh\"\n",
    "echo \"\"\n",
    "echo \"üí° For security, the setup_env.sh file should not be committed to version control\"\n",
    "\n",
    "# Check if we should do interactive setup\n",
    "if [ \"$INTERACTIVE_SETUP\" = \"true\" ]; then\n",
    "    echo \"\"\n",
    "    echo \"üöÄ Starting interactive API key setup...\"\n",
    "    \n",
    "    # Set up OpenAI API Key\n",
    "    if get_api_key \"OpenAI\" \"OPENAI_API_KEY\"; then\n",
    "        echo \"export OPENAI_API_KEY=\\\"$OPENAI_API_KEY\\\"\" >> .env_temp\n",
    "    fi\n",
    "    \n",
    "    # Set up Gemini API Key  \n",
    "    if get_api_key \"Gemini\" \"GEMINI_API_KEY\"; then\n",
    "        echo \"export GEMINI_API_KEY=\\\"$GEMINI_API_KEY\\\"\" >> .env_temp\n",
    "    fi\n",
    "    \n",
    "    if [ -f .env_temp ]; then\n",
    "        source .env_temp\n",
    "        rm .env_temp\n",
    "        echo \"üéâ API keys have been set for this session!\"\n",
    "    fi\n",
    "else\n",
    "    echo \"üìã To enable interactive setup, set INTERACTIVE_SETUP=true before running this cell\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65b51bc",
   "metadata": {},
   "source": [
    "\n",
    "#### ‚ö†Ô∏è **IMPORTANT** Don't forget to load the environment variables from the `setup_env.sh` file before running the indexing code or chat retriever code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41825b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After editing setup_env.sh with your actual API keys, run this to load them:\n",
    "import os\n",
    "import re\n",
    "\n",
    "def load_env_from_sh(filename=\"setup_env.sh\"):\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            # Match lines like: export VAR=\"value\"\n",
    "            match = re.match(r'export (\\w+)=(.*)', line)\n",
    "            if match:\n",
    "                key, val = match.groups()\n",
    "                val = val.strip().strip('\"').strip(\"'\")\n",
    "                os.environ[key] = val\n",
    "\n",
    "load_env_from_sh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da4487",
   "metadata": {},
   "source": [
    "See if environment variables are correctly accessible in the notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4eae671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking API Key Configuration...\n",
      "==================================================\n",
      "‚úÖ OPENAI_API_KEY is properly set: sk-proj-k9...4eEA\n",
      "‚úÖ GEMINI_API_KEY is properly set: AIzaSyDZ9j...Ghgg\n",
      "\n",
      "==================================================\n",
      "üéâ All API keys are properly configured!\n",
      "‚úÖ RAG system is ready to use both OpenAI and Gemini\n",
      "\n",
      "üöÄ Ready to proceed with RAG indexing!\n"
     ]
    }
   ],
   "source": [
    "# Verify environment variables are loaded in Python\n",
    "import os\n",
    "\n",
    "def check_api_keys():\n",
    "    \"\"\"Check if API keys are properly set and provide helpful guidance\"\"\"\n",
    "    print(\"üîç Checking API Key Configuration...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    gemini_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    \n",
    "    # Check OpenAI API Key\n",
    "    if openai_key and not openai_key.startswith(\"sk-your-\"):\n",
    "        print(f\"‚úÖ OPENAI_API_KEY is properly set: {openai_key[:10]}...{openai_key[-4:]}\")\n",
    "        openai_valid = True\n",
    "    elif openai_key:\n",
    "        print(\"‚ö†Ô∏è  OPENAI_API_KEY is set but appears to be a placeholder\")\n",
    "        print(\"   Please replace with your actual OpenAI API key\")\n",
    "        openai_valid = False\n",
    "    else:\n",
    "        print(\"‚ùå OPENAI_API_KEY is not set\")\n",
    "        openai_valid = False\n",
    "    \n",
    "    # Check Gemini API Key\n",
    "    if gemini_key and not gemini_key.startswith(\"your-gemini\"):\n",
    "        print(f\"‚úÖ GEMINI_API_KEY is properly set: {gemini_key[:10]}...{gemini_key[-4:]}\")\n",
    "        gemini_valid = True\n",
    "    elif gemini_key:\n",
    "        print(\"‚ö†Ô∏è  GEMINI_API_KEY is set but appears to be a placeholder\")\n",
    "        print(\"   Please replace with your actual Gemini API key\")\n",
    "        gemini_valid = False\n",
    "    else:\n",
    "        print(\"‚ùå GEMINI_API_KEY is not set\")\n",
    "        gemini_valid = False\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    \n",
    "    if openai_valid and gemini_valid:\n",
    "        print(\"üéâ All API keys are properly configured!\")\n",
    "        print(\"‚úÖ RAG system is ready to use both OpenAI and Gemini\")\n",
    "        return True\n",
    "    elif openai_valid:\n",
    "        print(\"‚ö° OpenAI API key is ready - RAG system can use OpenAI models\")\n",
    "        print(\"üí° Gemini is optional but recommended for model diversity\")\n",
    "        return True\n",
    "    elif gemini_valid:\n",
    "        print(\"‚ö° Gemini API key is ready - RAG system can use Gemini models\")\n",
    "        print(\"üí° OpenAI API key is highly recommended for embeddings\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"üö® No valid API keys found!\")\n",
    "        print(\"\\nüìã Setup Options:\")\n",
    "        print(\"1. Run the interactive Python setup above: setup_api_keys_interactive()\")\n",
    "        print(\"2. Edit setup_env.sh and run: source setup_env.sh\")\n",
    "        print(\"3. Set environment variables manually:\")\n",
    "        print(\"   export OPENAI_API_KEY='your-key-here'\")\n",
    "        print(\"   export GEMINI_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "\n",
    "# Check API keys and provide guidance\n",
    "api_keys_ready = check_api_keys()\n",
    "\n",
    "if api_keys_ready:\n",
    "    print(\"\\nüöÄ Ready to proceed with RAG indexing!\")\n",
    "else:\n",
    "    print(\"\\n‚è∏Ô∏è  Please set up API keys before continuing with the RAG system\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81860755",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üöÄ  Import the modules needed for Indexing the knowledge base content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40328dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAG Indexer components imported successfully!\n",
      "üìç Ready to process ScaleLayout and AppLayout knowledge base\n"
     ]
    }
   ],
   "source": [
    "# Import the RAG indexing components\n",
    "from block_indexer import IndexDocumentsBlock\n",
    "from aios_instance import TestContext, BlockTester\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"‚úÖ RAG Indexer components imported successfully!\")\n",
    "print(\"üìç Ready to process ScaleLayout and AppLayout knowledge base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b0b9ad",
   "metadata": {},
   "source": [
    "### üìÅ Configuring the Knowledge Base Directory\n",
    "\n",
    "We'll point our indexer to the knowledge base containing all the documentation for ScaleLayout and AppLayout systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49b66b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Knowledge base directory: knowledge_base\n",
      "üìÑ Processed passages will be saved to: tests/output/psgs_w100.jsonl\n",
      "üîç Vector index will be created at: tests/output/psgs_w100.index\n"
     ]
    }
   ],
   "source": [
    "# Configure directories for RAG knowledge base\n",
    "# This directory contains all ScaleLayout and AppLayout documentation\n",
    "KNOWLEDGE_BASE_DIRECTORY = \"knowledge_base\"  # Contains model cards, policy cards, use case documentation\n",
    "os.makedirs(KNOWLEDGE_BASE_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Output directory for processed data\n",
    "OUTPUT_DIR = \"tests/output\"\n",
    "PASSAGES_JSON = os.path.join(OUTPUT_DIR, \"psgs_w100.jsonl\")  # Processed text chunks\n",
    "INDEX_PATH = os.path.join(OUTPUT_DIR, \"psgs_w100.index\")     # Vector index for retrieval\n",
    "\n",
    "# Clean and recreate output directory\n",
    "shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"üìÇ Knowledge base directory: {KNOWLEDGE_BASE_DIRECTORY}\")\n",
    "print(f\"üìÑ Processed passages will be saved to: {PASSAGES_JSON}\")\n",
    "print(f\"üîç Vector index will be created at: {INDEX_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1075eae4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### ‚öôÔ∏è RAG Configuration for ScaleLayout/AppLayout\n",
    "\n",
    "#### Key Configuration Decisions:\n",
    "\n",
    "**üß† Embedding Model**: Using OpenAI's `text-embedding-3-large` for high-quality semantic understanding\n",
    "- **Why**: Superior performance on technical documentation and multi-domain knowledge\n",
    "- **Dimension**: 1024 for rich semantic representation\n",
    "\n",
    "**üìù Chunking Strategy**: \n",
    "- **Chunk Size**: 200 tokens (optimal for technical documentation)\n",
    "- **Overlap**: 50 tokens (ensures context continuity across chunks)\n",
    "- **Filename Prefix**: Enabled (preserves source document information)\n",
    "\n",
    "**üéØ Similarity Threshold**: 0.4 (enables broader context retrieval for complex queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcd9ac88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è RAG Configuration Summary:\n",
      "   üß† Embedding Model: openai/text-embedding-3-large\n",
      "   üìù Chunk Size: 200 tokens\n",
      "   üîÑ Chunk Overlap: 50 tokens\n",
      "   üéØ Similarity Threshold: 0.4\n",
      "   üíæ Vector Database: WEAVIATE\n"
     ]
    }
   ],
   "source": [
    "# RAG system configuration optimized for ScaleLayout/AppLayout\n",
    "EMBEDDING_DIM = 1024  # High-dimensional embeddings for technical content\n",
    "\n",
    "# Create AIOS specific test context with optimized settings\n",
    "context = TestContext()\n",
    "\n",
    "# Configure the knowledge base and storage\n",
    "context.block_init_data = {\n",
    "    \"repo_dir\": KNOWLEDGE_BASE_DIRECTORY,\n",
    "    \"passages_json\": PASSAGES_JSON,\n",
    "    \"index_path\": INDEX_PATH,\n",
    "    \n",
    "    # Embedding configuration for technical documentation\n",
    "    \"embed_model\": \"openai/text-embedding-3-large\",  # High-quality embeddings\n",
    "    \n",
    "    # Vector database configuration (Weaviate)\n",
    "    \"client_type\": \"weaviate\",\n",
    "    \"client_config\": {\n",
    "        \"uri\": \"http://localhost:8080\",\n",
    "        \"user\": \"coguser1\",\n",
    "        \"password\": \"sdf345BJw44HMy\",\n",
    "        \"dim\": EMBEDDING_DIM\n",
    "    }\n",
    "}\n",
    "\n",
    "# Text processing parameters optimized for technical content\n",
    "context.block_init_parameters = {\n",
    "    # Chunking strategy for technical documentation\n",
    "    \"chunk_size\": 200,              # Optimal size for technical concepts\n",
    "    \"chunk_overlap\": 50,            # Ensures context continuity\n",
    "    \"max_length\": EMBEDDING_DIM,    # Match embedding dimensions\n",
    "    \n",
    "    # Retrieval optimization\n",
    "    \"similarity_threshold\": 0.4,    # Broader context for complex queries\n",
    "    \"include_filename_prefix\": True  # Preserve source information\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è RAG Configuration Summary:\")\n",
    "print(f\"   üß† Embedding Model: {context.block_init_data['embed_model']}\")\n",
    "print(f\"   üìù Chunk Size: {context.block_init_parameters['chunk_size']} tokens\")\n",
    "print(f\"   üîÑ Chunk Overlap: {context.block_init_parameters['chunk_overlap']} tokens\")\n",
    "print(f\"   üéØ Similarity Threshold: {context.block_init_parameters['similarity_threshold']}\")\n",
    "print(f\"   üíæ Vector Database: {context.block_init_data['client_type'].upper()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d13fb7",
   "metadata": {},
   "source": [
    "### üöÄ Initialize RAG Indexer\n",
    "\n",
    "Now we'll initialize the indexer and process all the ScaleLayout and AppLayout documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bdc74d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/weaviate/warnings.py:14: UserWarning: Auth001: The client was configured to use authentication, but weaviate is configured without\n",
      "                    authentication. Are you sure this is correct?\n",
      "  warnings.warn(\n",
      "INFO:block_indexer:Initialized with chunk_size=200, chunk_overlap=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Resetting existing data...\n",
      "‚úÖ RAG Indexer initialized and ready!\n",
      "üìö About to process ScaleLayout and AppLayout knowledge base...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the RAG indexer with our configuration\n",
    "tester = BlockTester.init_with_context(IndexDocumentsBlock, context)\n",
    "\n",
    "# Reset any existing data to ensure clean indexing\n",
    "print(\"üßπ Resetting existing data...\")\n",
    "tester.block_instance.management(\"reset\", {})\n",
    "\n",
    "print(\"‚úÖ RAG Indexer initialized and ready!\")\n",
    "print(\"üìö About to process ScaleLayout and AppLayout knowledge base...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f8fdde",
   "metadata": {},
   "source": [
    "### üîÑ Processing the Knowledge Base and Run the Indexer\n",
    "\n",
    "This step will: (Read `block_indexer.py` for more details)\n",
    "1. **üìñ Extract text** from all documentation files (PDFs, Markdown, Json etc.)\n",
    "2. **‚úÇÔ∏è Chunk the content** into semantic segments with overlap\n",
    "3. **üß† Generate embeddings** using OpenAI's advanced model\n",
    "4. **üï∏Ô∏è Build a graph** connecting related concepts (Graph RAG). \n",
    "    - Chunk from same file will be connected with weight 1.0 \n",
    "    - Chunks across file will be connected with weight of cosine similarity\n",
    "    - Each chunk will be stored as a Node in Weaviate with Text, Embedding, Chunk ID, and File Name details.\n",
    "    - Chunk IDs will start with zero (-1 is reserved for summary node)\n",
    "    - Initial graph is constructed with the help of networkx.Graph library.\n",
    "    - Sequential edges refferes to Chunks of same file linking.\n",
    "    - Similarity edges refferes to Chunks of different file linking with cosine similarity\n",
    "    - Property name with `from_node` and `to_node` are used to store the edges in Weaviate.\n",
    "5. **üíæ Store everything(Graph,Node,Edges)** in Weaviate for fast retrieval\n",
    "6. **üìù (Optional) Enable Summary Node (`chunk_id = -1`)**:  \n",
    "    - A summary node is generated by an LLM to condense an entire document into a single, comprehensive node.  \n",
    "    - This provides the LLM with full document context for advanced reasoning.  \n",
    "    - *Note: This feature is currently commented out in the code to avoid excessive context length.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b28baf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:block_indexer:File: README_input_format_cards.md - Created 3 chunks with 50 token overlap\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting RAG knowledge base creation...\n",
      "üìä This will process all ScaleLayout and AppLayout documentation\n",
      "‚è≥ Please wait while we build the semantic knowledge graph...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:block_indexer:File: DOCUMENTATION_INDEX.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics.xlsx - Created 20 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/NodeAndGPUAssignementRules.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/UsecaseGroupingRules.md - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_gpumemory_and_gpuutility.csv - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_gpumemory_and_gpuutility.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/SetCreationRules.md - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/query_vcpu_ram_of_pod.py - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/ScaleLayoutRAGQueries.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/UsecaseVsNumberOfInputFrames.md - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/scalelayout.md - Created 16 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/collect_gpu_pid_podname.py - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/xlsx_to_md.py - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/CameraVsUsecaseMatrixs.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/ResolveConflictsForLLM_InScalelayout.md - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/PodMetrics_general_info.md - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/vehicles5Detection_360h_640.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/trackerlitefast_960x540.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/usecasmux_3input.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/policy.md - Created 18 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/firesmoke7Det_512h_896w.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/policy_mux.md - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/general7Detection_360h_640.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/fall7Detection_640h_640.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/usecase-frames.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/custbodygend75.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/fight3Det.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/luggage7Detection_1080_1920.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/pose-estimation-rt.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/camTamp_360h_640w.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/weapon7Detection_896_896.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/reidbaselineallres.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/trackerlite.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: ScaleLayout/pod_metrics/usecase.md - Created 13 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: input_source_cards/knowledge_base_input_source_cards_rtsp_stream.json - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: converter_cards/yolo_to_od1_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: converter_cards/yolo_to_od1_converter.json - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: converter_cards/README_converter_cards.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: converter_cards/yolo_to_od1_converter_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: output_format_cards/od1_format.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: output_format_cards/od1_output_format_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/01_LOW_LIGHT_CONDITIONS.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/USE_CASES_STATUS.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/06_FALSE_POSITIVE_MINIMIZATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/05_TRUE_POSITIVE_OPTIMIZATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/04_LARGE_VIEWPOINT_COVERAGE.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/MASTER_USECASE_PARAMETERS.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/00_PARAMETER_RECOMMENDATIONS_SUMMARY.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/03_FAR_OBJECT_DETECTION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/02_HIGH_ACCURACY_REQUIREMENTS.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/VANDALISM_DETECTION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/FIRE_DETECTION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/FACE_RECOGNITION_SYSTEM.md - Created 14 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/CROWD_GATHERING_DETECTION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/FACE_SEARCH.md - Created 11 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/INTRUSION_DETECTION.md - Created 11 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/PEOPLE_LINE_CROSS_COUNTING.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/GROUP_RUNNING_DETECTION.md - Created 12 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/ABANDONED_BAG_DETECTION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/OBJECT_INDEXING.md - Created 13 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/PERSON_TRACKING.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/MULTI_CAMERA_TRACKING.md - Created 12 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/LOITERING_DETECTION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/WEAPON_DETECTION.md - Created 13 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/STONE_THROWING_DETECTION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/SOCIAL_DISTANCE_MONITORING.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/FALL_DETECTION.md - Created 14 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/PARKING_VIOLATION_DETECTION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/GESTURE_WAVING_DETECTION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/CAMERA_TAMPERING_DETECTION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: parameter_recommendations/use_cases/CHAIN_SNATCHING_DETECTION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7vehicles5_tiny_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/facenet_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov3vehicles5_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/fastmot_tracker_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7luggage_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/reid.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7vehicles5_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov3vehicles5.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/facenet.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7_pose.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/viou_tracker.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov8n_DOCUMENTATION.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7luggage.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/fastmot_tracker.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/bytetrack.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/viou_tracker_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov8n.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/pose_fast.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7vehicles5_tiny.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/README_model_cards.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7_general_coco_DOCUMENTATION.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7vehicles5.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7_general_coco.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/yolov7_pose_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/bytetrack_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/reid_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: model_cards/pose_fast_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: pipeline_compositions/RAG_QUERY_EXAMPLES.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: pipeline_compositions/COMPLETE_PIPELINE_ARCHITECTURES.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: pipeline_compositions/CV_PIPELINE_TEMPLATES.md - Created 0 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: input_format_cards/od1_format.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: input_format_cards/README_input_format_cards.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: input_format_cards/od1_format_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: input_format_cards/od1_input_format_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/README_policy_cards.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/association_policy_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/dwell_time_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/inside_zone_policy_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_conf_filter_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/confidence_filter_policy_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/dwell_time_policy_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_filter_policy_DOCUMENTATION.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/confidence_filter_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_filter_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/interaction_policy_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_conf_filter_policy_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/zone_filtering_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/interaction_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/inside_zone_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_replacer_policy_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/zone_filtering_policy_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/association_policy.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: policy_cards/class_replacer_policy.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: usecase_cards/README_usecase_cards.md - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: usecase_cards/crowd_gathering_detection_simple.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: usecase_cards/loitering_detection_simple.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: usecase_cards/crowd_gathering_detection_simple_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: usecase_cards/loitering_detection_simple_DOCUMENTATION.md - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/cv_pipeline_best_practices.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/gpu_cards/nvidia_t4_DOCUMENTATION.md - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/gpu_cards/nvidia_t4.json - Created 1 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/license_type_cards/mit_license_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/license_type_cards/mit_license.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/dataset_cards/coco_DOCUMENTATION.md - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: general_cards/dataset_cards/coco.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_fallDetection_camera_49_49_18_restapi.json - Created 7 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_facesearch_camera_101_100_5.json - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_cameraTampering_camera_55_70_11.json - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_AbandonedBag_camera_70_10_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_groupRunning_camera_55_53_13_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/crowd_gathering.json - Created 3 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_objectIndexing_camera_55_145_14_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_frs_camera_100_100_1.json - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_cameraTampering_camera_55_70_11_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_AbandonedBag_camera_70_10.json - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/chain_snatching_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_vandalism_camera_54_39_12_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/chain_snatching.json - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_policybuilder_objectIndexing_camera_40_8_10_1_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_persontracking_camera_41_10_11_DOCUMENTATION.md - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/parking_violation.json - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_stonethrowing_camera_39_18_11.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0008_weaponDetectionmux_camera_55_67_11_2.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_persontracking_camera_41_10_11.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/loitering.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_objectIndexing_camera_55_145_14.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_policybuilder_objectIndexing_camera_40_8_10_1.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_PeopleLineCrossCounting_camera_34_29_11_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_facesearch_camera_101_100_5_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_socialDistance_camera_54_13_13_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_multicameratracking_camera_55_66_9.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0008_weaponDetectionmux_camera_55_67_11_2_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/parking_violation_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_stonethrowing_camera_39_18_11_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/crowd_gathering_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_intrusion_camera_56_3_10_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_frs_camera_100_100_1_DOCUMENTATION.md - Created 9 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/README_chaining_blueprints.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_PeopleLineCrossCounting_camera_34_29_11.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_intrusion_camera_56_3_10.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_fallDetection_camera_49_49_18_restapi_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_groupRunning_camera_55_53_13.json - Created 5 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/loitering_DOCUMENTATION.md - Created 8 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0002_multicameratracking_camera_55_66_9_DOCUMENTATION.md - Created 2 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_socialDistance_camera_54_13_13.json - Created 4 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_gestureWaving_camera_31_10_14_DOCUMENTATION.md - Created 10 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_vandalism_camera_54_39_12.json - Created 6 chunks with 50 token overlap\n",
      "INFO:block_indexer:File: chaining_blueprints_cards/0001_gestureWaving_camera_31_10_14.json - Created 5 chunks with 50 token overlap\n",
      "Embedding & pushing to Weaviate: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:11<00:00,  1.09it/s]\n",
      "Building similarity graph edges (top-K): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 406/406 [01:02<00:00,  6.47it/s]\n",
      "INFO:block_indexer:Indexed 406 passages with 50 token overlap.\n",
      "Built graph with 227 sequential edges and 14212 similarity edges.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìà INDEXING RESULTS\n",
      "============================================================\n",
      "Indexer output: [{'message': 'Indexed 406 passages with 50 token overlap.\\nBuilt graph with 227 sequential edges and 14212 similarity edges.'}]\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run the indexing process - this creates our RAG knowledge base\n",
    "print(\"üöÄ Starting RAG knowledge base creation...\")\n",
    "print(\"üìä This will process all ScaleLayout and AppLayout documentation\")\n",
    "print(\"‚è≥ Please wait while we build the semantic knowledge graph...\\n\")\n",
    "\n",
    "# Execute the indexing\n",
    "results = tester.run({})  # This triggers the on_data method to build the index\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà INDEXING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(\"Indexer output:\", results)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7b714",
   "metadata": {},
   "source": [
    "## üéØ Phase-2\n",
    "\n",
    "### **üèóÔ∏è AppLayout: DAG Creation Process**\n",
    "\n",
    "When a user queries for a specific use case (e.g., \"Monitor stationary vehicles in Indian road scenario\" or \"Person detection with face recognition\"):\n",
    "\n",
    "1. **üîç Query Understanding**: RAG retrieves relevant use case cards and pipeline blueprints\n",
    "2. **üß© Component Selection**: Identifies required AI models, preprocessing steps, and post-processing\n",
    "3. **üîó DAG Construction**: Creates optimal directed acyclic graph based on retrieved patterns\n",
    "4. **‚ö° Optimization**: Applies best practices from similar deployments\n",
    "\n",
    "### **‚öñÔ∏è ScaleLayout: Deployment Planning Process**\n",
    "\n",
    "When multiple cameras need deployment across servers:\n",
    "\n",
    "1. **üìä Resource Analysis**: RAG retrieves hardware specifications and capacity limits\n",
    "2. **üéØ Input Grouping(Camera Grouping)**: Uses policy cards to determine optimal camera grouping based on set creation rules\n",
    "3. **üíª Server Assignment**: Allocates resources based on historical performance data\n",
    "4. **üîß Load Balancing**: Applies optimization strategies from deployment experience\n",
    "\n",
    "### **ü§ù Synergy Between Systems**\n",
    "\n",
    "- **Unified Context**: Both systems access the same knowledge base for consistent decisions\n",
    "- **Cross-Optimization**: AppLayout DAG complexity informs ScaleLayout resource allocation\n",
    "- **Model Sharing**: Multiple cameras processed together for model sharing optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c276962",
   "metadata": {},
   "source": [
    "## üéì Next Steps: Using the RAG System\n",
    "\n",
    "\n",
    "### **üîÑ Integration with Retrieval Block**:\n",
    "The indexed knowledge will be used by the `block_retriever.py` component to:\n",
    "- **Answer complex technical questions**\n",
    "- **Provide deployment recommendations**\n",
    "- **Generate optimized configurations**\n",
    "- **Suggest alternative approaches**\n",
    "\n",
    "- **Query Embedding** will be used to retrieve topk relevant chunks from the knowledge base(Sequential Node property is used).\n",
    "- For every Sequential node, `edge_limit` number of Similarity edges will be traversed and relevant chunks will be retrieved from the knowledge base.\n",
    "- **Reranker** will be used to get `reranking_topk` chunk from the retrieved chunks.\n",
    "- This will be the context for the LLM to generate the response. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab4fbef",
   "metadata": {},
   "source": [
    "## üí¨ Interactive RAG Chat Interface\n",
    "\n",
    "Now let's set up an interactive chat interface to query our knowledge base in real-time. This interface allows you to:\n",
    "\n",
    "- **üéØ Ask specific questions** about AppLayout DAG creation\n",
    "- **‚öñÔ∏è Query ScaleLayout deployment strategies** \n",
    "- **üîç Explore the knowledge base** interactively\n",
    "- **üìä Get real-time answers** with source citations\n",
    "- **üí° Test different query approaches** for optimization\n",
    "\n",
    "### **Chat Interface Features**:\n",
    "- **Context-aware responses** using RAG retrieval\n",
    "- **Source citations** showing which documents informed the answer\n",
    "- **Session management** with conversation history\n",
    "- **Command support** (help, history, clear, quit)\n",
    "- **Real-time processing** of complex technical queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f487b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the interactive chat components\n",
    "from block_retriever import RagQAServiceBlock\n",
    "import time\n",
    "from datetime import datetime\n",
    "from aios_instance import TestContext, BlockTester\n",
    "\n",
    "# Setup RAG retriever for interactive chat\n",
    "def setup_rag_chat_interface():\n",
    "    \"\"\"Setup the RAG retriever with CV pipeline knowledge\"\"\"\n",
    "    \n",
    "    # Create a unique session ID for this chat\n",
    "    session_id = f\"cv_pipeline_chat_{int(time.time())}\"\n",
    "    \n",
    "    # Configure the model and generation parameters\n",
    "    modelType = \"gemini\"  # Can be \"gemini\" or \"openai\"\n",
    "    \n",
    "    if modelType == \"gemini\":\n",
    "        #llm_model = 'gemini-2.0-flash-exp'  # Fast and efficient for chat\n",
    "        llm_model = 'gemini-2.5-pro'  # Fast and efficient for chat\n",
    "        generation_kwargs = {\n",
    "            \"max_tokens\": 16384,  # Increased for full passage processing\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.95\n",
    "        }\n",
    "    elif modelType == \"openai\":\n",
    "        llm_model = 'gpt-4o-mini'  # Cost-effective for chat interactions\n",
    "        generation_kwargs = {\n",
    "            \"max_tokens\": 16384,\n",
    "            \"temperature\": 0.25,\n",
    "            \"top_p\": 0.95,\n",
    "            \"frequency_penalty\": 0.1,\n",
    "            \"presence_penalty\": 0.05\n",
    "        }\n",
    "    \n",
    "    # Configure the RAG block\n",
    "    context = TestContext()\n",
    "    context.block_init_data = {\n",
    "        \"weaviate_url\": \"http://localhost:8080\",\n",
    "        \"node_class\": \"PassageNode\", \n",
    "        \"edge_class\": \"PassageEdge\",\n",
    "        \"llm_model\": llm_model,\n",
    "        \"embed_model\": \"openai/text-embedding-3-large\",\n",
    "    }\n",
    "    \n",
    "    # Set embedding dimension and reranking model\n",
    "    EMBEDDING_DIM = 1024\n",
    "    RERANKING_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "    PASSAGES_JSON = \"\"\n",
    "    \n",
    "    # Configure RAG parameters\n",
    "    context.block_init_parameters = {\n",
    "        \"topk\": 200,              # Initial retrieval count\n",
    "        \"reranking_topk\": 30,     # After reranking\n",
    "        \"max_length\": EMBEDDING_DIM,\n",
    "        \"edge_limit\": 50,\n",
    "        \"debug\": False,\n",
    "        \"passages_json\": PASSAGES_JSON,\n",
    "        \"similarity_threshold\": 0.45,\n",
    "        \"auto_references\": True,\n",
    "        \"generation_config\": generation_kwargs,\n",
    "        \"reranking_model\": RERANKING_MODEL\n",
    "    }\n",
    "    \n",
    "    # Initialize the RAG block\n",
    "    tester = BlockTester.init_with_context(RagQAServiceBlock, context)\n",
    "    \n",
    "    return tester, session_id\n",
    "\n",
    "# Initialize the chat interface\n",
    "print(\"üîß Setting up RAG chat interface...\")\n",
    "chat_tester, chat_session_id = setup_rag_chat_interface()\n",
    "print(\"‚úÖ Chat interface ready!\")\n",
    "print(f\"üì± Session ID: {chat_session_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39467d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotebookChatInterface:\n",
    "    \"\"\"Interactive chat interface for Jupyter notebook\"\"\"\n",
    "    \n",
    "    def __init__(self, tester, session_id):\n",
    "        self.tester = tester\n",
    "        self.session_id = session_id\n",
    "        self.conversation_history = []\n",
    "        \n",
    "        # Initialize chat session with system message\n",
    "        self.tester.run({\n",
    "            \"mode\": \"chat\",\n",
    "            \"session_id\": self.session_id,\n",
    "#             \"system_message\": \"\"\"You are a specialized assistant for answering questions based on provided context. Your primary directive is to adhere strictly to the following rules:\n",
    "\n",
    "# 1. **Context is King:** You must base your answers exclusively on the information present in the provided context chunks. Do not use any prior knowledge or external information.\n",
    "\n",
    "# 2. **Cite Your Sources Precisely:** For every piece of information, data point, or decision step you take, you must cite its source. The citation must include **both the name of the source** (e.g., `pod_metrics`) **and the chunk number** it came from in brackets (e.g., `[4]`).\n",
    "\n",
    "# 3. **No Assumptions:** If the context does not provide the necessary information to answer a question, state that the information is not available. Never ask the user to estimate or provide missing details.\n",
    "\n",
    "# 4. **Topic-Specific Logic:**\n",
    "#    * **For AppLayout Creation:** When a question is about creating an `AppLayout`, you must ignore any context or information related to `ScaleLayout`.\n",
    "#    * **For Deployment Planning:** When a question is about deployment planning, you must follow the exact five-step process, providing precise citations for each step.\n",
    "# \"\"\",\n",
    "            \"message\": \"Hello! I'm ready to help with your ScaleLayout and AppLayout questions.\"\n",
    "        })\n",
    "    \n",
    "    def ask(self, question):\n",
    "        \"\"\"Ask a question and get a response from the RAG system\"\"\"\n",
    "        try:\n",
    "            # Process the query using RAG-enhanced chat\n",
    "            result = self.tester.run({\n",
    "                \"mode\": \"rag-chat\",\n",
    "                \"session_id\": self.session_id,\n",
    "                \"message\": question\n",
    "            })\n",
    "            \n",
    "            if result and len(result) > 0:\n",
    "                response = result[0].get(\"reply\", \"Sorry, I couldn't generate a response.\")\n",
    "            else:\n",
    "                response = \"Sorry, I couldn't process your query. Please try rephrasing.\"\n",
    "            \n",
    "            # Save to conversation history\n",
    "            timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "            self.conversation_history.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"question\": question,\n",
    "                \"response\": response\n",
    "            })\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error processing query: {str(e)}\"\n",
    "            print(f\"‚ùå {error_msg}\")\n",
    "            return error_msg\n",
    "    \n",
    "    def show_history(self):\n",
    "        \"\"\"Display conversation history\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            print(\"No conversation history yet.\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìù Conversation History\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, entry in enumerate(self.conversation_history, 1):\n",
    "            print(f\"\\n[{i}] {entry['timestamp']}\")\n",
    "            print(f\"Q: {entry['question']}\")\n",
    "            print(f\"A: {entry['response'][:200]}{'...' if len(entry['response']) > 200 else ''}\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear conversation history\"\"\"\n",
    "        self.conversation_history = []\n",
    "        print(\"‚úÖ Conversation history cleared.\")\n",
    "\n",
    "# Create the chat interface\n",
    "chat_interface = NotebookChatInterface(chat_tester, chat_session_id)\n",
    "print(\"üéØ Chat interface created and ready for questions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc71a654",
   "metadata": {},
   "source": [
    "### üéØ How to Use the Chat Interface\n",
    "\n",
    "The chat interface is now ready! You can ask questions in the following ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üí¨ Interactive RAG Chat - Unified AppLayout & ScaleLayout Assistant\n",
    "print(\"üí¨ Interactive RAG Chat Assistant\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Ask questions about AppLayout DAG creation OR ScaleLayout deployment planning.\")\n",
    "print(\"The system will automatically understand your question type and provide relevant answers.\")\n",
    "print()\n",
    "print(\"üéØ Example Questions:\")\n",
    "print(\"   AppLayout: 'How do I create a DAG for person detection with face recognition?'\")\n",
    "print(\"   ScaleLayout: 'How should I deploy 15 cameras across 3 servers?'\")\n",
    "print(\"   General: 'What parameters should I use for low-light vehicle detection?'\")\n",
    "print(\"   Mixed: 'Plan deployment for retail analytics with face recognition'\")\n",
    "print()\n",
    "print(\"Commands: Type 'quit', 'exit', or 'q' to stop | 'history' to view past questions | 'clear' to reset\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        question = input(\"\\nüéØ Your Question: \").strip()\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "            \n",
    "        # Handle special commands\n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Thanks for using the RAG Chat Assistant! Goodbye!\")\n",
    "            break\n",
    "            \n",
    "        elif question.lower() == 'history':\n",
    "            chat_interface.show_history()\n",
    "            continue\n",
    "            \n",
    "        elif question.lower() == 'clear':\n",
    "            chat_interface.clear_history()\n",
    "            print(\"‚úÖ Chat history cleared. Starting fresh!\")\n",
    "            continue\n",
    "            \n",
    "        # Process the question\n",
    "        print(\"ü§î Processing your question...\")\n",
    "        response = chat_interface.ask(question)\n",
    "        print(f\"\\nü§ñ Assistant: {response}\")\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\nüëã Chat session interrupted. Goodbye!\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error: {str(e)}\")\n",
    "        print(\"Please try again or type 'quit' to exit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7eb0022",
   "metadata": {},
   "source": [
    "### üñ•Ô∏è Command-Line Chat Interface\n",
    "\n",
    "For those who prefer a command-line interactive experience, you can also run the standalone chat script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d4c58",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Run the standalone interactive chat script\n",
    "# Uncomment the line below to run the full interactive terminal interface\n",
    "# python test_retriever_interactive.py\n",
    "\n",
    "echo \"üí° To run the full interactive command-line interface:\"\n",
    "echo \"   python test_retriever_interactive.py\"\n",
    "echo \"\"\n",
    "echo \"üéØ Features of the command-line interface:\"\n",
    "echo \"   ‚Ä¢ Full conversation management\"\n",
    "echo \"   ‚Ä¢ Command support (help, history, clear, quit)\"\n",
    "echo \"   ‚Ä¢ Real-time streaming responses\"\n",
    "echo \"   ‚Ä¢ Better for extended conversations\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da955cc0",
   "metadata": {},
   "source": [
    "## üåê Streamlit Web Chat Interface\n",
    "\n",
    "For a more user-friendly web-based experience, you can also deploy the RAG chat system as a Streamlit web application. This provides:\n",
    "\n",
    "- **üåê Web-based Interface**: Beautiful, modern chat UI accessible via browser\n",
    "- **üì± Mobile-Friendly**: Responsive design that works on all devices  \n",
    "- **üé® Rich Formatting**: Support for markdown, code blocks, and formatted responses\n",
    "- **üíæ Session Persistence**: Chat history maintained across browser sessions\n",
    "- **üîÑ Real-time Updates**: Live response streaming and progress indicators\n",
    "- **üë• Multi-user Support**: Multiple users can access simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0161b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Streamlit RAG Chat Application created!\n",
      "üìÅ File saved as: streamlit_rag_chat.py\n",
      "\n",
      "üöÄ To run the Streamlit web application:\n",
      "   streamlit run streamlit_rag_chat.py\n",
      "\n",
      "üåê The web interface will be available at:\n",
      "   Local URL: http://localhost:8501\n",
      "   Network URL: http://[your-ip]:8501\n"
     ]
    }
   ],
   "source": [
    "# Create a Streamlit web application for the RAG chat interface\n",
    "streamlit_app_code = '''\n",
    "import streamlit as st\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# Suppress verbose logging when running from notebook\n",
    "logging.getLogger(\"streamlit\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"weaviate\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"openai\").setLevel(logging.WARNING)\n",
    "\n",
    "# Add the current directory to path to import our modules\n",
    "sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n",
    "\n",
    "from block_retriever import RagQAServiceBlock\n",
    "from aios_instance import TestContext, BlockTester\n",
    "\n",
    "# Configure Streamlit page\n",
    "st.set_page_config(\n",
    "    page_title=\"RAG Chat Assistant - AppLayout & ScaleLayout\",\n",
    "    page_icon=\"ü§ñ\",\n",
    "    layout=\"wide\",\n",
    "    initial_sidebar_state=\"expanded\"\n",
    ")\n",
    "\n",
    "# Custom CSS for better styling\n",
    "st.markdown(\"\"\"\n",
    "<style>\n",
    "    .main-header {\n",
    "        text-align: center;\n",
    "        padding: 1rem 0;\n",
    "        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
    "        color: white;\n",
    "        border-radius: 10px;\n",
    "        margin-bottom: 2rem;\n",
    "    }\n",
    "    .chat-message {\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "        margin: 1rem 0;\n",
    "        box-shadow: 0 2px 4px rgba(0,0,0,0.1);\n",
    "    }\n",
    "    .user-message {\n",
    "        background-color: #e3f2fd;\n",
    "        border-left: 4px solid #2196f3;\n",
    "    }\n",
    "    .assistant-message {\n",
    "        background-color: #f3e5f5;\n",
    "        border-left: 4px solid #9c27b0;\n",
    "    }\n",
    "    .sidebar-content {\n",
    "        background-color: #f8f9fa;\n",
    "        padding: 1rem;\n",
    "        border-radius: 10px;\n",
    "    }\n",
    "</style>\n",
    "\"\"\", unsafe_allow_html=True)\n",
    "\n",
    "class StreamlitChatInterface:\n",
    "    \"\"\"Streamlit-based chat interface for RAG system\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.setup_session_state()\n",
    "        self.setup_rag_interface()\n",
    "    \n",
    "    def setup_session_state(self):\n",
    "        \"\"\"Initialize Streamlit session state variables\"\"\"\n",
    "        if 'messages' not in st.session_state:\n",
    "            st.session_state.messages = []\n",
    "        if 'chat_interface' not in st.session_state:\n",
    "            st.session_state.chat_interface = None\n",
    "        if 'session_id' not in st.session_state:\n",
    "            st.session_state.session_id = f\"streamlit_chat_{int(time.time())}\"\n",
    "    \n",
    "    def setup_rag_interface(self):\n",
    "        \"\"\"Setup the RAG retriever for Streamlit\"\"\"\n",
    "        if st.session_state.chat_interface is None:\n",
    "            with st.spinner(\"üîß Setting up RAG chat interface...\"):\n",
    "                try:\n",
    "                    # Configure the model and generation parameters\n",
    "                    modelType = \"gemini\"  # Can be \"gemini\" or \"openai\"\n",
    "                    llm_model = None\n",
    "                    generation_kwargs = {}\n",
    "                    if modelType == \"gemini\":\n",
    "                        llm_model = 'gemini-2.5-pro'\n",
    "                        generation_kwargs = {\n",
    "                            \"max_tokens\": 16384,\n",
    "                            \"temperature\": 0.2,\n",
    "                            \"top_p\": 0.95\n",
    "                        }\n",
    "                    elif modelType == \"openai\":\n",
    "                        llm_model = 'gpt-4o-mini'\n",
    "                        generation_kwargs = {\n",
    "                            \"max_tokens\": 16384,\n",
    "                            \"temperature\": 0.25,\n",
    "                            \"top_p\": 0.95,\n",
    "                            \"frequency_penalty\": 0.1,\n",
    "                            \"presence_penalty\": 0.05\n",
    "                        }\n",
    "                    \n",
    "                    # Configure the RAG block\n",
    "                    context = TestContext()\n",
    "                    context.block_init_data = {\n",
    "                        \"weaviate_url\": \"http://localhost:8080\",\n",
    "                        \"node_class\": \"PassageNode\", \n",
    "                        \"edge_class\": \"PassageEdge\",\n",
    "                        \"llm_model\": llm_model,\n",
    "                        \"embed_model\": \"openai/text-embedding-3-large\",\n",
    "                    }\n",
    "                    \n",
    "                    # Set embedding dimension and reranking model\n",
    "                    EMBEDDING_DIM = 1024\n",
    "                    RERANKING_MODEL = \"cross-encoder/ms-marco-MiniLM-L-12-v2\"\n",
    "                    \n",
    "                    # Configure RAG parameters\n",
    "                    context.block_init_parameters = {\n",
    "                        \"topk\": 200,\n",
    "                        \"reranking_topk\": 30,\n",
    "                        \"max_length\": EMBEDDING_DIM,\n",
    "                        \"edge_limit\": 50,\n",
    "                        \"debug\": False,\n",
    "                        \"passages_json\": \"\",\n",
    "                        \"similarity_threshold\": 0.45,\n",
    "                        \"auto_references\": True,\n",
    "                        \"generation_config\": generation_kwargs,\n",
    "                        \"reranking_model\": RERANKING_MODEL\n",
    "                    }\n",
    "                    \n",
    "                    # Initialize the RAG block\n",
    "                    tester = BlockTester.init_with_context(RagQAServiceBlock, context)\n",
    "                    \n",
    "                    # Create chat interface using existing NotebookChatInterface logic\n",
    "                    st.session_state.chat_interface = self.create_chat_interface(tester)\n",
    "                    st.success(\"‚úÖ RAG Chat Interface ready!\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    st.error(f\"‚ùå Error setting up chat interface: {str(e)}\")\n",
    "                    st.session_state.chat_interface = None\n",
    "    \n",
    "    def create_chat_interface(self, tester):\n",
    "        \"\"\"Create chat interface similar to NotebookChatInterface\"\"\"\n",
    "        class StreamlitRAGInterface:\n",
    "            def __init__(self, tester, session_id):\n",
    "                self.tester = tester\n",
    "                self.session_id = session_id\n",
    "                \n",
    "                # Initialize chat session with system message\n",
    "                self.tester.run({\n",
    "                    \"mode\": \"chat\",\n",
    "                    \"session_id\": self.session_id,\n",
    "#                     \"system_message\": \\\"\\\"\\\"You are a specialized assistant for answering questions based on provided context. Your primary directive is to adhere strictly to the following rules:\n",
    "\n",
    "# 1. **Context is King:** You must base your answers exclusively on the information present in the provided context chunks. Do not use any prior knowledge or external information.\n",
    "\n",
    "# 2. **Cite Your Sources Precisely:** For every piece of information, data point, or decision step you take, you must cite its source. The citation must include **both the name of the source** (e.g., `pod_metrics`) **and the chunk number** it came from in brackets (e.g., `[4]`).\n",
    "\n",
    "# 3. **No Assumptions:** If the context does not provide the necessary information to answer a question, state that the information is not available. Never ask the user to estimate or provide missing details.\n",
    "\n",
    "# 4. **Topic-Specific Logic:**\n",
    "#    * **For AppLayout Creation:** When a question is about creating an `AppLayout`, you must ignore any context or information related to `ScaleLayout`.\n",
    "#    * **For Deployment Planning:** When a question is about deployment planning, you must follow the exact five-step process, providing precise citations for each step.\n",
    "# \\\"\\\"\\\",\n",
    "                    \"message\": \"Hello! I'm ready to help with your ScaleLayout and AppLayout questions.\"\n",
    "                })\n",
    "            \n",
    "            def ask(self, question):\n",
    "                \\\"\\\"\\\"Ask a question and get a response from the RAG system\\\"\\\"\\\"\n",
    "                try:\n",
    "                    result = self.tester.run({\n",
    "                        \"mode\": \"rag-chat\",\n",
    "                        \"session_id\": self.session_id,\n",
    "                        \"message\": question\n",
    "                    })\n",
    "                    \n",
    "                    if result and len(result) > 0:\n",
    "                        return result[0].get(\"reply\", \"Sorry, I couldn't generate a response.\")\n",
    "                    else:\n",
    "                        return \"Sorry, I couldn't process your query. Please try rephrasing.\"\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    return f\"Error processing query: {str(e)}\"\n",
    "        \n",
    "        return StreamlitRAGInterface(tester, st.session_state.session_id)\n",
    "    \n",
    "    def render_sidebar(self):\n",
    "        \"\"\"Render the sidebar with information and controls\"\"\"\n",
    "        with st.sidebar:\n",
    "            st.markdown('<div class=\"sidebar-content\">', unsafe_allow_html=True)\n",
    "            \n",
    "            st.markdown(\"### üéØ RAG Chat Assistant\")\n",
    "            st.markdown(\"**AppLayout & ScaleLayout Expert**\")\n",
    "            \n",
    "            st.markdown(\"---\")\n",
    "            \n",
    "            st.markdown(\"### üìö What can I help with?\")\n",
    "            st.markdown(\"\"\"\n",
    "            **üèóÔ∏è AppLayout Questions:**\n",
    "            - DAG creation for use cases\n",
    "            - Pipeline architecture design\n",
    "            - Component selection\n",
    "            - Parameter optimization\n",
    "            \n",
    "            **‚öñÔ∏è ScaleLayout Questions:**\n",
    "            - Deployment planning\n",
    "            - Resource allocation\n",
    "            - Camera grouping strategies\n",
    "            - Hardware optimization\n",
    "            \n",
    "            **üí° General Questions:**\n",
    "            - Best practices\n",
    "            - Troubleshooting\n",
    "            - Performance tuning\n",
    "            \"\"\")\n",
    "            \n",
    "            st.markdown(\"---\")\n",
    "            \n",
    "            # Chat controls\n",
    "            if st.button(\"üóëÔ∏è Clear Chat History\"):\n",
    "                st.session_state.messages = []\n",
    "                st.rerun()\n",
    "            \n",
    "            # Session info\n",
    "            st.markdown(\"### üìä Session Info\")\n",
    "            st.markdown(f\"**Messages:** {len(st.session_state.messages)}\")\n",
    "            st.markdown(f\"**Session ID:** `{st.session_state.session_id[:12]}...`\")\n",
    "            \n",
    "            st.markdown('</div>', unsafe_allow_html=True)\n",
    "    \n",
    "    def render_chat_interface(self):\n",
    "        \"\"\"Render the main chat interface\"\"\"\n",
    "        # Header\n",
    "        st.markdown(\"\"\"\n",
    "        <div class=\"main-header\">\n",
    "            <h1>ü§ñ RAG Chat Assistant</h1>\n",
    "            <p>Ask questions about AppLayout DAG creation or ScaleLayout deployment planning</p>\n",
    "        </div>\n",
    "        \"\"\", unsafe_allow_html=True)\n",
    "        \n",
    "        # Display chat messages\n",
    "        for message in st.session_state.messages:\n",
    "            message_class = \"user-message\" if message[\"role\"] == \"user\" else \"assistant-message\"\n",
    "            icon = \"üéØ\" if message[\"role\"] == \"user\" else \"ü§ñ\"\n",
    "            \n",
    "            st.markdown(f\"\"\"\n",
    "            <div class=\"chat-message {message_class}\">\n",
    "                <strong>{icon} {message[\"role\"].title()}:</strong><br>\n",
    "                {message[\"content\"]}\n",
    "            </div>\n",
    "            \"\"\", unsafe_allow_html=True)\n",
    "        \n",
    "        # Chat input\n",
    "        if prompt := st.chat_input(\"Ask about AppLayout or ScaleLayout...\"):\n",
    "            # Add user message\n",
    "            st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            # Display user message immediately\n",
    "            st.markdown(f\"\"\"\n",
    "            <div class=\"chat-message user-message\">\n",
    "                <strong>üéØ You:</strong><br>\n",
    "                {prompt}\n",
    "            </div>\n",
    "            \"\"\", unsafe_allow_html=True)\n",
    "            \n",
    "            # Generate and display assistant response\n",
    "            if st.session_state.chat_interface:\n",
    "                with st.spinner(\"ü§î Processing your question...\"):\n",
    "                    response = st.session_state.chat_interface.ask(prompt)\n",
    "                \n",
    "                # Add assistant response\n",
    "                st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "                \n",
    "                # Display assistant response\n",
    "                st.markdown(f\"\"\"\n",
    "                <div class=\"chat-message assistant-message\">\n",
    "                    <strong>ü§ñ Assistant:</strong><br>\n",
    "                    {response}\n",
    "                </div>\n",
    "                \"\"\", unsafe_allow_html=True)\n",
    "                \n",
    "                st.rerun()\n",
    "            else:\n",
    "                st.error(\"‚ùå Chat interface not available. Please refresh the page.\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main function to run the Streamlit app\"\"\"\n",
    "        self.render_sidebar()\n",
    "        self.render_chat_interface()\n",
    "\n",
    "# Example usage section with instructions\n",
    "st.markdown(\"### üéØ Example Questions to Try:\")\n",
    "\n",
    "st.markdown(\"\"\"\n",
    "**üèóÔ∏è AppLayout Examples:**\n",
    "- \"How do I create a DAG for person detection with face recognition?\"\n",
    "- \"What components are needed for vehicle counting in a parking lot?\"\n",
    "- \"Design a pipeline for retail analytics with privacy compliance\"\n",
    "\n",
    "**‚öñÔ∏è ScaleLayout Examples:**\n",
    "- \"How should I deploy 15 cameras across 3 servers for mixed use cases?\"\n",
    "- \"What's the optimal camera grouping strategy for resource efficiency?\"\n",
    "- \"Plan deployment for 50 cameras with different FPS requirements\"\n",
    "\n",
    "**üí° Mixed Questions:**\n",
    "- \"Plan deployment for retail analytics with face recognition across 5 servers\"\n",
    "- \"Create DAG and deployment plan for vehicle counting in multiple parking lots\"\n",
    "- \"Design pipeline and scale for privacy-compliant person detection system\"\n",
    "\"\"\")\n",
    "\n",
    "# Initialize and run the Streamlit chat interface\n",
    "if __name__ == \"__main__\":\n",
    "    chat_app = StreamlitChatInterface()\n",
    "    chat_app.run()\n",
    "'''\n",
    "\n",
    "# Save the Streamlit application to a file\n",
    "with open(\"streamlit_rag_chat.py\", \"w\") as f:\n",
    "    f.write(streamlit_app_code)\n",
    "\n",
    "print(\"‚úÖ Streamlit RAG Chat Application created!\")\n",
    "print(\"üìÅ File saved as: streamlit_rag_chat.py\")\n",
    "print()\n",
    "print(\"üöÄ To run the Streamlit web application:\")\n",
    "print(\"   streamlit run streamlit_rag_chat.py\")\n",
    "print()\n",
    "print(\"üåê The web interface will be available at:\")\n",
    "print(\"   Local URL: http://localhost:8501\")\n",
    "print(\"   Network URL: http://[your-ip]:8501\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4c707c",
   "metadata": {},
   "source": [
    "### üöÄ Running Streamlit from Jupyter Notebook\n",
    "\n",
    "You can run the Streamlit web application directly from this notebook with automatic lifecycle management. The Streamlit server will:\n",
    "\n",
    "- **üü¢ Start automatically** when you run the cell below\n",
    "- **üîó Open in browser** with the correct URL\n",
    "- **üîÑ Auto-restart** if you make changes to the code\n",
    "- **üõë Stop automatically** when the notebook kernel stops or restarts\n",
    "- **üìä Show logs** directly in the notebook output\n",
    "\n",
    "This approach ensures clean resource management and eliminates orphaned processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d6d86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import signal\n",
    "import atexit\n",
    "from IPython.display import display, HTML\n",
    "import webbrowser\n",
    "\n",
    "class StreamlitManager:\n",
    "    \"\"\"Manage Streamlit server lifecycle from Jupyter notebook\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.process = None\n",
    "        self.thread = None\n",
    "        self.is_running = False\n",
    "        self.port = 8501\n",
    "        \n",
    "        # Register cleanup function to run when notebook kernel stops\n",
    "        atexit.register(self.stop_server)\n",
    "    \n",
    "    def start_server(self, app_file=\"streamlit_rag_chat.py\", port=8501, auto_open=True):\n",
    "        \"\"\"Start the Streamlit server\"\"\"\n",
    "        if self.is_running:\n",
    "            print(\"‚ö†Ô∏è  Streamlit server is already running!\")\n",
    "            self._show_server_info()\n",
    "            return\n",
    "        \n",
    "        self.port = port\n",
    "        \n",
    "        # Check if the app file exists\n",
    "        if not os.path.exists(app_file):\n",
    "            print(f\"‚ùå Error: {app_file} not found!\")\n",
    "            print(\"Please run the cell above to create the Streamlit app file first.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"üöÄ Starting Streamlit server...\")\n",
    "        print(f\"üìÅ App file: {app_file}\")\n",
    "        print(f\"üîå Port: {port}\")\n",
    "        \n",
    "        try:\n",
    "            # Start Streamlit in a subprocess\n",
    "            cmd = [\n",
    "                \"streamlit\", \"run\", app_file,\n",
    "                \"--server.port\", str(port),\n",
    "                \"--server.headless\", \"true\",\n",
    "                \"--server.fileWatcherType\", \"none\",\n",
    "                \"--browser.gatherUsageStats\", \"false\"\n",
    "            ]\n",
    "            \n",
    "            self.process = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                preexec_fn=os.setsid  # Create new process group for clean shutdown\n",
    "            )\n",
    "            \n",
    "            self.is_running = True\n",
    "            \n",
    "            # Start thread to monitor output\n",
    "            self.thread = threading.Thread(target=self._monitor_output, daemon=True)\n",
    "            self.thread.start()\n",
    "            \n",
    "            # Wait a moment for server to start\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Show server information\n",
    "            self._show_server_info()\n",
    "            \n",
    "            # Auto-open browser if requested\n",
    "            if auto_open:\n",
    "                self._open_browser()\n",
    "                \n",
    "        except FileNotFoundError:\n",
    "            print(\"‚ùå Error: Streamlit not found!\")\n",
    "            print(\"Please install streamlit: pip install streamlit\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error starting server: {str(e)}\")\n",
    "            self.is_running = False\n",
    "    \n",
    "    def stop_server(self):\n",
    "        \"\"\"Stop the Streamlit server and cleanup\"\"\"\n",
    "        if not self.is_running:\n",
    "            return\n",
    "        \n",
    "        print(\"üõë Stopping Streamlit server...\")\n",
    "        \n",
    "        try:\n",
    "            if self.process:\n",
    "                # Kill the entire process group\n",
    "                os.killpg(os.getpgid(self.process.pid), signal.SIGTERM)\n",
    "                self.process.wait(timeout=5)\n",
    "        except (ProcessLookupError, subprocess.TimeoutExpired):\n",
    "            # Force kill if needed\n",
    "            try:\n",
    "                if self.process:\n",
    "                    os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)\n",
    "            except ProcessLookupError:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Warning during shutdown: {str(e)}\")\n",
    "        \n",
    "        self.process = None\n",
    "        self.is_running = False\n",
    "        print(\"‚úÖ Streamlit server stopped\")\n",
    "    \n",
    "    def restart_server(self, app_file=\"streamlit_rag_chat.py\"):\n",
    "        \"\"\"Restart the Streamlit server\"\"\"\n",
    "        print(\"üîÑ Restarting Streamlit server...\")\n",
    "        self.stop_server()\n",
    "        time.sleep(2)\n",
    "        self.start_server(app_file, self.port)\n",
    "    \n",
    "    def _monitor_output(self):\n",
    "        \"\"\"Monitor Streamlit output in a separate thread\"\"\"\n",
    "        if not self.process:\n",
    "            return\n",
    "            \n",
    "        for line in iter(self.process.stdout.readline, ''):\n",
    "            if not self.is_running:\n",
    "                break\n",
    "            \n",
    "            # Filter out verbose logs, show only important ones\n",
    "            if any(keyword in line.lower() for keyword in ['error', 'warning', 'failed', 'exception']):\n",
    "                print(f\"üìã Streamlit: {line.strip()}\")\n",
    "    \n",
    "    def _show_server_info(self):\n",
    "        \"\"\"Display server information with clickable links\"\"\"\n",
    "        local_url = f\"http://localhost:{self.port}\"\n",
    "        network_url = f\"http://0.0.0.0:{self.port}\"\n",
    "        \n",
    "        print(\"‚úÖ Streamlit server is running!\")\n",
    "        print(\"üåê Access your RAG Chat Interface at:\")\n",
    "        \n",
    "        # Display clickable links in Jupyter\n",
    "        display(HTML(f\"\"\"\n",
    "        <div style=\"background-color: #f0f8ff; padding: 15px; border-radius: 10px; margin: 10px 0;\">\n",
    "            <h4>üöÄ RAG Chat Interface is Ready!</h4>\n",
    "            <p><strong>üåê Local URL:</strong> <a href=\"{local_url}\" target=\"_blank\">{local_url}</a></p>\n",
    "            <p><strong>üì° Network URL:</strong> <a href=\"{network_url}\" target=\"_blank\">{network_url}</a></p>\n",
    "            <p><em>Click the links above to open the chat interface in a new tab</em></p>\n",
    "        </div>\n",
    "        \"\"\"))\n",
    "        \n",
    "        print(f\"üìä Server Status: {'üü¢ Running' if self.is_running else 'üî¥ Stopped'}\")\n",
    "        print(f\"üîå Port: {self.port}\")\n",
    "        print(\"\\nüí° Commands:\")\n",
    "        print(\"   ‚Ä¢ streamlit_manager.stop_server() - Stop the server\")\n",
    "        print(\"   ‚Ä¢ streamlit_manager.restart_server() - Restart the server\")\n",
    "    \n",
    "    def _open_browser(self):\n",
    "        \"\"\"Attempt to open browser automatically\"\"\"\n",
    "        try:\n",
    "            local_url = f\"http://localhost:{self.port}\"\n",
    "            threading.Timer(2.0, lambda: webbrowser.open(local_url)).start()\n",
    "            print(f\"üåê Opening browser to {local_url}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not auto-open browser: {str(e)}\")\n",
    "    \n",
    "    def status(self):\n",
    "        \"\"\"Show current server status\"\"\"\n",
    "        if self.is_running:\n",
    "            self._show_server_info()\n",
    "        else:\n",
    "            print(\"üî¥ Streamlit server is not running\")\n",
    "            print(\"üí° Run streamlit_manager.start_server() to start it\")\n",
    "\n",
    "# Create global manager instance\n",
    "streamlit_manager = StreamlitManager()\n",
    "\n",
    "# Auto-start the server\n",
    "print(\"üéØ Streamlit Manager initialized!\")\n",
    "print(\"üöÄ Starting RAG Chat Interface...\")\n",
    "streamlit_manager.start_server()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ STREAMLIT INTEGRATION READY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"The Streamlit web interface is now running alongside your notebook.\")\n",
    "print(\"Both interfaces share the same RAG backend and knowledge base.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3bd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience functions for easy server management\n",
    "def start_streamlit():\n",
    "    \"\"\"Start the Streamlit server\"\"\"\n",
    "    streamlit_manager.start_server()\n",
    "\n",
    "def stop_streamlit():\n",
    "    \"\"\"Stop the Streamlit server\"\"\"\n",
    "    streamlit_manager.stop_server()\n",
    "\n",
    "def restart_streamlit():\n",
    "    \"\"\"Restart the Streamlit server\"\"\"\n",
    "    streamlit_manager.restart_server()\n",
    "\n",
    "def streamlit_status():\n",
    "    \"\"\"Show Streamlit server status\"\"\"\n",
    "    streamlit_manager.status()\n",
    "\n",
    "print(\"üéõÔ∏è  Streamlit Control Functions Available:\")\n",
    "print(\"   ‚Ä¢ start_streamlit() - Start the web interface\")\n",
    "print(\"   ‚Ä¢ stop_streamlit() - Stop the web interface\") \n",
    "print(\"   ‚Ä¢ restart_streamlit() - Restart the web interface\")\n",
    "print(\"   ‚Ä¢ streamlit_status() - Check server status\")\n",
    "print(\"\\nüí° Example usage:\")\n",
    "print(\"   start_streamlit()  # Start the server\")\n",
    "print(\"   streamlit_status() # Check status\")\n",
    "print(\"   stop_streamlit()   # Stop when done\")\n",
    "stop_streamlit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3983fbe6",
   "metadata": {},
   "source": [
    "## üéâ Summary\n",
    "\n",
    "‚úÖ **RAG Knowledge Base Created**: All ScaleLayout and AppLayout documentation indexed  \n",
    "‚úÖ **Semantic Search Ready**: Vector embeddings enable intelligent retrieval  \n",
    "‚úÖ **Graph Connections Built**: Related concepts linked for comprehensive answers  \n",
    "‚úÖ **Multi-Modal Support**: PDFs, Markdown, code files, and structured data processed  \n",
    "‚úÖ **Production Ready**: Optimized for technical queries and deployment planning  \n",
    "\n",
    "Your RAG system is now ready to power intelligent AppLayout DAG creation and ScaleLayout deployment planning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
