{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1abc9473",
   "metadata": {},
   "source": [
    "# AIOS Model Onboarding Tutorial: Prerequisites & Setup\n",
    "What is AIOS - #DK \n",
    "Overall Goal - #SS\n",
    "    This Video Series\n",
    "    This Jupyter notebook GOAL-Local context goal\n",
    "    \n",
    "Welcome to the AIOS Model Onboarding Tutorial Series! This series of videos and notebooks will guide you through the entire process of integrating and deploying a model on the AIOS platform.\n",
    "\n",
    "## Overview & Prerequisites\n",
    "\n",
    "This first video and notebook will cover the foundational concepts and prerequisites you'll need to get started. We will discuss:\n",
    "\n",
    "- The overall plan for the tutorial series.\n",
    "- The AIOS SDKs and how to choose the right one for your model.\n",
    "- Key concepts like the AIOS Packet and the methods for model onboarding.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, it's important to have a basic understanding of the AIOS SDKs. These SDKs provide the tools and utilities to integrate your model with the AIOS platform.\n",
    "\n",
    "### AIOS Instance SDK\n",
    "Better info/version from #DK\n",
    "\n",
    "The AIOS Instance SDK is a Python library designed as a foundational framework for implementing custom computational logic that operates as servable instances within a modular block architecture. It is general-purpose and supports a wide range of computational workloads, including AI model inference, web application backends, and more.\n",
    "\n",
    "**Core Features:**\n",
    "- Modular execution logic (e.g., preprocessing, inference, postprocessing).\n",
    "- Custom management operations.\n",
    "- Native batching and multiplexing support.\n",
    "- GPU compatibility.\n",
    "- State management utilities.\n",
    "- Custom telemetry and performance metrics.\n",
    "\n",
    "### AIOS SDK Utilities\n",
    "\n",
    "AIOS provides several SDK utilities to simplify the integration process for different types of models. The two main utilities are:\n",
    "\n",
    "- **`aios_llama_cpp`**: A utility wrapper around the `llama_cpp` library to simplify model loading, tokenization, streaming, and batch inference for GGUF models. It supports both single-GPU and multi-GPU execution.\n",
    "- **`aios_transformers`**: A utility wrapper around Hugging Face's `transformers` library, designed for inference with large language models. It supports both single-GPU and multi-GPU execution using tensor parallelism.\n",
    "\n",
    "### Choosing `aios_llama_cpp`\n",
    "\n",
    "For this tutorial, we will be using the `aios_llama_cpp` SDK. It is well-suited for our purposes because it provides a simple and efficient way to serve GGUF models. While it also supports multi-GPU setups, its straightforward approach is ideal for demonstrating the core concepts of model onboarding. The SDK includes the `LLAMAUtils` class for model interaction and the `LLMMetrics` class for performance monitoring.\n",
    "\n",
    "### What is a Packet in AIOS V1?\n",
    "\n",
    "The AIOS Packet is the standard data structure used for communication between different components in the AIOS ecosystem. It is defined as follows:\n",
    "\n",
    "```protobuf\n",
    "message AIOSPacket {\n",
    "    string session_id = 1;  // A unique session_id for each block - useful for stateful inference\n",
    "    uint64 seq_no = 2;      // The unique sequence number if you are doing sequence inference\n",
    "    string data = 4;        // The data payload (optional)\n",
    "    double ts = 5;          // The timestamp in unix epoch format (optional)\n",
    "    string output_ptr = 6;  // The output pointer structure (optional)\n",
    "    repeated FileInfo files = 7; // A list of file structures (optional)\n",
    "}\n",
    "\n",
    "message FileInfo {\n",
    "    string metadata = 1;        // JSON string containing metadata for the file\n",
    "    bytes file_data = 2;        // Raw file content as byte array\n",
    "}\n",
    "```\n",
    "\n",
    "### Methods in AIOS V1 for Model Onboarding\n",
    "\n",
    "The `aios_instance` SDK provides a structured way to integrate your model by implementing a custom class with the following methods:\n",
    "\n",
    "-   `__init__(self, context)`: Initializes the block, loads models, and sets up configurations.\n",
    "-   `on_preprocess(self, packet)`: Pre-processes incoming data packets.\n",
    "-   `on_data(self, preprocessed_entry)`: Performs the main inference logic.\n",
    "-   `management(self, action, data)`: Handles custom management commands.\n",
    "-   `get_muxer(self)`: (Optional) Provides a muxer for packet merging.\n",
    "\n",
    "This structure allows for a clear separation of concerns and makes it easy to integrate custom logic into the AIOS ecosystem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb7b678",
   "metadata": {},
   "source": [
    "# Hello World For AIOS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7417df",
   "metadata": {},
   "source": [
    "#### Code Examples for Core Methods\n",
    "\n",
    "Here are some basic code examples to illustrate the implementation of the core methods in the `aios_instance` SDK. These examples are conceptual and would need to be adapted for a specific use case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0ff0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# __init__: Initialize the block, load models, and set up configurations.\n",
    "def __init__(self, context):\n",
    "    super().__init__(context)\n",
    "    # Example: self.model = load_model('path_to_your_model')\n",
    "    self.context.logger.info(\"Block initialized.\")\n",
    "\n",
    "# on_preprocess: Pre-process incoming data packets.\n",
    "def on_preprocess(self, packet):\n",
    "    # Example: data = json.loads(packet.data)\n",
    "    # return True, [PreProcessResult(packet=packet, extra_data={\"input\": data})]\n",
    "    return True, [PreProcessResult(packet=packet, extra_data={})]\n",
    "\n",
    "# on_data: Perform the main inference logic.\n",
    "def on_data(self, preprocessed_entry):\n",
    "    # Example: result = self.model.predict(preprocessed_entry.extra_data[\"input\"])\n",
    "    # return True, OnDataResult(output={\"prediction\": result})\n",
    "    return True, OnDataResult(output={\"status\": \"ok\"})\n",
    "\n",
    "\n",
    "# management: Handle custom management commands.\n",
    "def management(self, action, data):\n",
    "    if action == 'reload_model':\n",
    "        # self.model.reload()\n",
    "        return {\"status\": \"ok\", \"message\": \"Model reloaded.\"}\n",
    "    return {\"status\": \"error\", \"message\": f\"Unknown action: {action}\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760aefd",
   "metadata": {},
   "source": [
    "Custom  Model , prebuilt images context as Hellow world where to start, recommedation. Formal use these, and proced,skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d86f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f400d57",
   "metadata": {},
   "source": [
    "## Local Setup and Project Structure\n",
    "\n",
    "This section guides you through setting up your local environment and organizing your project files. This structure is designed to be generic and will be used for volume mounting when deploying your model.\n",
    "\n",
    "### 1. Create a Virtual Environment\n",
    "\n",
    "It is highly recommended to use a virtual environment to manage your project's dependencies.\n",
    "\n",
    "```bash\n",
    "python3 -m venv aios_env\n",
    "source aios_env/bin/activate\n",
    "pip install  grpcio grpcio-tools protobuf huggingface-hub\n",
    "```\n",
    "\n",
    "### 2. Recommended Project Structure\n",
    "\n",
    "We recommend the following folder structure for your AIOS model projects. This structure will be assumed for the rest of the tutorial series.\n",
    "\n",
    "```\n",
    "my-aios-model/\n",
    "├── models/\n",
    "│   └── your-model-file.gguf\n",
    "├── component.json\n",
    "├── allocation.json\n",
    "└── inference_client/\n",
    "    ├── service_pb2.py\n",
    "    └── service_pb2_grpc.py\n",
    "```\n",
    "\n",
    "- **models/**: This directory will store your model files (e.g., GGUF files).\n",
    "- **component.json**: The asset definition file for your model.\n",
    "- **allocation.json**: The block allocation configuration.\n",
    "- **inference_client/**: This directory will contain the gRPC client files for sending inference requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c820e2ec",
   "metadata": {},
   "source": [
    "## Installing Hugging Face CLI\n",
    "\n",
    "The Hugging Face CLI is the recommended way to download models for AIOS integration.\n",
    "\n",
    "### Install Hugging Face Hub\n",
    "\n",
    "Run the following command to install the Hugging Face CLI:\n",
    "\n",
    "```bash\n",
    "pip install huggingface-hub\n",
    "```\n",
    "\n",
    "### Create Models Directory\n",
    "\n",
    "```bash\n",
    "mkdir -p models\n",
    "```\n",
    "\n",
    "### Optional: Login to Hugging Face\n",
    "\n",
    "For access to private models and higher download limits:\n",
    "\n",
    "```bash\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76381c9f",
   "metadata": {},
   "source": [
    "## Downloading Models with Hugging Face CLI\n",
    "\n",
    "Now let's download a sample model to test our setup. We'll use the Gemma 2B model as an example.\n",
    "\n",
    "### Download Gemma 2B model using shell \n",
    "```bash\n",
    "huggingface-cli download google/gemma-2b-it --local-dir ./models/gemma-2b-it\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4402b9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🐳 **Docker Images Disclaimer**\n",
    "\n",
    "**Important**: The following tutorials assume you have already built the base Docker images. We will be using these pre-built Docker containers as the foundation for our AIOS model integration.\n",
    "\n",
    "For the next tutorial on onboardiing prebuilt docker images into AIOS Ecosystem, you can refer [02_Part1_onboard_gemma3_llama_cpp](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/tree/tutorial_notebooks/video_tutorial_series/02_Part1_onboard_gemma3_llama_cpp)\n",
    "\n",
    "If you want to build your own custom images with your selected models,refer below,before proceeding for [02_Part2_onboard_custom_llama_cpp](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/tree/tutorial_notebooks/video_tutorial_series/02_Part2_onboard_custom_llama_cpp)\n",
    "\n",
    "- [building aios_instance:v1-gpu base image](https://docs.aigr.id/aios-instance/aios-instance/#building-the-container-image)\n",
    "- [building aios_llama_cpp:v1-gpu base image](https://docs.aigr.id/llm-docs/llm-method-1/#using-the-docker-images-for-building)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3f992",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
