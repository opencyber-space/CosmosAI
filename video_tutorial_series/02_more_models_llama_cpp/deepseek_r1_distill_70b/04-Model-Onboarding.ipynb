{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9727ed0b",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Model Onboarding in AIOS\n",
    "\n",
    "Welcome to this guide on onboarding a model to the OpenOS/AIGr.id platform. This notebook serves as a standalone tutorial that walks you through every step of the process, from registering your model as a digital asset to running inference and managing its lifecycle. We will use the `deepseek_r1_distill_70b` model as our primary example.\n",
    "\n",
    "## The AIOS Ecosystem: A Brief Overview\n",
    "\n",
    "Before we dive in, it's helpful to understand the key components of the AIOS ecosystem. AIOS is designed as a decentralized, modular, and extensible platform for AI development and deployment. Its architecture consists of several core services that work together to manage the lifecycle of AI models and applications. These include:\n",
    "\n",
    "- **Asset DB Registry**: A central catalog for discovering and managing versioned, runnable software components (Assets).\n",
    "- **Resource Allocator**: Responsible for scheduling and allocating resources for running Assets on the network of clusters.\n",
    "- **Block Controller**: Manages the lifecycle of a Block, which is a running instance of an Asset.\n",
    "- **Metrics System**: A comprehensive system for monitoring the health, status, and performance of Blocks.\n",
    "- [AIOS Ecosystem Architecture](https://docs.aigr.id/assets/aios-all-arch.drawio.png)\n",
    "\n",
    "This guide will touch on each of these components as we walk through the model onboarding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307a079e",
   "metadata": {},
   "source": [
    "## Onboarding Process Overview\n",
    "\n",
    "In this guide, we will cover the following steps in detail:\n",
    "\n",
    "1. **Registering an Asset**: We will create a `component.json` file that describes our model and register it with the AIOS Component Registry.\n",
    "2. **Allocating a Block**: We will define the block's configuration in an `allocation.json` file and allocate a block using the AIOS API.\n",
    "3. **Checking Block Status & Metrics**: We will use `curl` commands to check the block's status, health, and metrics.\n",
    "4. **Performing Inference**: We will write a Python script to send an inference request to the block via gRPC.\n",
    "5. **Chat UI with streaming**: We will launch an interactive chat interface to demonstrate streaming capabilities.\n",
    "6. **Cleaning Up**: We will deallocate the block and deregister the asset when done.\n",
    "\n",
    "The following animation provides a high-level overview of the entire model onboarding process.\n",
    "\n",
    "![Model Onboarding Process](block_onboarding.gif)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb22b1",
   "metadata": {},
   "source": [
    "## 1. Registering an Asset\n",
    "\n",
    "First, we need to define our model as an **Asset**. An asset is a static, versioned, and runnable software component that is registered in the AIOS **Asset DB Registry**. The registry acts as a central catalog, allowing developers to discover, share, and reuse assets across the ecosystem. By registering an asset, you are making it available for deployment on the AIOS network.\n",
    "\n",
    "For a deeper dive into how the asset registry works, you can refer to the official documentation:\n",
    "- [Asset DB Registry Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/assets-db-registry/assets-db-registry.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392535b6",
   "metadata": {},
   "source": [
    "### A note on using prebuilt docker images. \n",
    "Below docker images can be a quick start for testing the features of AIOS. For custom model onboarding refer \n",
    "    - [Custom Model Onboarding](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/llm-docs/llm-method-1.md)\n",
    "- `MANAGEMENTMASTER:31280/llama4-scout-17b:v1`\n",
    "- `MANAGEMENTMASTER:31280/gemma3-27b:v1`\n",
    "- `MANAGEMENTMASTER:31280/deepseek-r1-distill-70b:v1`\n",
    "- `MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1`\n",
    "- `MANAGEMENTMASTER:31280/qwen-3-32b-llama-cpp:v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c100e9",
   "metadata": {},
   "source": [
    "Let's look at the `component.json` file for `deepseek_r1_distill_70b`. This file is the asset's manifest, containing all the essential information AIOS needs. It defines:\n",
    "- **`componentId`**: The unique name, version, and release tag for the asset.\n",
    "- **`componentType`**: Specifies that this is a `model` asset.\n",
    "- **`containerRegistryInfo`**: Points to the container image (`deepseek-r1-distill-70b:v1`) and includes metadata like the author and a description.\n",
    "- **`componentMetadata`**: A rich set of details including the model's use case (`chat-completion`), hardware requirements (`gpu`), and performance benchmarks.\n",
    "- **`componentInitData`**: Specifies the default model files to be loaded.\n",
    "\n",
    "This file effectively serves as a comprehensive \"passport\" for the model within the AIOS ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d964688",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat component.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3951ff",
   "metadata": {},
   "source": [
    "Now, let's register this asset with the AIOS Component Registry using a `curl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9161d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/registerComponent -H \"Content-Type: application/json\" -d @./component.json | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97d06b1",
   "metadata": {},
   "source": [
    "## 2. Allocating a Block\n",
    "\n",
    "Now that the asset is registered, we can create a running instance of it. In AIOS, a running instance of an asset is called a **Block**. A Block is the fundamental unit of execution in AIOS, responsible for serving inference requests or running any computational workload defined by an Asset. When you allocate a Block, the AIOS **Resource Allocator** finds a suitable cluster and schedules the Block for execution.\n",
    "\n",
    "For more details, you can refer to the official documentation:\n",
    "- [Block Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1123f959",
   "metadata": {},
   "source": [
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration for the block. Let's examine the key fields in our `allocation.json` for the `deepseek_r1_distill_70b` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `deepseek-r1-distill-70b-block`.\n",
    "- **`blockComponentURI`**: The block should be an instance of the `model.deepseek-r1-distill-70b:1.0.0-stable` asset we registered earlier.\n",
    "- **`blockInitData`**: This section provides initial data to the block, such as the `model_name`.\n",
    "- **`initSettings`**: These are settings for the block's runtime, including `tensor_parallel`, `device` (`cuda`), and `quantization_type`.\n",
    "- **`policyRulesSpec`**: This is a crucial section that defines the chain of policies governing the block's behavior:\n",
    "    - **`clusterAllocator`**: Selects a specific cluster, in this case, `gcp-cluster-2`.\n",
    "    - **`resourceAllocator`**: Assigns the block to a specific node (`wc-gpu-node1`) and GPU (`0`).\n",
    "    - **`loadBalancer`**: Manages how requests are distributed, configured here to cache sessions.\n",
    "    - **`stabilityChecker`**: Defines a health check mechanism to ensure the block is running correctly.\n",
    "    - **`autoscaler`**: Enables autoscaling for the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08485405",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat allocation.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12475415",
   "metadata": {},
   "source": [
    "Now, let's allocate the block using `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b77075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4ec155",
   "metadata": {},
   "source": [
    "## 3. Checking Block Status and Metrics\n",
    "\n",
    "After allocating the block, it's important to check its status to ensure it's running correctly. AIOS provides a comprehensive **Metrics System** that exposes endpoints for health, status, and resource consumption, giving you a complete picture of your block's operational state.\n",
    "\n",
    "For more information on the metrics system, see the documentation:\n",
    "- [Metrics System Overview](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md)\n",
    "\n",
    "Let's check the status, health, and metrics of our block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78588cd-cc62-4e09-93e4-2629cf64d5a5",
   "metadata": {},
   "source": [
    "### Check Block Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ab6db4",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "# Check Block Health - This command verifies the health of the running service within the block.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/health/deepseek-r1-distill-70b-block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ee79f8",
   "metadata": {},
   "source": [
    "### View Block Metrics\n",
    "We can also check the block's metrics, such as CPU and memory usage, by querying the metrics endpoint.\n",
    "\n",
    "- [Block Metrics API](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-services)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59852a",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "### Check Block Metrics (Before Inference)\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/deepseek-r1-distill-70b-block | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06228a3",
   "metadata": {},
   "source": [
    "#### Core Metric Categories\n",
    "\n",
    "##### ðŸ”§ **Runtime Metrics**\n",
    "- **CPU Usage**: Real-time CPU utilization percentage for the block container\n",
    "- **Memory Usage**: Current memory consumption and peak memory usage\n",
    "- **GPU Utilization**: GPU usage percentage and VRAM consumption (for GPU-enabled blocks)\n",
    "\n",
    "##### ðŸ“Š **Performance Metrics**\n",
    "- **Request Latency**: Average,  response times for inference requests\n",
    "- **Throughput**: Requests per second (RPS) and tokens per second (TPS)\n",
    "- **Queue Length**: Number of pending requests in the processing queue\n",
    "\n",
    "##### ðŸ”„ **Operational Metrics**\n",
    "- **Request Count**: Total number of requests processed over time\n",
    "- **Error Rate**: Percentage of failed requests and error types\n",
    "\n",
    "##### ðŸ¥ **Health Metrics**\n",
    "- **Uptime**: Total running time since last restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ba895d",
   "metadata": {},
   "source": [
    "## 4. Performing Inference\n",
    "\n",
    "Now that the block is running, let's perform an inference task. We'll use a Python script to send a request to the block via gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf33b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the necessary libraries for our gRPC client and import them.\n",
    "# We also need to add the `inference_client` directory to our Python path to import the generated gRPC files.\n",
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb890e9",
   "metadata": {},
   "source": [
    "Now, let's define a function to send an inference request to our block. This function will connect to the gRPC server, construct a request packet, and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18c581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config, image_url=None):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "    if image_url:\n",
    "        data = {\n",
    "            \"mode\": \"chat\",\n",
    "            \"gen_params\": generation_config,\n",
    "            \"messages\": [{\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": message},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        data = {\n",
    "            \"mode\": \"chat\",\n",
    "            \"message\": message,\n",
    "            \"gen_params\": generation_config\n",
    "        }\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id=block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        data=json.dumps(data),\n",
    "        ts=time.time()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        \n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(\"Data:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "             print(f\"Data: {response.data}\")\n",
    "\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7b5fad",
   "metadata": {},
   "source": [
    "Finally, let's call our function to get a response from the `deepseek-r1-distill-70b-block`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8daa331",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"deepseek-r1-distill-70b-block\",\n",
    "    session_id=\"session_notebook_chat-1\",\n",
    "    seq_no=1,\n",
    "    message=\"Explain about the architecturual difference between chat and reasoning models?\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f122c819",
   "metadata": {},
   "source": [
    "## 4.a Block Metrics After Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb62385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Block Metrics after running inference\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/deepseek-r1-distill-70b-block | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0ccb2",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat with Streamlit\n",
    "\n",
    "To provide an interactive way to test the model, we will launch a Streamlit application. The following cell will import the necessary function and launch the app, providing a public URL for you to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adbd80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok nest_asyncio websockets > /dev/null\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utils' to the path to find the inference_client\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "sys.path.append(os.path.abspath('../utils/streamlit_app'))\n",
    "\n",
    "from utils import run_streamlit_direct\n",
    "\n",
    "# Define the block_id and grpc_server_address\n",
    "BLOCK_ID = \"deepseek-r1-distill-70b-block\"\n",
    "GRPC_SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "streamlit_url = run_streamlit_direct(BLOCK_ID, GRPC_SERVER_ADDRESS, port=8501)\n",
    "print(f\"Streamlit App URL: {streamlit_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d2a4d",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up\n",
    "\n",
    "After you are finished with the block, it is important to deallocate it to free up resources. You can also deregister the asset if you no longer need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f9694c",
   "metadata": {},
   "source": [
    "### Deallocate the Block\n",
    "This command will stop the running block and release all associated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f53b10",
   "metadata": {},
   "source": [
    "`K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4eb116",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30600/controller/removeBlock/gcp-cluster-2 -H \"Content-Type: application/json\" -d '{\"block_id\": \"deepseek-r1-distill-70b-block\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33cd3a8",
   "metadata": {},
   "source": [
    "### Deregister the Asset\n",
    "If you no longer need the asset in the registry, you can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ab1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/unregisterComponent -H \"Content-Type: application/json\" -d '{\"uri\": \"model.deepseek-r1-distill-70b:1.0.0-stable\"}' | json_pp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
