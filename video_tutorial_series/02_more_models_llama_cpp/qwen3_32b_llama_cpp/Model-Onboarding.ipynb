{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a96f07db",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Model Onboarding in AIOS - Qwen3 32B\n",
    "\n",
    "Welcome to this guide on onboarding the **Qwen3 32B** model to the OpenOS/AIGr.id platform. This notebook serves as a standalone tutorial that walks you through every step of the process, from registering your model as a digital asset to running inference and managing its lifecycle. We will use the `qwen3_32b_llama_cpp` model as our primary example.\n",
    "\n",
    "## The AIOS Ecosystem: A Brief Overview\n",
    "- [AIOS Ecosystem Architecture](https://docs.aigr.id/assets/aios-all-arch.drawio.png)\n",
    "Before we dive in, it's helpful to understand the key components of the AIOS ecosystem. AIOS is designed as a decentralized, modular, and extensible platform for AI development and deployment. Its architecture consists of several core services that work together to manage the lifecycle of AI models and applications. These include:\n",
    "\n",
    "- **Asset DB Registry**: A central catalog for discovering and managing versioned, runnable software components (Assets).\n",
    "- **Resource Allocator**: Responsible for scheduling and allocating resources for running Assets on the network of clusters.\n",
    "- **Cluster Controller**: Manages the lifecycle of a Block, which is a running instance of an Asset.\n",
    "- **Metrics System**: A comprehensive system for monitoring the health, status, and performance of Blocks.\n",
    "\n",
    "This guide will touch on each of these components as we walk through the model onboarding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ecd14",
   "metadata": {},
   "source": [
    "## Onboarding Process Overview\n",
    "\n",
    "In this guide, we will cover the following steps in detail:\n",
    "\n",
    "1. **Registering an Asset**: We will create a `component.json` file that describes our model and register it with the AIOS Component Registry.\n",
    "2. **Allocating a Block**: We will define the block's configuration in an `allocation.json` file and allocate a block using the AIOS API.\n",
    "3. **Checking Block Status & Metrics**: We will use `curl` commands to check the block's status, health, and metrics.\n",
    "4. **Performing Inference**: We will write a Python script to send an inference request to the block via gRPC.\n",
    "5. **Chat UI with streaming**: We will launch an interactive chat interface to demonstrate streaming capabilities.\n",
    "6. **Cleaning Up**: We will deallocate the block and deregister the asset when done.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a768877",
   "metadata": {},
   "source": [
    "The following image provides a high-level overview of the entire model onboarding process, from defining the asset to monitoring the running service.\n",
    "\n",
    "<!-- ![Model Onboarding Process](block_onboarding.gif) -->\n",
    "<img src=\"../02_Part1_onboard_gemma3_llama_cpp/onboarding.png\" alt=\"Model Onboarding Process\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009bf119",
   "metadata": {},
   "source": [
    "### A note on using prebuilt docker images\n",
    "Below docker images can be a quick start for testing the features of AIOS. For custom model onboarding refer:\n",
    "- [Custom Model Onboarding](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/llm-docs/llm-method-1.md)\n",
    "- `MANAGEMENTMASTER:31280/llama4-scout-17b:v1`\n",
    "- `MANAGEMENTMASTER:31280/gemma3-27b:v1`\n",
    "- `MANAGEMENTMASTER:31280/deepseek-r1-distill-70b:v1`\n",
    "- `MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1`\n",
    "- `MANAGEMENTMASTER:31280/qwen-3-32b-llama-cpp:v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3aae6a",
   "metadata": {},
   "source": [
    "## 1. Registering an Asset\n",
    "\n",
    "First, we need to define our model as an **Asset**. An asset is a static, versioned, and runnable software component that is registered in the AIOS **Asset DB Registry**. The registry acts as a central catalog, allowing developers to discover, share, and reuse assets across the ecosystem. By registering an asset, you are making it available for deployment on the AIOS network.\n",
    "\n",
    "For a deeper dive into how the asset registry works, you can refer to the official documentation:\n",
    "- [Asset DB Registry Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/assets-db-registry/assets-db-registry.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933be671",
   "metadata": {},
   "source": [
    "Let's look at the `component.json` file for `qwen3_32b_llama_cpp`. This file is the asset's manifest, containing all the essential information AIOS needs. It defines:\n",
    "- **`componentId`**: The unique name, version, and release tag for the asset (`qwen3-32b-llama_cpp2`).\n",
    "- **`componentType`**: Specifies that this is a `model` asset.\n",
    "- **`containerRegistryInfo`**: Points to the container image and includes metadata like the author and a description.\n",
    "- **`componentMetadata`**: A rich set of details including the model's use case (`chat-completion`), hardware requirements (`gpu`), large context length (128K tokens), performance benchmarks, and known limitations.\n",
    "- **`componentInitData`**: Specifies the default model files to be loaded, such as the GGUF file for the model (`Qwen3-32B-Q8_0.gguf`).\n",
    "\n",
    "This file effectively serves as a comprehensive \"passport\" for the model within the AIOS ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7300ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat component.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20fc0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 21935  100  7666  100 14269   585k  1089k --:--:-- --:--:-- --:--:-- 1785k\n",
      "{\n",
      "   \"error\" : false,\n",
      "   \"payload\" : {\n",
      "      \"__v\" : 0,\n",
      "      \"_id\" : \"689230146e3a468c79698c8e\",\n",
      "      \"componentId\" : {\n",
      "         \"name\" : \"qwen3-32b-llama_cpp2\",\n",
      "         \"releaseTag\" : \"stable\",\n",
      "         \"version\" : \"1.0.0\"\n",
      "      },\n",
      "      \"componentInitData\" : {\n",
      "         \"model_name\" : \"unsloth/Qwen3-32B-GGUF/Qwen3-32B-Q8_0.gguf\",\n",
      "         \"system_message\" : \"You are a helpfull assistant\"\n",
      "      },\n",
      "      \"componentInitParametersProtocol\" : {\n",
      "         \"max_tokens\" : {\n",
      "            \"description\" : \"Maximum token to generate (Should be less than n_ctx)\",\n",
      "            \"max\" : 128000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"min_p\" : {\n",
      "            \"description\" : \"Min Probability\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"temperature\" : {\n",
      "            \"description\" : \"Sampling temperature\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"top_p\" : {\n",
      "            \"description\" : \"Top p\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         }\n",
      "      },\n",
      "      \"componentInitSettings\" : {\n",
      "         \"enable_metrics\" : true,\n",
      "         \"gpu_id\" : 0,\n",
      "         \"model_config\" : {\n",
      "            \"n_ctx\" : 4096,\n",
      "            \"n_gpu_layers\" : -1,\n",
      "            \"n_threads\" : -1,\n",
      "            \"seed\" : 3407,\n",
      "            \"verbose\" : true\n",
      "         },\n",
      "         \"use_gpu\" : true\n",
      "      },\n",
      "      \"componentInitSettingsProtocol\" : {\n",
      "         \"cleanup_check_interval\" : {\n",
      "            \"description\" : \"interval(in seconds) with which check happens for cleanup\",\n",
      "            \"max\" : 10000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"cleanup_enabled\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"To enable the Clean Up of sessions\",\n",
      "            \"type\" : \"boolean\"\n",
      "         },\n",
      "         \"cleanup_session_timeout\" : {\n",
      "            \"description\" : \"Session ID will be removed beyond this timeout this threshold in seconds\",\n",
      "            \"max\" : 18000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"enable_metrics\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"To enable the metrics of AIOS\",\n",
      "            \"type\" : \"boolean\"\n",
      "         },\n",
      "         \"gpu_id\" : {\n",
      "            \"default\" : \"0\",\n",
      "            \"description\" : \"Device to run model on\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"model_config\" : {\n",
      "            \"description\" : \"Model Configuration\",\n",
      "            \"properties\" : {\n",
      "               \"n_ctx\" : {\n",
      "                  \"description\" : \"Max Context for the model (Check the Model supported n_ctx)\",\n",
      "                  \"max\" : 40960,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"n_gpu_layers\" : {\n",
      "                  \"description\" : \"Number of layers to GPU\",\n",
      "                  \"max\" : 1000,\n",
      "                  \"min\" : -1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"n_threads\" : {\n",
      "                  \"description\" : \"Number of CPU Threads \",\n",
      "                  \"max\" : 1000,\n",
      "                  \"min\" : -1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"seed\" : {\n",
      "                  \"description\" : \"Seed (To generate same result for same input)\",\n",
      "                  \"max\" : 10000,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"verbose\" : {\n",
      "                  \"default\" : true,\n",
      "                  \"description\" : \"Verbose to print or not\",\n",
      "                  \"type\" : \"boolean\"\n",
      "               }\n",
      "            },\n",
      "            \"type\" : \"object\"\n",
      "         },\n",
      "         \"use_gpu\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"Enable GPU Inference\",\n",
      "            \"type\" : \"boolean\"\n",
      "         }\n",
      "      },\n",
      "      \"componentInputProtocol\" : {\n",
      "         \"gen_params\" : {\n",
      "            \"description\" : \"Additional generation parameters\",\n",
      "            \"type\" : \"object\"\n",
      "         },\n",
      "         \"message\" : {\n",
      "            \"description\" : \"Input message from the user (for chat mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"prompt\" : {\n",
      "            \"description\" : \"Input prompt (for generate/tokens mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"session_id\" : {\n",
      "            \"default\" : \"default\",\n",
      "            \"description\" : \"Unique identifier for the chat session\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"system_message\" : {\n",
      "            \"description\" : \"System message for chat initialization\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"text\" : {\n",
      "            \"description\" : \"Input text (for embed mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         }\n",
      "      },\n",
      "      \"componentManagementCommandsTemplate\" : {\n",
      "         \"detokenize\" : {\n",
      "            \"args\" : {\n",
      "               \"text\" : {\n",
      "                  \"description\" : \"Tokens to detokenize\",\n",
      "                  \"type\" : \"tokens\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To DeTokenize\"\n",
      "         },\n",
      "         \"device_info\" : {\n",
      "            \"description\" : \"Returns device and model diagnostics\"\n",
      "         },\n",
      "         \"info\" : {\n",
      "            \"description\" : \"Get info\"\n",
      "         },\n",
      "         \"reload_model\" : {\n",
      "            \"args\" : {\n",
      "               \"device\" : {\n",
      "                  \"default\" : \"cuda\",\n",
      "                  \"description\" : \"Device to run model on\",\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"generation_config\" : {\n",
      "                  \"description\" : \"Parameters needed for inference/chat/generation\",\n",
      "                  \"properties\" : {\n",
      "                     \"do_sample\" : {\n",
      "                        \"default\" : true,\n",
      "                        \"description\" : \"do_sample\",\n",
      "                        \"type\" : \"boolean\"\n",
      "                     },\n",
      "                     \"max_new_tokens\" : {\n",
      "                        \"description\" : \"Maximum number of tokens to generate\",\n",
      "                        \"max\" : 40960,\n",
      "                        \"min\" : 1,\n",
      "                        \"type\" : \"number\"\n",
      "                     },\n",
      "                     \"repetition_penalty\" : {\n",
      "                        \"description\" : \"repetition_penalty\",\n",
      "                        \"max\" : 10,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     },\n",
      "                     \"temperature\" : {\n",
      "                        \"description\" : \"Sampling temperature\",\n",
      "                        \"max\" : 1,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     },\n",
      "                     \"top_k\" : {\n",
      "                        \"description\" : \"Topk \",\n",
      "                        \"max\" : 1000,\n",
      "                        \"min\" : 1,\n",
      "                        \"type\" : \"number\"\n",
      "                     },\n",
      "                     \"top_p\" : {\n",
      "                        \"description\" : \"top_p\",\n",
      "                        \"max\" : 1,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     }\n",
      "                  },\n",
      "                  \"type\" : \"object\"\n",
      "               },\n",
      "               \"hf_token\" : {\n",
      "                  \"description\" : \"Hugging Face authentication token for private models or rate limiting\",\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"quantization_type\" : {\n",
      "                  \"default\" : null,\n",
      "                  \"description\" : \"Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization\",\n",
      "                  \"enum\" : [\n",
      "                     \"4bit\",\n",
      "                     \"8bit\",\n",
      "                     \"fp16\",\n",
      "                     \"fp8\"\n",
      "                  ],\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"tensor_parallel\" : {\n",
      "                  \"default\" : true,\n",
      "                  \"description\" : \"Enable tensor parallelism\",\n",
      "                  \"type\" : \"boolean\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Reloads the model with optional new parameters\"\n",
      "         },\n",
      "         \"remove_chat_session\" : {\n",
      "            \"args\" : {\n",
      "               \"session_id\" : {\n",
      "                  \"description\" : \"Session ID to remove\",\n",
      "                  \"type\" : \"string\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Removes a specific chat session\"\n",
      "         },\n",
      "         \"reset\" : {\n",
      "            \"description\" : \"Reset all chat session\"\n",
      "         },\n",
      "         \"set_generation_config\" : {\n",
      "            \"args\" : {\n",
      "               \"generation_config\" : {\n",
      "                  \"description\" : \"New generation configuration\",\n",
      "                  \"type\" : \"object\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Updates generation configuration parameters\"\n",
      "         },\n",
      "         \"set_seed\" : {\n",
      "            \"args\" : {\n",
      "               \"seed\" : {\n",
      "                  \"description\" : \"Set the seed\",\n",
      "                  \"max\" : 10000,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To set seed\"\n",
      "         },\n",
      "         \"tokenize\" : {\n",
      "            \"args\" : {\n",
      "               \"text\" : {\n",
      "                  \"description\" : \"Text to tokenize\",\n",
      "                  \"type\" : \"string\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To Tokenize\"\n",
      "         },\n",
      "         \"update_cleanup_config\" : {\n",
      "            \"args\" : {\n",
      "               \"cleanup_config\" : {\n",
      "                  \"description\" : \"Send enabled: boolen, check_interval: in seconds for keep checcking, session_timeout: remove session beyond this in seconds\",\n",
      "                  \"type\" : \"object\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Update Clean Up Config\"\n",
      "         }\n",
      "      },\n",
      "      \"componentMetadata\" : {\n",
      "         \"architecture\" : {\n",
      "            \"contextLength\" : 128000,\n",
      "            \"parameterCountB\" : 32.8\n",
      "         },\n",
      "         \"biasAndFairness\" : {\n",
      "            \"assessmentSummary\" : \"Internal testing showed some stereotypical associations. The model may reflect biases present in the training data.\",\n",
      "            \"knownBiases\" : [\n",
      "               \"cultural biases towards Western norms\",\n",
      "               \"potential for generating stereotypical content\"\n",
      "            ]\n",
      "         },\n",
      "         \"capabilities\" : {\n",
      "            \"supportsStreaming\" : true\n",
      "         },\n",
      "         \"dataUsagePolicy\" : {\n",
      "            \"policyStatement\" : \"User data is not used for training or improving this model.\",\n",
      "            \"usedForTraining\" : false\n",
      "         },\n",
      "         \"evaluation\" : {\n",
      "            \"benchmarks\" : {\n",
      "               \"HellaSwag\" : {\n",
      "                  \"metric\" : \"10-shot accuracy\",\n",
      "                  \"value\" : 71.1\n",
      "               },\n",
      "               \"MMLU\" : {\n",
      "                  \"metric\" : \"5-shot accuracy\",\n",
      "                  \"value\" : 80.96\n",
      "               },\n",
      "               \"TruthfulQA\" : {\n",
      "                  \"metric\" : \"mc2\",\n",
      "                  \"value\" : 58.63\n",
      "               }\n",
      "            },\n",
      "            \"evaluationData\" : {\n",
      "               \"details\" : \"Evaluated on a standard set of academic benchmarks to assess reasoning, knowledge, and safety.\"\n",
      "            }\n",
      "         },\n",
      "         \"framework\" : \"llamacpp\",\n",
      "         \"hardware\" : \"gpu\",\n",
      "         \"knownLimitations\" : \"May generate plausible but incorrect information. Performance on highly specialized topics may be limited.\",\n",
      "         \"model_path_resolution\" : \"automatic\",\n",
      "         \"provider\" : {\n",
      "            \"modelIdentifier\" : \"Qwen3_32B\",\n",
      "            \"name\" : \"Alibaba\"\n",
      "         },\n",
      "         \"quantization_methods\" : [\n",
      "            \"4bit\",\n",
      "            \"8bit\",\n",
      "            \"fp16\",\n",
      "            \"fp8\"\n",
      "         ],\n",
      "         \"supports_local_models\" : true,\n",
      "         \"supports_quantization\" : true,\n",
      "         \"trainingDetails\" : {\n",
      "            \"trainingData\" : {\n",
      "               \"dataPreprocessing\" : \"Standard cleaning and filtering techniques were applied.\",\n",
      "               \"datasetName\" : \"Proprietary mix of publicly available web data\",\n",
      "               \"datasetSize\" : \"Not disclosed\"\n",
      "            },\n",
      "            \"trainingProcedure\" : {\n",
      "               \"hyperparameters\" : {\n",
      "                  \"batchSize\" : \"Not disclosed\",\n",
      "                  \"learningRate\" : \"Not disclosed\",\n",
      "                  \"optimizer\" : \"AdamW\"\n",
      "               },\n",
      "               \"trainingFramework\" : \"Custom distributed training framework\"\n",
      "            }\n",
      "         },\n",
      "         \"usecase\" : \"chat-completion\"\n",
      "      },\n",
      "      \"componentOutputProtocol\" : {\n",
      "         \"embedding\" : {\n",
      "            \"description\" : \"Text embedding vector (embed mode)\",\n",
      "            \"type\" : \"array\"\n",
      "         },\n",
      "         \"generated\" : {\n",
      "            \"description\" : \"Generated text (generate mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"reply\" : {\n",
      "            \"description\" : \"Chat reply from the model (chat mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"tokens\" : {\n",
      "            \"description\" : \"Generated token IDs (tokens mode)\",\n",
      "            \"type\" : \"array\"\n",
      "         }\n",
      "      },\n",
      "      \"componentParameters\" : {\n",
      "         \"max_tokens\" : 2048,\n",
      "         \"min_p\" : 0.01,\n",
      "         \"temperature\" : 0.2,\n",
      "         \"top_p\" : 0.95\n",
      "      },\n",
      "      \"componentType\" : \"model\",\n",
      "      \"componentURI\" : \"model.qwen3-32b-llama_cpp2:1.0.0-stable\",\n",
      "      \"containerRegistryInfo\" : {\n",
      "         \"componentMode\" : \"aios\",\n",
      "         \"containerImage\" : \"MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1\",\n",
      "         \"containerImageMetadata\" : {\n",
      "            \"author\" : \"llm-team\",\n",
      "            \"description\" : \"AIOS block for chat using Hugging Face Transformers in-process with local model path support for Mistral-ai Magistral Small 2506 model\"\n",
      "         },\n",
      "         \"containerRegistryId\" : \"MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1\"\n",
      "      },\n",
      "      \"createdAt\" : \"2025-08-05T16:23:48.227Z\",\n",
      "      \"lastModifiedAt\" : \"2025-08-05T16:23:48.227Z\",\n",
      "      \"tags\" : [\n",
      "         \"llamacpp\",\n",
      "         \"chat\",\n",
      "         \"huggingface\",\n",
      "         \"Alibaba\",\n",
      "         \"llm\",\n",
      "         \"embedding\",\n",
      "         \"in-process\",\n",
      "         \"local-models\",\n",
      "         \"model-path-resolution\",\n",
      "         \"quantization\",\n",
      "         \"4bit\",\n",
      "         \"8bit\",\n",
      "         \"fp16\"\n",
      "      ]\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Register the asset with the AIOS Component Registry\n",
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/registerComponent -H \"Content-Type: application/json\" -d @./component.json | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2c487",
   "metadata": {},
   "source": [
    "## 2. Allocating a Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aadc02c",
   "metadata": {},
   "source": [
    "Now that the asset is registered, we can create a running instance of it. In AIOS, a running instance of an asset is called a **Block**. A Block is the fundamental unit of execution in AIOS, responsible for serving inference requests or running any computational workload defined by an Asset. When you allocate a Block, the AIOS **Resource Allocator** finds a suitable cluster and schedules the Block for execution.\n",
    "\n",
    "For more details, you can refer to the official documentation:\n",
    "- [Block Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md)\n",
    "- [Block APIs/Services](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-services)\n",
    "\n",
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration for the block. Let's examine the key fields in our `allocation.json` for the `qwen3_32b_llama_cpp` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `qwen3-32b-2`.\n",
    "- **`blockComponentURI`**: The block should be an instance of the `model.qwen3-32b-llama_cpp2:1.0.0-stable` asset we registered earlier.\n",
    "- **`blockInitData`**: This section provides initial data to the block. Here we specify the `model_name` (`Qwen3-32B-Q8_0.gguf`) and the `system_message`.\n",
    "- **`initSettings`**: These are settings for the block's runtime. We have settings for GPU usage, device configuration, large context window (8192 tokens), and model parameters optimized for the Qwen3 architecture.\n",
    "- **`policyRulesSpec`**: This is a crucial section that defines the chain of policies governing the block's behavior:\n",
    "    - **`clusterAllocator`**: Selects a specific cluster, in this case, `gcp-cluster-2`.\n",
    "    - **`resourceAllocator`**: Assigns the block to a specific node (`wc-gpu-node1`) and GPU (`0`).\n",
    "    - **`loadBalancer`**: Manages how requests are distributed, configured here to cache sessions.\n",
    "    - **`stabilityChecker`**: Defines a health check mechanism to ensure the block is running correctly.\n",
    "    - **`autoscaler`**: Enables autoscaling for the block based on GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d423645",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat allocation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c5dd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": {\n",
      "    \"data\": {\n",
      "      \"message\": \"task scheduled in background\",\n",
      "      \"task_id\": \"c46aa9d1-2146-419f-93f4-7f394a455248\"\n",
      "    },\n",
      "    \"success\": true\n",
      "  },\n",
      "  \"success\": true,\n",
      "  \"task_id\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now, let's allocate the block using `curl`.\n",
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df52ea8",
   "metadata": {},
   "source": [
    "## 3. Checking Block Status and Metrics\n",
    "\n",
    "After allocating the block, it's important to check its status to ensure it's running correctly. AIOS provides a comprehensive **Metrics System** that exposes endpoints for health status, and resource consumption, giving you a complete picture of your block's operational state. The Metrics System is designed to be extensible, allowing you to define and expose custom metrics for your applications.\n",
    "\n",
    "For more information on the metrics system, see the documentation:\n",
    "- [Metrics System Overview-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#metrics-system)\n",
    "- [Custom Metrics-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#custom-metrics)\n",
    "\n",
    "Let's check the health status and metrics of our block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7804f",
   "metadata": {},
   "source": [
    "### Check Block Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ecf4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"block_id\": \"qwen3-32b-2\",\n",
      "  \"healthy\": true,\n",
      "  \"instances\": [\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"executor\",\n",
      "      \"reason\": \"executor instance\"\n",
      "    },\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"in-bwep\",\n",
      "      \"lastMetrics\": \"10.151591777801514s ago\"\n",
      "    }\n",
      "  ],\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Health - This command verifies the health of the running service within the block.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/health/qwen3-32b-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d79bee",
   "metadata": {},
   "source": [
    "### Check Block Metrics (Before Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdaf94e",
   "metadata": {},
   "source": [
    "#### Core Metric Categories\n",
    "\n",
    "##### üîß **Runtime Metrics**\n",
    "- **CPU Usage**: Real-time CPU utilization percentage for the block container\n",
    "- **Memory Usage**: Current memory consumption and peak memory usage\n",
    "- **GPU Utilization**: GPU usage percentage and VRAM consumption (for GPU-enabled blocks)\n",
    "\n",
    "##### üìä **Performance Metrics**\n",
    "- **Request Latency**: Average response times for inference requests\n",
    "- **Throughput**: Requests per second (RPS) and tokens per second (TPS)\n",
    "- **Queue Length**: Number of pending requests in the processing queue\n",
    "\n",
    "##### üîÑ **Operational Metrics**\n",
    "- **Request Count**: Total number of requests processed over time\n",
    "- **Error Rate**: Percentage of failed requests and error types\n",
    "\n",
    "##### üè• **Health Metrics**\n",
    "- **Uptime**: Total running time since last restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a975623f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  2589  100  2589    0     0   111k      0 --:--:-- --:--:-- --:--:--  114k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"qwen3-32b-2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"qwen3-32b-2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__qwen3-32b-2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754411057.62712,\n",
      "                  \"tasks_processed_total\" : 0\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 70.0237765312195,\n",
      "               \"uptime_hours\" : 0.0194510490364499,\n",
      "               \"uptime_minutes\" : 1.16706294218699\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"qwen3-32b-2\",\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 1.72,\n",
      "                     \"load1m\" : 4.99,\n",
      "                     \"load5m\" : 2.86,\n",
      "                     \"percent\" : 423.75\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 42041.88,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 91.18,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 39878.12,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 41057.88,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 90.49,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 40862.12,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 9.14,\n",
      "                     \"totalMem\" : 342406.93,\n",
      "                     \"usedMem\" : 31296.16\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-bwep\",\n",
      "               \"llm_active_sessions\" : 0,\n",
      "               \"llm_cpu_utilization\" : 0,\n",
      "               \"llm_gpu_utilization\" : 0,\n",
      "               \"llm_inference_duration_seconds_bucket\" : 0,\n",
      "               \"llm_inference_duration_seconds_count\" : 0,\n",
      "               \"llm_inference_duration_seconds_sum\" : 0,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_memory_usage_bytes\" : 0,\n",
      "               \"llm_prompt_tokens_total\" : 0,\n",
      "               \"llm_prompts_total\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 0,\n",
      "               \"llm_tokens_generated_total\" : 0,\n",
      "               \"llm_tokens_per_second\" : 0,\n",
      "               \"nodeId\" : \"wc-gpu-node2\",\n",
      "               \"nodeKey\" : \"in-bwep__qwen3-32b-2\",\n",
      "               \"timestamp\" : 1754411120.01243,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics before launching the app\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/qwen3-32b-2 | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d69b2",
   "metadata": {},
   "source": [
    "## 4. Performing Inference\n",
    "\n",
    "Now that the block is running, let's perform an inference task. We'll use a Python script to send a request to the block via gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037caa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the necessary libraries for our gRPC client and import them.\n",
    "# We also need to add the `inference_client` directory to our Python path to import the generated gRPC files.\n",
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e7821",
   "metadata": {},
   "source": [
    "Now, let's define a function to send an inference request to our block. This function will connect to the gRPC server, construct a request packet, and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0b32104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "    data = {\n",
    "        \"mode\": \"chat\",\n",
    "        \"message\": message,\n",
    "        \"gen_params\": generation_config\n",
    "    }\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id=block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        data=json.dumps(data),\n",
    "        ts=time.time()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        \n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(\"Data:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "             print(f\"Data: {response.data}\")\n",
    "\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37fc758c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 74.24909567832947s\n",
      "Session ID: session_notebook_qwen3-1\n",
      "Sequence No: 1\n",
      "Data:\n",
      "{\n",
      "  \"reply\": \"<think>\\nOkay, the user is asking about the key features and capabilities of the Qwen3 32B model and how its 128K context window benefits users. Let me start by recalling what I know about Qwen3. It's a large language model developed by Alibaba, and the 32B refers to the number of parameters, which is 32 billion. That's a significant size, so I should mention that it's a large-scale model with strong language understanding and generation capabilities.\\n\\nFirst, the key features. I need to list them out. The user probably wants to know what makes Qwen3 stand out. Let me think: parameter count, training data, multilingual support, reasoning abilities, dialogue understanding, code generation, and maybe some specific optimizations. Also, the 128K context window is a big part of the question, so I should highlight that as a key feature.\\n\\nFor the 128K context window, the benefits would include handling longer texts, better context retention, improved code generation with longer files, and more coherent long-form content. The user might be interested in how this affects real-world applications, like processing long documents or maintaining context in conversations.\\n\\nWait, the user might be a developer or someone looking to use the model for specific tasks. They might want to know how the context window helps in their use case. For example, if they're working with long documents or need the model to remember previous parts of a conversation, the 128K window is a big plus. Also, code generation could benefit from seeing more of the codebase at once.\\n\\nI should also mention the training data cutoff date, which is October 2024. That's important for knowing the model's knowledge up to that point. Multilingual support is another key point, as it makes the model versatile for different languages.\\n\\nReasoning abilities: the user might be interested in how the model handles complex reasoning tasks, like math problems or logical deductions. Dialogue understanding is crucial for chatbots or virtual assistants, so emphasizing that would be good.\\n\\nCode generation is a specific capability that developers care about. The model can generate and understand code in multiple languages, which is a big plus for programming tasks.\\n\\nI need to structure the answer clearly, first listing the key features and then explaining the benefits of the 128K context window. Make sure to use bullet points or sections for readability. Also, avoid technical jargon where possible, but since the user is asking about\"\n",
      "}\n",
      "Timestamp: 1754411143.460831\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"qwen3-32b-2\",\n",
    "    session_id=\"session_notebook_qwen3-1\",\n",
    "    seq_no=1,\n",
    "    message=\"What are the key features and capabilities of the Qwen3 32B model? How does its 128K context window benefit users?\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b82bc7",
   "metadata": {},
   "source": [
    "Let's test the large context capability with a longer prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96f90b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 17.778135776519775s\n",
      "Session ID: session_notebook_qwen3-2\n",
      "Sequence No: 1\n",
      "Data:\n",
      "{\n",
      "  \"reply\": \"<think>\\nOkay, let's see. The user wants me to analyze and summarize the key findings from the given research paper abstract and methodology. The abstract talks about large language models and the challenges with their computational requirements. They mention model compression techniques like quantization, pruning, and knowledge distillation. The methodology section says they evaluated these techniques on models from 7B to 70B parameters across various tasks.\\n\\nFirst, I need to figure out what the research likely discovered. The abstract mentions that the trade-offs between compression ratio, inference speed, and task performance aren't well understood. So the study probably looked into how different compression methods affect these factors across different model sizes and architectures.\\n\\nThe methodology includes 8-bit and 4-bit quantization. I know that lower bit quantization reduces model size but might affect performance. They also did structured and unstructured pruning at various sparsity levels. Structured pruning might be better for hardware compatibility, while unstructured could offer more compression but be harder to deploy. Then there's teacher-student distillation with different temperatures. Higher temperatures might lead to better knowledge transfer but could be computationally expensive.\\n\\nThe key findings would likely compare how each technique performs in terms of compression ratio, speed, and task performance. For example, 4-bit quantization might offer higher compression but with some performance drop, while 8-bit might be a sweet spot. Pruning might vary in effectiveness depending on the model size and task. Distillation could help maintain performance but might require careful tuning of temperature settings.\\n\\nImplications for practical AI deployment would involve which techniques are most effective for different scenarios. Maybe smaller models benefit more from certain methods, while larger models need a combination. Also, the trade-offs between speed and performance would be important for real-world applications where resources are limited.\\n\\nI should also consider the tasks mentioned: reading comprehension, math reasoning, code generation, and multilingual understanding. These tasks have different requirements. For example, math reasoning might be more sensitive to compression than reading comprehension. The study might have found that some tasks are more resilient to compression techniques than others.\\n\\nI need to make sure I cover all the aspects mentioned in the methodology and abstract. The user wants a detailed analysis, so I should break down each technique and how they performed across different model sizes and tasks. Also, the implications for deployment should highlight which methods are practical for different use cases and the trade-offs involved.\\n\\nWait, the user also mentioned \\\"practical AI deployment,\\\" so I should emphasize the real-world applications. For example, if\"\n",
      "}\n",
      "Timestamp: 1754411254.625837\n"
     ]
    }
   ],
   "source": [
    "# Bit longer message\n",
    "long_context_message = \"\"\"\n",
    "Given the following research paper abstract and methodology section, please analyze and summarize the key findings:\n",
    "\n",
    "Abstract: Large language models have revolutionized natural language processing, but their computational requirements remain a significant barrier to widespread deployment. Recent advances in model compression, including quantization, pruning, and knowledge distillation, have shown promise in reducing model size while maintaining performance. However, the trade-offs between compression ratio, inference speed, and task performance are not well understood across different model architectures and sizes.\n",
    "\n",
    "Methodology: We conducted a comprehensive evaluation of compression techniques on models ranging from 7B to 70B parameters across multiple benchmark tasks including reading comprehension, mathematical reasoning, code generation, and multilingual understanding. Our experiments included 8-bit and 4-bit quantization, structured and unstructured pruning at various sparsity levels, and teacher-student distillation with different temperature settings.\n",
    "\n",
    "Please provide a detailed analysis of what this research likely discovered and its implications for practical AI deployment.\n",
    "\"\"\"\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"qwen3-32b-2\",\n",
    "    session_id=\"session_notebook_qwen3-2\",\n",
    "    seq_no=1,\n",
    "    message=long_context_message,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179cda24",
   "metadata": {},
   "source": [
    "## 4.a Block Metrics After Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1b7a68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5897  100  5897    0     0   159k      0 --:--:-- --:--:-- --:--:--  164k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"qwen3-32b-2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"qwen3-32b-2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0.000131607055664062\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__qwen3-32b-2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754411057.62712,\n",
      "                  \"tasks_processed_total\" : 2\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 260.059189558029,\n",
      "               \"uptime_hours\" : 0.0722386637661192,\n",
      "               \"uptime_minutes\" : 4.33431982596715\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"qwen3-32b-2\",\n",
      "               \"end_to_end_count_total\" : 4,\n",
      "               \"end_to_end_fps\" : 0.0562859736448289,\n",
      "               \"end_to_end_latency\" : 17.7664155960083,\n",
      "               \"fps\" : {\n",
      "                  \"average_15m\" : 0.0558409967966206,\n",
      "                  \"average_1m\" : 0.0562859736448289,\n",
      "                  \"average_5m\" : 0.0558409967966206,\n",
      "                  \"current\" : 0.0562859736448289\n",
      "               },\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 1.81,\n",
      "                     \"load1m\" : 1.09,\n",
      "                     \"load5m\" : 2.46,\n",
      "                     \"percent\" : 6.81\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 22990.31,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 92.11,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 58929.69,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 22230.31,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 91.69,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 59689.69,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 12.26,\n",
      "                     \"totalMem\" : 342406.93,\n",
      "                     \"usedMem\" : 41986.48\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-bwep\",\n",
      "               \"latency\" : {\n",
      "                  \"average_15m\" : 17.909126996994,\n",
      "                  \"average_1m\" : 17.7664155960083,\n",
      "                  \"average_5m\" : 17.909126996994,\n",
      "                  \"current\" : 17.7664155960083\n",
      "               },\n",
      "               \"llm_active_sessions\" : 2,\n",
      "               \"llm_active_sessions_rolling\" : {\n",
      "                  \"average_15m\" : 1.5,\n",
      "                  \"average_1m\" : 2,\n",
      "                  \"average_5m\" : 1.5,\n",
      "                  \"current\" : 2\n",
      "               },\n",
      "               \"llm_cpu_utilization\" : 31.9,\n",
      "               \"llm_cpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 15.95,\n",
      "                  \"average_1m\" : 31.9,\n",
      "                  \"average_5m\" : 15.95,\n",
      "                  \"current\" : 31.9\n",
      "               },\n",
      "               \"llm_gpu_utilization\" : 44,\n",
      "               \"llm_gpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 44,\n",
      "                  \"average_1m\" : 44,\n",
      "                  \"average_5m\" : 44,\n",
      "                  \"current\" : 44\n",
      "               },\n",
      "               \"llm_inference_duration_rolling\" : {\n",
      "                  \"average_15m\" : 17.1179035,\n",
      "                  \"average_1m\" : 17.150728,\n",
      "                  \"average_5m\" : 17.1179035,\n",
      "                  \"current\" : 17.150728\n",
      "               },\n",
      "               \"llm_inference_duration_seconds_bucket\" : 2,\n",
      "               \"llm_inference_duration_seconds_count\" : 2,\n",
      "               \"llm_inference_duration_seconds_sum\" : 34.235807,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_input_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 124.5,\n",
      "                  \"average_1m\" : 199,\n",
      "                  \"average_5m\" : 124.5,\n",
      "                  \"current\" : 199\n",
      "               },\n",
      "               \"llm_memory_usage_bytes\" : 2932224000,\n",
      "               \"llm_memory_usage_rolling\" : {\n",
      "                  \"average_15m\" : 2931732480,\n",
      "                  \"average_1m\" : 2932224000,\n",
      "                  \"average_5m\" : 2931732480,\n",
      "                  \"current\" : 2932224000\n",
      "               },\n",
      "               \"llm_output_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 511,\n",
      "                  \"average_1m\" : 511,\n",
      "                  \"average_5m\" : 511,\n",
      "                  \"current\" : 511\n",
      "               },\n",
      "               \"llm_prompt_tokens_total\" : 249,\n",
      "               \"llm_prompts_total\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 0.0700197280503066,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 2,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 2,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 1.52247357368469,\n",
      "               \"llm_tokens_generated_total\" : 1022,\n",
      "               \"llm_tokens_per_second\" : 28.7755750555325,\n",
      "               \"llm_tpot_rolling\" : {\n",
      "                  \"average_15m\" : 0.0350098640251533,\n",
      "                  \"average_1m\" : 0.0347528620941998,\n",
      "                  \"average_5m\" : 0.0350098640251533,\n",
      "                  \"current\" : 0.0347528620941998\n",
      "               },\n",
      "               \"llm_tps_rolling\" : {\n",
      "                  \"average_15m\" : 28.5659081834916,\n",
      "                  \"average_1m\" : 28.7755750555325,\n",
      "                  \"average_5m\" : 28.5659081834916,\n",
      "                  \"current\" : 28.7755750555325\n",
      "               },\n",
      "               \"llm_ttft_rolling\" : {\n",
      "                  \"average_15m\" : 0.761236786842346,\n",
      "                  \"average_1m\" : 0.58492112159729,\n",
      "                  \"average_5m\" : 0.761236786842346,\n",
      "                  \"current\" : 0.58492112159729\n",
      "               },\n",
      "               \"nodeId\" : \"wc-gpu-node2\",\n",
      "               \"nodeKey\" : \"in-bwep__qwen3-32b-2\",\n",
      "               \"on_data_count_total\" : 2,\n",
      "               \"on_data_fps\" : 0.0562958415796827,\n",
      "               \"on_data_latency\" : 17.7633013725281,\n",
      "               \"on_preprocess_count_total\" : 2,\n",
      "               \"on_preprocess_fps\" : 3603.35395189003,\n",
      "               \"on_preprocess_latency\" : 0.000277519226074219,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0.0909090909090909,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0.0909090909090909,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"tasks_processed\" : {\n",
      "                  \"average_15m\" : 1,\n",
      "                  \"average_1m\" : 1,\n",
      "                  \"average_5m\" : 1,\n",
      "                  \"current\" : 1\n",
      "               },\n",
      "               \"timestamp\" : 1754411300.4071,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics after inference\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/qwen3-32b-2 | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91b4746",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat with Streamlit\n",
    "\n",
    "To provide an interactive way to test the model, we will launch a Streamlit application. The following cell will import the necessary function and launch the app, providing a public URL for you to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7dab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok nest_asyncio websockets > /dev/null\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utils' to the path to find the inference_client\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "sys.path.append(os.path.abspath('../utils/streamlit_app'))\n",
    "\n",
    "from utils import run_streamlit_direct\n",
    "\n",
    "# Define the block_id and grpc_server_address\n",
    "BLOCK_ID = \"qwen3-32b-2\"\n",
    "GRPC_SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "streamlit_url = run_streamlit_direct(BLOCK_ID, GRPC_SERVER_ADDRESS, port=8501)\n",
    "print(f\"Streamlit App URL: {streamlit_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00583ab",
   "metadata": {},
   "source": [
    "You can now open the URL above in your browser to interact with the model. Once you are done, you can proceed to the cleanup steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a5205c",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up\n",
    "\n",
    "After you are finished with the block, it is important to deallocate it to free up resources. You can also deregister the asset if you no longer need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ff3aa",
   "metadata": {},
   "source": [
    "### Deallocate the Block\n",
    "This command will stop the running block and release all associated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dea8f79",
   "metadata": {},
   "source": [
    "`K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9a5217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    94  100    67  100    27     12      5  0:00:05  0:00:05 --:--:--    16\n",
      "{\n",
      "   \"data\" : {\n",
      "      \"data\" : \"Action performed\",\n",
      "      \"success\" : true\n",
      "   },\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show the Block in K8s first and then show after deletion!\n",
    "!curl -X POST http://MANAGEMENTMASTER:30600/controller/removeBlock/gcp-cluster-2 -H \"Content-Type: application/json\" -d '{\"block_id\": \"qwen3-32b-2\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ba5220",
   "metadata": {},
   "source": [
    "### Deregister the Asset\n",
    "If you no longer need the asset in the registry, you can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c90eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/unregisterComponent -H \"Content-Type: application/json\" -d '{\"uri\": \"model.qwen3-32b-llama_cpp2:1.0.0-stable\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8e301",
   "metadata": {},
   "source": [
    "## More models sample notebooks can be found [here](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/tree/main/video_tutorial_series/02_more_models_llama_cpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
