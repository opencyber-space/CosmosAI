{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ce89ae",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Model Onboarding in AIOS\n",
    "\n",
    "Welcome to this guide on onboarding a model to the OpenOS/AIGr.id platform. This notebook serves as a standalone tutorial that walks you through every step of the process, from registering your model as a digital asset to running inference and managing its lifecycle. We will use the `llama4_scout` model as our primary example.\n",
    "\n",
    "## The AIOS Ecosystem: A Brief Overview\n",
    "\n",
    "Before we dive in, it's helpful to understand the key components of the AIOS ecosystem. AIOS is designed as a decentralized, modular, and extensible platform for AI development and deployment. Its architecture consists of several core services that work together to manage the lifecycle of AI models and applications. These include:\n",
    "\n",
    "- **Asset DB Registry**: A central catalog for discovering and managing versioned, runnable software components (Assets).\n",
    "- **Resource Allocator**: Responsible for scheduling and allocating resources for running Assets on the network of clusters.\n",
    "- **Block Controller**: Manages the lifecycle of a Block, which is a running instance of an Asset.\n",
    "- **Metrics System**: A comprehensive system for monitoring the health, status, and performance of Blocks.\n",
    "- [AIOS Ecosystem Architecture](https://docs.aigr.id/assets/aios-all-arch.drawio.png)\n",
    "\n",
    "This guide will touch on each of these components as we walk through the model onboarding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c22a0",
   "metadata": {},
   "source": [
    "## Onboarding Process Overview\n",
    "\n",
    "In this guide, we will cover the following steps in detail:\n",
    "\n",
    "1. **Registering an Asset**: We will create a `component.json` file that describes our model and register it with the AIOS Component Registry.\n",
    "2. **Allocating a Block**: We will define the block's configuration in an `allocation.json` file and allocate a block using the AIOS API.\n",
    "3. **Checking Block Status & Metrics**: We will use `curl` commands to check the block's status, health, and metrics.\n",
    "4. **Performing Inference**: We will write a Python script to send an inference request to the block via gRPC.\n",
    "5. **Chat UI with streaming**: We will launch an interactive chat interface to demonstrate streaming capabilities.\n",
    "6. **Cleaning Up**: We will deallocate the block and deregister the asset when done.\n",
    "\n",
    "The following animation provides a high-level overview of the entire model onboarding process.\n",
    "\n",
    "![Model Onboarding Process](block_onboarding.gif)\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5fcb8",
   "metadata": {},
   "source": [
    "## 1. Registering an Asset\n",
    "\n",
    "First, we need to define our model as an **Asset**. An asset is a static, versioned, and runnable software component that is registered in the AIOS **Asset DB Registry**. The registry acts as a central catalog, allowing developers to discover, share, and reuse assets across the ecosystem. By registering an asset, you are making it available for deployment on the AIOS network.\n",
    "\n",
    "For a deeper dive into how the asset registry works, you can refer to the official documentation:\n",
    "- [Asset DB Registry Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/assets-db-registry/assets-db-registry.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf22ee9a",
   "metadata": {},
   "source": [
    "### A note on using prebuilt docker images. \n",
    "Below docker images can be a quick start for testing the features of AIOS. For custom model onboarding refer \n",
    "    - [Custom Model Onboarding](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/llm-docs/llm-method-1.md)\n",
    "- `MANAGEMENTMASTER:31280/llama4-scout-17b:v1`\n",
    "- `MANAGEMENTMASTER:31280/gemma3-27b:v1`\n",
    "- `MANAGEMENTMASTER:31280/deepseek-r1-distill-70b:v1`\n",
    "- `MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1`\n",
    "- `MANAGEMENTMASTER:31280/qwen-3-32b-llama-cpp:v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f3a79",
   "metadata": {},
   "source": [
    "Let's look at the `component.json` file for `llama4_scout`. This file is the asset's manifest, containing all the essential information AIOS needs. It defines:\n",
    "- **`componentId`**: The unique name, version, and release tag for the asset.\n",
    "- **`componentType`**: Specifies that this is a `model` asset.\n",
    "- **`containerRegistryInfo`**: Points to the container image (`llama4-scout-17b:v1`) and includes metadata like the author and a description.\n",
    "- **`componentMetadata`**: A rich set of details including the model's use case (`chat-completion`), hardware requirements (`gpu`), and performance benchmarks.\n",
    "- **`componentInitData`**: Specifies the default model files to be loaded.\n",
    "\n",
    "This file effectively serves as a comprehensive \"passport\" for the model within the AIOS ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42570b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat component.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a908068",
   "metadata": {},
   "source": [
    "Now, let's register this asset with the AIOS Component Registry using a `curl` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bdb8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/registerComponent -H \"Content-Type: application/json\" -d @./component.json | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528c1bb",
   "metadata": {},
   "source": [
    "## 2. Allocating a Block\n",
    "\n",
    "Now that the asset is registered, we can create a running instance of it. In AIOS, a running instance of an asset is called a **Block**. A Block is the fundamental unit of execution in AIOS, responsible for serving inference requests or running any computational workload defined by an Asset. When you allocate a Block, the AIOS **Resource Allocator** finds a suitable cluster and schedules the Block for execution.\n",
    "\n",
    "For more details, you can refer to the official documentation:\n",
    "- [Block Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93afdfff",
   "metadata": {},
   "source": [
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration for the block. Let's examine the key fields in our `allocation.json` for the `llama4_scout` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `llama4-scout-17b-block`.\n",
    "- **`blockComponentURI`**: The block should be an instance of the `model.llama4-scout-17b:1.0.0-stable` asset we registered earlier.\n",
    "- **`blockInitData`**: This section provides initial data to the block, such as the `model_name`.\n",
    "- **`initSettings`**: These are settings for the block's runtime, including `tensor_parallel`, `device` (`cuda`), and `quantization_type`.\n",
    "- **`policyRulesSpec`**: This is a crucial section that defines the chain of policies governing the block's behavior:\n",
    "    - **`clusterAllocator`**: Selects a specific cluster, in this case, `gcp-cluster-2`.\n",
    "    - **`resourceAllocator`**: Assigns the block to a specific node (`wc-gpu-node1`) and GPU (`0`).\n",
    "    - **`loadBalancer`**: Manages how requests are distributed, configured here to cache sessions.\n",
    "    - **`stabilityChecker`**: Defines a health check mechanism to ensure the block is running correctly.\n",
    "    - **`autoscaler`**: Enables autoscaling for the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3bb00b3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"head\": {\n",
      "        \"templateUri\": \"Parser/V1\",\n",
      "        \"parameters\": {}\n",
      "    },\n",
      "    \"body\": {\n",
      "        \"spec\": {\n",
      "            \"values\": {\n",
      "                \"mode\": \"allocate\",\n",
      "                \"blockId\": \"llama4-scout-17b-block\",\n",
      "                \"blockComponentURI\": \"model.llama4-scout-17b:1.0.0-stable\",\n",
      "                \"minInstances\": 1,\n",
      "                \"maxInstances\": 3,\n",
      "                \"blockInitData\": {\n",
      "                    \"model_name\": \"Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00001-of-00003.gguf\",\n",
      "                    \"system_message\": \"You are an expert in analysing video anlaytics critical events like snatching,weapon usage,assualt,sos,accident,theft and provide detailed insights with confidence value and reasoning. You are a helpful assistant.\"\n",
      "\n",
      "                },\n",
      "                \"initSettings\": {\n",
      "                    \"tensor_parallel\": true,\n",
      "                    \"device\": \"cuda\",\n",
      "                    \"quantization_type\": \"int8\",\n",
      "                    \"cleanup_enabled\":  true,\n",
      "                    \"cleanup_check_interval\": 60,\n",
      "                    \"cleanup_session_timeout\": 1800,\n",
      "                    \"generation_config\": {\n",
      "                        \"max_new_tokens\": 2048,\n",
      "                        \"temperature\": 0.6,\n",
      "                        \"top_k\": 50,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"repetition_penalty\": 1.1,\n",
      "                        \"do_sample\": true\n",
      "                    }\n",
      "                },\n",
      "                \"policyRulesSpec\": [\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"clusterAllocator\",\n",
      "                            \"policyRuleURI\": \"cluster-selector:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"filter\": {\n",
      "                                    \"clusterQuery\": {\n",
      "                                        \"variable\": \"id\",\n",
      "                                        \"operator\": \"==\",\n",
      "                                        \"value\": \"gcp-cluster-2\"\n",
      "                                    }\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"max_candidates\": 2\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"resourceAllocator\",\n",
      "                            \"policyRuleURI\": \"allocator:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"allocation_data\": {\n",
      "                                    \"node_id\": \"wc-gpu-node7\",\n",
      "                                    \"gpus\": [0,1]\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"selection_mode\": \"balanced\"\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"loadBalancer\",\n",
      "                            \"policyRuleURI\": \"load_balancer:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"cache_sessions\": true\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"session_cache_size\": 2000\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"stabilityChecker\",\n",
      "                            \"policyRuleURI\": \"health_checker:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"unhealthy_threshold\": 2\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"check_interval_sec\": 10\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"autoscaler\",\n",
      "                            \"policyRuleURI\": \"autoscaler:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"target_gpu_utilization\": 0.8\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"scale_up_cooldown\": 45,\n",
      "                                \"scale_down_cooldown\": 90\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat allocation.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb7e9b-8c2b-472f-8983-b2875b923bcc",
   "metadata": {},
   "source": [
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration for the block. Let's examine the key fields in our `allocation.json` for the `llama4_scout` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `llama4-scout-17b-block`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b75c31",
   "metadata": {},
   "source": [
    "Now, let's allocate the block using `curl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74128745",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a923e6-247a-4964-8aa2-a7baaf254fcf",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 3. Checking Block Status and Metrics\n",
    "\n",
    "After allocating the block, it's important to check its status to ensure it's running correctly. AIOS provides a comprehensive **Metrics System** that exposes endpoints for health status, and resource consumption, giving you a complete picture of your block's operational state. The Metrics System is designed to be extensible, allowing you to define and expose custom metrics for your applications.\n",
    "\n",
    "For more information on the metrics system, see the documentation:\n",
    "- [Metrics System Overview-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#metrics-system)\n",
    "- [Custom Metrics-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#custom-metrics)\n",
    "\n",
    "Let's check the health status and metrics of our block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd4b740",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"block_id\": \"llama4-scout-17b-block\",\n",
      "  \"healthy\": true,\n",
      "  \"instances\": [\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"executor\",\n",
      "      \"reason\": \"executor instance\"\n",
      "    },\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"in-6nvx\",\n",
      "      \"lastMetrics\": \"29.046629428863525s ago\"\n",
      "    }\n",
      "  ],\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Health - This command verifies the health of the running service within the block.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/health/llama4-scout-17b-block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069f434",
   "metadata": {},
   "source": [
    "### View Block Metrics\n",
    "We can also check the block's metrics, such as CPU and memory usage, by querying the metrics endpoint.\n",
    "\n",
    "- [Block Metrics API](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-services)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3fe26b",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5456  100  5456    0     0   197k      0 --:--:-- --:--:-- --:--:--  204k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"llama4-scout-17b-block\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"llama4-scout-17b-block\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0.000114202499389648\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__llama4-scout-17b-block\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1753467250.76577,\n",
      "                  \"tasks_processed_total\" : 191\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 944474.598779917,\n",
      "               \"uptime_hours\" : 262.354055216644,\n",
      "               \"uptime_minutes\" : 15741.2433129986\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"llama4-scout-17b-block\",\n",
      "               \"end_to_end_count_total\" : 388,\n",
      "               \"end_to_end_fps\" : 0.159217940503877,\n",
      "               \"end_to_end_latency\" : 6.28069925308228,\n",
      "               \"fps\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 0.2,\n",
      "                     \"load1m\" : 0.19,\n",
      "                     \"load5m\" : 0.26,\n",
      "                     \"percent\" : 0.1\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 19151.88,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 73.75,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 62768.12,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 19815.88,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 72.13,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 62104.12,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 0.22,\n",
      "                     \"totalMem\" : 342407.27,\n",
      "                     \"usedMem\" : 745.41\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-6nvx\",\n",
      "               \"latency\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_active_sessions\" : 0,\n",
      "               \"llm_active_sessions_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_cpu_utilization\" : 0.1,\n",
      "               \"llm_cpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_gpu_utilization\" : 42,\n",
      "               \"llm_gpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_inference_duration_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_inference_duration_seconds_bucket\" : 189,\n",
      "               \"llm_inference_duration_seconds_count\" : 189,\n",
      "               \"llm_inference_duration_seconds_sum\" : 526.581184,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_input_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_memory_usage_bytes\" : 3113316352,\n",
      "               \"llm_memory_usage_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_output_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_prompt_tokens_total\" : 13459,\n",
      "               \"llm_prompts_total\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 4.92584345987567,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 189,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 189,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 28.5768268108368,\n",
      "               \"llm_tokens_generated_total\" : 18702,\n",
      "               \"llm_tokens_per_second\" : 41.1067418748899,\n",
      "               \"llm_tpot_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_tps_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_ttft_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"nodeId\" : \"wc-gpu-node7\",\n",
      "               \"nodeKey\" : \"in-6nvx__llama4-scout-17b-block\",\n",
      "               \"on_data_count_total\" : 189,\n",
      "               \"on_data_fps\" : 0.159716871527866,\n",
      "               \"on_data_latency\" : 6.26107931137085,\n",
      "               \"on_preprocess_count_total\" : 194,\n",
      "               \"on_preprocess_fps\" : 3100.0029563932,\n",
      "               \"on_preprocess_latency\" : 0.000322580337524414,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"tasks_processed\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"timestamp\" : 1754411729.24703,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics (Before Inference)\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/llama4-scout-17b-block | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a8e2b5",
   "metadata": {},
   "source": [
    "#### Core Metric Categories\n",
    "\n",
    "##### 🔧 **Runtime Metrics**\n",
    "- **CPU Usage**: Real-time CPU utilization percentage for the block container\n",
    "- **Memory Usage**: Current memory consumption and peak memory usage\n",
    "- **GPU Utilization**: GPU usage percentage and VRAM consumption (for GPU-enabled blocks)\n",
    "\n",
    "##### 📊 **Performance Metrics**\n",
    "- **Request Latency**: Average,  response times for inference requests\n",
    "- **Throughput**: Requests per second (RPS) and tokens per second (TPS)\n",
    "- **Queue Length**: Number of pending requests in the processing queue\n",
    "\n",
    "##### 🔄 **Operational Metrics**\n",
    "- **Request Count**: Total number of requests processed over time\n",
    "- **Error Rate**: Percentage of failed requests and error types\n",
    "\n",
    "##### 🏥 **Health Metrics**\n",
    "- **Uptime**: Total running time since last restart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3df8e",
   "metadata": {},
   "source": [
    "## 4. Performing Inference\n",
    "\n",
    "Now that the block is running, let's perform an inference task. We'll use a Python script to send a request to the block via gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdd36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: grpcio-tools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: protobuf in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (6.31.1)\n",
      "Requirement already satisfied: setuptools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (from grpcio-tools) (80.9.0)\n"
     ]
    }
   ],
   "source": [
    "# First, let's install the necessary libraries for our gRPC client and import them.\n",
    "# We also need to add the `inference_client` directory to our Python path to import the generated gRPC files.\n",
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8cf29",
   "metadata": {},
   "source": [
    "Now, let's define a function to send an inference request to our block. This function will connect to the gRPC server, construct a request packet, and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fd8fd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "    data = {\n",
    "        \"mode\": \"chat\",\n",
    "        \"message\": message,\n",
    "        \"gen_params\": generation_config\n",
    "    }\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id=block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        data=json.dumps(data),\n",
    "        ts=time.time()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        \n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(\"Data:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "             print(f\"Data: {response.data}\")\n",
    "\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a109c6",
   "metadata": {},
   "source": [
    "Finally, let's call our function to get a response from the `llama4-scout-17b-block`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cf07a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 12.428791046142578s\n",
      "Session ID: session_notebook_chat\n",
      "Sequence No: 1\n",
      "Data:\n",
      "{\n",
      "  \"reply\": \"DeepSeek LLM and Qwen LLM are two large language models that have gained significant attention in the field of natural language processing. Here's a comparison of the two models:\\n\\n**Overview**\\n\\n* **DeepSeek LLM**: Developed by DeepSeek, a company that focuses on creating AI models for various applications. DeepSeek LLM is a transformer-based language model designed to handle a wide range of NLP tasks.\\n* **Qwen LLM**: Developed by Alibaba's DAMO Academy, Qwen LLM is a large language model that has achieved state-of-the-art results in various NLP benchmarks.\\n\\n**Architecture**\\n\\n* **DeepSeek LLM**: DeepSeek LLM is based on a transformer architecture with a decoder-only design. It uses a combination of self-attention mechanisms and feed-forward neural networks to process input sequences.\\n* **Qwen LLM**: Qwen LLM also employs a transformer architecture, but with an encoder-decoder design. It utilizes a multi-head attention mechanism and a feed-forward neural network to generate output sequences.\\n\\n**Key Features**\\n\\n* **DeepSeek LLM**:\\n\\t+ Supports multiple languages, including English, Chinese, and others.\\n\\t+ Can handle a wide range of NLP tasks, such as text classification, sentiment analysis, and question answering.\\n\\t+ Uses a unique training objective called \\\"masked language modeling\\\" to improve its performance.\\n* **Qwen LLM**:\\n\\t+ Specifically designed for tasks that require generating coherent and context-specific text, such as text summarization and dialogue generation.\\n\\t+ Utilizes a technique called \\\"sparse attention\\\" to reduce computational costs and improve efficiency.\\n\\t+ Trained on a large dataset of text from various sources, including books, articles, and websites.\\n\\n**Performance Comparison**\\n\\n* **DeepSeek LLM**: DeepSeek LLM has achieved competitive results on various NLP benchmarks, including GLUE, SQuAD, and IMDB.\\n* **Qwen LLM**: Qwen LLM has achieved state-of-the-art results on several NLP benchmarks, including the popular Chinese NLP benchmark, CLUE.\\n\\n**Training Data and Parameters**\\n\\n* **DeepSeek LLM**: Trained on a dataset of approximately 1.5T tokens, with 175 billion parameters.\\n* **Qwen LLM**: Trained on a dataset of approximately 2T tokens, with 200 billion parameters.\\n\\n**Confidence Value and Reasoning**\\n\\nBased on the available information, I would assign a confidence value of 0.8 to the comparison of DeepSeek LLM and Qwen LLM. My reasoning is as follows:\\n\\n* Both models have demonstrated impressive performance on various NLP benchmarks, but the specific tasks and\"\n",
      "}\n",
      "Timestamp: 1754411783.9769084\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"llama4-scout-17b-block\",\n",
    "    session_id=\"session_notebook_chat\",\n",
    "    seq_no=1,\n",
    "    message=\"Compare and Contrast Deepseek LLM and Qwen LLM Models?\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd9cf8f",
   "metadata": {},
   "source": [
    "## 4.a Block Metrics After Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e17f561c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5541  100  5541    0     0   157k      0 --:--:-- --:--:-- --:--:--  159k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"llama4-scout-17b-block\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"llama4-scout-17b-block\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0.000103235244750977\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__llama4-scout-17b-block\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1753467250.76577,\n",
      "                  \"tasks_processed_total\" : 192\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 944559.612567902,\n",
      "               \"uptime_hours\" : 262.37767015775,\n",
      "               \"uptime_minutes\" : 15742.660209465\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"llama4-scout-17b-block\",\n",
      "               \"end_to_end_count_total\" : 388,\n",
      "               \"end_to_end_fps\" : 0.159217940503877,\n",
      "               \"end_to_end_latency\" : 6.28069925308228,\n",
      "               \"fps\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 0.19,\n",
      "                     \"load1m\" : 0.15,\n",
      "                     \"load5m\" : 0.23,\n",
      "                     \"percent\" : 17.73\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 19151.88,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 210.58,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 62768.12,\n",
      "                        \"utilization\" : 39\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 19815.88,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 218.07,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 62104.12,\n",
      "                        \"utilization\" : 42\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 0.22,\n",
      "                     \"totalMem\" : 342407.27,\n",
      "                     \"usedMem\" : 745.41\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-6nvx\",\n",
      "               \"latency\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_active_sessions\" : 1,\n",
      "               \"llm_active_sessions_rolling\" : {\n",
      "                  \"average_15m\" : 1,\n",
      "                  \"average_1m\" : 1,\n",
      "                  \"average_5m\" : 1,\n",
      "                  \"current\" : 1\n",
      "               },\n",
      "               \"llm_cpu_utilization\" : 0.1,\n",
      "               \"llm_cpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_gpu_utilization\" : 42,\n",
      "               \"llm_gpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_inference_duration_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_inference_duration_seconds_bucket\" : 189,\n",
      "               \"llm_inference_duration_seconds_count\" : 189,\n",
      "               \"llm_inference_duration_seconds_sum\" : 526.581184,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_input_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_memory_usage_bytes\" : 3113316352,\n",
      "               \"llm_memory_usage_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_output_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_prompt_tokens_total\" : 13459,\n",
      "               \"llm_prompts_total\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 189,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 4.92584345987567,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 190,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 190,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 28.7202413082123,\n",
      "               \"llm_tokens_generated_total\" : 18702,\n",
      "               \"llm_tokens_per_second\" : 41.1067418748899,\n",
      "               \"llm_tpot_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_tps_rolling\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_ttft_rolling\" : {\n",
      "                  \"average_15m\" : 0.143414497375488,\n",
      "                  \"average_1m\" : 0.143414497375488,\n",
      "                  \"average_5m\" : 0.143414497375488,\n",
      "                  \"current\" : 0.143414497375488\n",
      "               },\n",
      "               \"nodeId\" : \"wc-gpu-node7\",\n",
      "               \"nodeKey\" : \"in-6nvx__llama4-scout-17b-block\",\n",
      "               \"on_data_count_total\" : 189,\n",
      "               \"on_data_fps\" : 0.159716871527866,\n",
      "               \"on_data_latency\" : 6.26107931137085,\n",
      "               \"on_preprocess_count_total\" : 195,\n",
      "               \"on_preprocess_fps\" : 2605.15776397516,\n",
      "               \"on_preprocess_latency\" : 0.000383853912353516,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"tasks_processed\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"timestamp\" : 1754411789.27803,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics after running inference\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/llama4-scout-17b-block | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a1a5dc",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat with Streamlit\n",
    "\n",
    "To provide an interactive way to test the model, we will launch a Streamlit application. The following cell will import the necessary function and launch the app, providing a public URL for you to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798787f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok nest_asyncio websockets > /dev/null\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utils' to the path to find the inference_client\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "sys.path.append(os.path.abspath('../utils/streamlit_app'))\n",
    "\n",
    "from utils import run_streamlit_direct\n",
    "\n",
    "# Define the block_id and grpc_server_address\n",
    "BLOCK_ID = \"llama4-scout-17b-block\"\n",
    "GRPC_SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "streamlit_url = run_streamlit_direct(BLOCK_ID, GRPC_SERVER_ADDRESS, port=8501)\n",
    "print(f\"Streamlit App URL: {streamlit_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9bc0da",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up\n",
    "\n",
    "After you are finished with the block, it is important to deallocate it to free up resources. You can also deregister the asset if you no longer need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d9540",
   "metadata": {},
   "source": [
    "### Deallocate the Block\n",
    "This command will stop the running block and release all associated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503b238",
   "metadata": {},
   "source": [
    "`K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5f52a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30600/controller/removeBlock/gcp-cluster-2 -H \"Content-Type: application/json\" -d '{\"block_id\": \"llama4-scout-17b-block\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c05fb5",
   "metadata": {},
   "source": [
    "### Deregister the Asset\n",
    "If you no longer need the asset in the registry, you can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029d0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/unregisterComponent -H \"Content-Type: application/json\" -d '{\"uri\": \"model.llama4-scout-17b:1.0.0-stable\"}' | json_pp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
