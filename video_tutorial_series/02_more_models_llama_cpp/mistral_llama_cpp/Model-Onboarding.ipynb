{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f8749af",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Model Onboarding in AIOS - Mistral Magistral Small 2506\n",
    "\n",
    "Welcome to this guide on onboarding the **Mistral Magistral Small 2506** model to the OpenOS/AIGr.id platform. This notebook serves as a standalone tutorial that walks you through every step of the process, from registering your model as a digital asset to running inference and managing its lifecycle. We will use the `mistral_llama_cpp` model as our primary example.\n",
    "\n",
    "## The AIOS Ecosystem: A Brief Overview\n",
    "- [AIOS Ecosystem Architecture](https://docs.aigr.id/assets/aios-all-arch.drawio.png)\n",
    "Before we dive in, it's helpful to understand the key components of the AIOS ecosystem. AIOS is designed as a decentralized, modular, and extensible platform for AI development and deployment. Its architecture consists of several core services that work together to manage the lifecycle of AI models and applications. These include:\n",
    "\n",
    "- **Asset DB Registry**: A central catalog for discovering and managing versioned, runnable software components (Assets).\n",
    "- **Resource Allocator**: Responsible for scheduling and allocating resources for running Assets on the network of clusters.\n",
    "- **Cluster Controller**: Manages the lifecycle of a Block, which is a running instance of an Asset.\n",
    "- **Metrics System**: A comprehensive system for monitoring the health, status, and performance of Blocks.\n",
    "\n",
    "This guide will touch on each of these components as we walk through the model onboarding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d130786",
   "metadata": {},
   "source": [
    "## Onboarding Process Overview\n",
    "\n",
    "In this guide, we will cover the following steps in detail:\n",
    "\n",
    "1. **Registering an Asset**: We will create a `component.json` file that describes our model and register it with the AIOS Component Registry.\n",
    "2. **Allocating a Block**: We will define the block's configuration in an `allocation.json` file and allocate a block using the AIOS API.\n",
    "3. **Checking Block Status & Metrics**: We will use `curl` commands to check the block's status, health, and metrics.\n",
    "4. **Performing Inference**: We will write a Python script to send an inference request to the block via gRPC.\n",
    "5. **Chat UI with streaming**: We will launch an interactive chat interface to demonstrate streaming capabilities.\n",
    "6. **Cleaning Up**: We will deallocate the block and deregister the asset when done.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a23fd",
   "metadata": {},
   "source": [
    "The following image provides a high-level overview of the entire model onboarding process, from defining the asset to monitoring the running service.\n",
    "\n",
    "<!-- ![Model Onboarding Process](block_onboarding.gif) -->\n",
    "<img src=\"../02_Part1_onboard_gemma3_llama_cpp/onboarding.png\" alt=\"Model Onboarding Process\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df20e9",
   "metadata": {},
   "source": [
    "### A note on using prebuilt docker images\n",
    "Below docker images can be a quick start for testing the features of AIOS. For custom model onboarding refer:\n",
    "- [Custom Model Onboarding](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/llm-docs/llm-method-1.md)\n",
    "- `MANAGEMENTMASTER:31280/llama4-scout-17b:v1`\n",
    "- `MANAGEMENTMASTER:31280/gemma3-27b:v1`\n",
    "- `MANAGEMENTMASTER:31280/deepseek-r1-distill-70b:v1`\n",
    "- `MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1`\n",
    "- `MANAGEMENTMASTER:31280/qwen-3-32b-llama-cpp:v1`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6ddee",
   "metadata": {},
   "source": [
    "## 1. Registering an Asset\n",
    "\n",
    "First, we need to define our model as an **Asset**. An asset is a static, versioned, and runnable software component that is registered in the AIOS **Asset DB Registry**. The registry acts as a central catalog, allowing developers to discover, share, and reuse assets across the ecosystem. By registering an asset, you are making it available for deployment on the AIOS network.\n",
    "\n",
    "For a deeper dive into how the asset registry works, you can refer to the official documentation:\n",
    "- [Asset DB Registry Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/assets-db-registry/assets-db-registry.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c50dfbd",
   "metadata": {},
   "source": [
    "Let's look at the `component.json` file for `mistral_llama_cpp`. This file is the asset's manifest, containing all the essential information AIOS needs. It defines:\n",
    "- **`componentId`**: The unique name, version, and release tag for the asset (`magistral-small-2506-llama_cpp2`).\n",
    "- **`componentType`**: Specifies that this is a `model` asset.\n",
    "- **`containerRegistryInfo`**: Points to the container image (`magistral-small-2506-llama-cpp:v1`) and includes metadata like the author and a description.\n",
    "- **`componentMetadata`**: A rich set of details including the model's use case (`chat-completion`), hardware requirements (`gpu`), performance benchmarks, and known limitations.\n",
    "- **`componentInitData`**: Specifies the default model files to be loaded, such as the GGUF file for the model (`Magistral-Small-2506_Q8_0.gguf`).\n",
    "\n",
    "This file effectively serves as a comprehensive \"passport\" for the model within the AIOS ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f6fec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat component.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ff444a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 22105  100  7751  100 14354   509k   943k --:--:-- --:--:-- --:--:-- 1541k\n",
      "{\n",
      "   \"error\" : false,\n",
      "   \"payload\" : {\n",
      "      \"__v\" : 0,\n",
      "      \"_id\" : \"68922e1e6e3a468c79698c89\",\n",
      "      \"componentId\" : {\n",
      "         \"name\" : \"magistral-small-2506-llama_cpp2\",\n",
      "         \"releaseTag\" : \"stable\",\n",
      "         \"version\" : \"1.0.0\"\n",
      "      },\n",
      "      \"componentInitData\" : {\n",
      "         \"model_name\" : \"mistralai/Magistral-Small-2506_gguf/Magistral-Small-2506_Q8_0.gguf\",\n",
      "         \"system_message\" : \"You are a helpfull assistant\"\n",
      "      },\n",
      "      \"componentInitParametersProtocol\" : {\n",
      "         \"max_tokens\" : {\n",
      "            \"description\" : \"Maximum token to generate (Should be less than n_ctx)\",\n",
      "            \"max\" : 128000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"min_p\" : {\n",
      "            \"description\" : \"Min Probability\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"temperature\" : {\n",
      "            \"description\" : \"Sampling temperature\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"top_p\" : {\n",
      "            \"description\" : \"Top p\",\n",
      "            \"max\" : 1,\n",
      "            \"min\" : 0,\n",
      "            \"type\" : \"number\"\n",
      "         }\n",
      "      },\n",
      "      \"componentInitSettings\" : {\n",
      "         \"enable_metrics\" : true,\n",
      "         \"gpu_id\" : 0,\n",
      "         \"model_config\" : {\n",
      "            \"n_ctx\" : 4096,\n",
      "            \"n_gpu_layers\" : -1,\n",
      "            \"n_threads\" : -1,\n",
      "            \"seed\" : 3407,\n",
      "            \"verbose\" : true\n",
      "         },\n",
      "         \"use_gpu\" : true\n",
      "      },\n",
      "      \"componentInitSettingsProtocol\" : {\n",
      "         \"cleanup_check_interval\" : {\n",
      "            \"description\" : \"interval(in seconds) with which check happens for cleanup\",\n",
      "            \"max\" : 10000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"cleanup_enabled\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"To enable the Clean Up of sessions\",\n",
      "            \"type\" : \"boolean\"\n",
      "         },\n",
      "         \"cleanup_session_timeout\" : {\n",
      "            \"description\" : \"Session ID will be removed beyond this timeout this threshold in seconds\",\n",
      "            \"max\" : 18000,\n",
      "            \"min\" : 1,\n",
      "            \"type\" : \"number\"\n",
      "         },\n",
      "         \"enable_metrics\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"To enable the metrics of AIOS\",\n",
      "            \"type\" : \"boolean\"\n",
      "         },\n",
      "         \"gpu_id\" : {\n",
      "            \"default\" : \"0\",\n",
      "            \"description\" : \"Device to run model on\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"model_config\" : {\n",
      "            \"description\" : \"Model Configuration\",\n",
      "            \"properties\" : {\n",
      "               \"n_ctx\" : {\n",
      "                  \"description\" : \"Max Context for the model (Check the Model supported n_ctx)\",\n",
      "                  \"max\" : 40960,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"n_gpu_layers\" : {\n",
      "                  \"description\" : \"Number of layers to GPU\",\n",
      "                  \"max\" : 1000,\n",
      "                  \"min\" : -1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"n_threads\" : {\n",
      "                  \"description\" : \"Number of CPU Threads \",\n",
      "                  \"max\" : 1000,\n",
      "                  \"min\" : -1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"seed\" : {\n",
      "                  \"description\" : \"Seed (To generate same result for same input)\",\n",
      "                  \"max\" : 10000,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               },\n",
      "               \"verbose\" : {\n",
      "                  \"default\" : true,\n",
      "                  \"description\" : \"Verbose to print or not\",\n",
      "                  \"type\" : \"boolean\"\n",
      "               }\n",
      "            },\n",
      "            \"type\" : \"object\"\n",
      "         },\n",
      "         \"use_gpu\" : {\n",
      "            \"default\" : true,\n",
      "            \"description\" : \"Enable GPU Inference\",\n",
      "            \"type\" : \"boolean\"\n",
      "         }\n",
      "      },\n",
      "      \"componentInputProtocol\" : {\n",
      "         \"gen_params\" : {\n",
      "            \"description\" : \"Additional generation parameters\",\n",
      "            \"type\" : \"object\"\n",
      "         },\n",
      "         \"message\" : {\n",
      "            \"description\" : \"Input message from the user (for chat mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"prompt\" : {\n",
      "            \"description\" : \"Input prompt (for generate/tokens mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"session_id\" : {\n",
      "            \"default\" : \"default\",\n",
      "            \"description\" : \"Unique identifier for the chat session\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"system_message\" : {\n",
      "            \"description\" : \"System message for chat initialization\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"text\" : {\n",
      "            \"description\" : \"Input text (for embed mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         }\n",
      "      },\n",
      "      \"componentManagementCommandsTemplate\" : {\n",
      "         \"detokenize\" : {\n",
      "            \"args\" : {\n",
      "               \"text\" : {\n",
      "                  \"description\" : \"Tokens to detokenize\",\n",
      "                  \"type\" : \"tokens\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To DeTokenize\"\n",
      "         },\n",
      "         \"device_info\" : {\n",
      "            \"description\" : \"Returns device and model diagnostics\"\n",
      "         },\n",
      "         \"info\" : {\n",
      "            \"description\" : \"Get info\"\n",
      "         },\n",
      "         \"reload_model\" : {\n",
      "            \"args\" : {\n",
      "               \"device\" : {\n",
      "                  \"default\" : \"cuda\",\n",
      "                  \"description\" : \"Device to run model on\",\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"generation_config\" : {\n",
      "                  \"description\" : \"Parameters needed for inference/chat/generation\",\n",
      "                  \"properties\" : {\n",
      "                     \"do_sample\" : {\n",
      "                        \"default\" : true,\n",
      "                        \"description\" : \"do_sample\",\n",
      "                        \"type\" : \"boolean\"\n",
      "                     },\n",
      "                     \"max_new_tokens\" : {\n",
      "                        \"description\" : \"Maximum number of tokens to generate\",\n",
      "                        \"max\" : 40960,\n",
      "                        \"min\" : 1,\n",
      "                        \"type\" : \"number\"\n",
      "                     },\n",
      "                     \"repetition_penalty\" : {\n",
      "                        \"description\" : \"repetition_penalty\",\n",
      "                        \"max\" : 10,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     },\n",
      "                     \"temperature\" : {\n",
      "                        \"description\" : \"Sampling temperature\",\n",
      "                        \"max\" : 1,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     },\n",
      "                     \"top_k\" : {\n",
      "                        \"description\" : \"Topk \",\n",
      "                        \"max\" : 1000,\n",
      "                        \"min\" : 1,\n",
      "                        \"type\" : \"number\"\n",
      "                     },\n",
      "                     \"top_p\" : {\n",
      "                        \"description\" : \"top_p\",\n",
      "                        \"max\" : 1,\n",
      "                        \"min\" : 0,\n",
      "                        \"type\" : \"float\"\n",
      "                     }\n",
      "                  },\n",
      "                  \"type\" : \"object\"\n",
      "               },\n",
      "               \"hf_token\" : {\n",
      "                  \"description\" : \"Hugging Face authentication token for private models or rate limiting\",\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"quantization_type\" : {\n",
      "                  \"default\" : null,\n",
      "                  \"description\" : \"Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization\",\n",
      "                  \"enum\" : [\n",
      "                     \"4bit\",\n",
      "                     \"8bit\",\n",
      "                     \"fp16\",\n",
      "                     \"fp8\"\n",
      "                  ],\n",
      "                  \"type\" : \"string\"\n",
      "               },\n",
      "               \"tensor_parallel\" : {\n",
      "                  \"default\" : true,\n",
      "                  \"description\" : \"Enable tensor parallelism\",\n",
      "                  \"type\" : \"boolean\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Reloads the model with optional new parameters\"\n",
      "         },\n",
      "         \"remove_chat_session\" : {\n",
      "            \"args\" : {\n",
      "               \"session_id\" : {\n",
      "                  \"description\" : \"Session ID to remove\",\n",
      "                  \"type\" : \"string\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Removes a specific chat session\"\n",
      "         },\n",
      "         \"reset\" : {\n",
      "            \"description\" : \"Reset all chat session\"\n",
      "         },\n",
      "         \"set_generation_config\" : {\n",
      "            \"args\" : {\n",
      "               \"generation_config\" : {\n",
      "                  \"description\" : \"New generation configuration\",\n",
      "                  \"type\" : \"object\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Updates generation configuration parameters\"\n",
      "         },\n",
      "         \"set_seed\" : {\n",
      "            \"args\" : {\n",
      "               \"seed\" : {\n",
      "                  \"description\" : \"Set the seed\",\n",
      "                  \"max\" : 10000,\n",
      "                  \"min\" : 1,\n",
      "                  \"type\" : \"number\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To set seed\"\n",
      "         },\n",
      "         \"tokenize\" : {\n",
      "            \"args\" : {\n",
      "               \"text\" : {\n",
      "                  \"description\" : \"Text to tokenize\",\n",
      "                  \"type\" : \"string\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"To Tokenize\"\n",
      "         },\n",
      "         \"update_cleanup_config\" : {\n",
      "            \"args\" : {\n",
      "               \"cleanup_config\" : {\n",
      "                  \"description\" : \"Send enabled: boolen, check_interval: in seconds for keep checcking, session_timeout: remove session beyond this in seconds\",\n",
      "                  \"type\" : \"object\"\n",
      "               }\n",
      "            },\n",
      "            \"description\" : \"Update Clean Up Config\"\n",
      "         }\n",
      "      },\n",
      "      \"componentMetadata\" : {\n",
      "         \"architecture\" : {\n",
      "            \"contextLength\" : 40960,\n",
      "            \"parameterCountB\" : 24\n",
      "         },\n",
      "         \"biasAndFairness\" : {\n",
      "            \"assessmentSummary\" : \"Internal testing showed some stereotypical associations. The model may reflect biases present in the training data.\",\n",
      "            \"knownBiases\" : [\n",
      "               \"cultural biases towards Western norms\",\n",
      "               \"potential for generating stereotypical content\"\n",
      "            ]\n",
      "         },\n",
      "         \"capabilities\" : {\n",
      "            \"supportsStreaming\" : true\n",
      "         },\n",
      "         \"dataUsagePolicy\" : {\n",
      "            \"policyStatement\" : \"User data is not used for training or improving this model.\",\n",
      "            \"usedForTraining\" : false\n",
      "         },\n",
      "         \"evaluation\" : {\n",
      "            \"benchmarks\" : {\n",
      "               \"HellaSwag\" : {\n",
      "                  \"metric\" : \"10-shot accuracy\",\n",
      "                  \"value\" : 69.1\n",
      "               },\n",
      "               \"MMLU\" : {\n",
      "                  \"metric\" : \"5-shot accuracy\",\n",
      "                  \"value\" : 74.6\n",
      "               },\n",
      "               \"TruthfulQA\" : {\n",
      "                  \"metric\" : \"mc2\",\n",
      "                  \"value\" : 66.4\n",
      "               }\n",
      "            },\n",
      "            \"evaluationData\" : {\n",
      "               \"details\" : \"Evaluated on a standard set of academic benchmarks to assess reasoning, knowledge, and safety.\"\n",
      "            }\n",
      "         },\n",
      "         \"framework\" : \"transformers\",\n",
      "         \"hardware\" : \"gpu\",\n",
      "         \"knownLimitations\" : \"May generate plausible but incorrect information. Performance on highly specialized topics may be limited.\",\n",
      "         \"model_path_resolution\" : \"automatic\",\n",
      "         \"provider\" : {\n",
      "            \"modelIdentifier\" : \"Magistral-Small-2506-24B\",\n",
      "            \"name\" : \"Mistral AI\"\n",
      "         },\n",
      "         \"quantization_methods\" : [\n",
      "            \"4bit\",\n",
      "            \"8bit\",\n",
      "            \"fp16\",\n",
      "            \"fp8\"\n",
      "         ],\n",
      "         \"supports_local_models\" : true,\n",
      "         \"supports_quantization\" : true,\n",
      "         \"trainingDetails\" : {\n",
      "            \"trainingData\" : {\n",
      "               \"dataPreprocessing\" : \"Standard cleaning and filtering techniques were applied.\",\n",
      "               \"datasetName\" : \"Proprietary mix of publicly available web data\",\n",
      "               \"datasetSize\" : \"Not disclosed\"\n",
      "            },\n",
      "            \"trainingProcedure\" : {\n",
      "               \"hyperparameters\" : {\n",
      "                  \"batchSize\" : \"Not disclosed\",\n",
      "                  \"learningRate\" : \"Not disclosed\",\n",
      "                  \"optimizer\" : \"AdamW\"\n",
      "               },\n",
      "               \"trainingFramework\" : \"Custom distributed training framework\"\n",
      "            }\n",
      "         },\n",
      "         \"usecase\" : \"chat-completion\"\n",
      "      },\n",
      "      \"componentOutputProtocol\" : {\n",
      "         \"embedding\" : {\n",
      "            \"description\" : \"Text embedding vector (embed mode)\",\n",
      "            \"type\" : \"array\"\n",
      "         },\n",
      "         \"generated\" : {\n",
      "            \"description\" : \"Generated text (generate mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"reply\" : {\n",
      "            \"description\" : \"Chat reply from the model (chat mode)\",\n",
      "            \"type\" : \"string\"\n",
      "         },\n",
      "         \"tokens\" : {\n",
      "            \"description\" : \"Generated token IDs (tokens mode)\",\n",
      "            \"type\" : \"array\"\n",
      "         }\n",
      "      },\n",
      "      \"componentParameters\" : {\n",
      "         \"max_tokens\" : 2048,\n",
      "         \"min_p\" : 0.01,\n",
      "         \"temperature\" : 0.2,\n",
      "         \"top_p\" : 0.95\n",
      "      },\n",
      "      \"componentType\" : \"model\",\n",
      "      \"componentURI\" : \"model.magistral-small-2506-llama_cpp2:1.0.0-stable\",\n",
      "      \"containerRegistryInfo\" : {\n",
      "         \"componentMode\" : \"aios\",\n",
      "         \"containerImage\" : \"MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1\",\n",
      "         \"containerImageMetadata\" : {\n",
      "            \"author\" : \"llm-team\",\n",
      "            \"description\" : \"AIOS block for chat using Hugging Face Transformers in-process with local model path support for Mistral-ai Magistral Small 2506 model\"\n",
      "         },\n",
      "         \"containerRegistryId\" : \"MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1\"\n",
      "      },\n",
      "      \"createdAt\" : \"2025-08-05T16:15:26.356Z\",\n",
      "      \"lastModifiedAt\" : \"2025-08-05T16:15:26.356Z\",\n",
      "      \"tags\" : [\n",
      "         \"transformers\",\n",
      "         \"chat\",\n",
      "         \"huggingface\",\n",
      "         \"mistral-ai\",\n",
      "         \"llm\",\n",
      "         \"embedding\",\n",
      "         \"in-process\",\n",
      "         \"local-models\",\n",
      "         \"model-path-resolution\",\n",
      "         \"quantization\",\n",
      "         \"4bit\",\n",
      "         \"8bit\",\n",
      "         \"fp16\",\n",
      "         \"bitsandbytes\"\n",
      "      ]\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Register the asset with the AIOS Component Registry\n",
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/registerComponent -H \"Content-Type: application/json\" -d @./component.json | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7027ff65",
   "metadata": {},
   "source": [
    "## 2. Allocating a Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d38ae3d",
   "metadata": {},
   "source": [
    "Now that the asset is registered, we can create a running instance of it. In AIOS, a running instance of an asset is called a **Block**. A Block is the fundamental unit of execution in AIOS, responsible for serving inference requests or running any computational workload defined by an Asset. When you allocate a Block, the AIOS **Resource Allocator** finds a suitable cluster and schedules the Block for execution.\n",
    "\n",
    "For more details, you can refer to the official documentation:\n",
    "- [Block Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md)\n",
    "- [Block APIs/Services](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-services)\n",
    "\n",
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration for the block. Let's examine the key fields in our `allocation.json` for the `mistral_llama_cpp` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `magistral-small-2506-2`.\n",
    "- **`blockComponentURI`**: The block should be an instance of the `model.magistral-small-2506-llama_cpp2:1.0.0-stable` asset we registered earlier.\n",
    "- **`blockInitData`**: This section provides initial data to the block. Here we specify the `model_name` (`Magistral-Small-2506_Q8_0.gguf`) and the `system_message`.\n",
    "- **`initSettings`**: These are settings for the block's runtime. We have settings for GPU usage, device configuration, quantization, and model parameters.\n",
    "- **`policyRulesSpec`**: This is a crucial section that defines the chain of policies governing the block's behavior:\n",
    "    - **`clusterAllocator`**: Selects a specific cluster, in this case, `gcp-cluster-2`.\n",
    "    - **`resourceAllocator`**: Assigns the block to a specific node (`wc-gpu-node1`) and GPU (`0`).\n",
    "    - **`loadBalancer`**: Manages how requests are distributed, configured here to cache sessions.\n",
    "    - **`stabilityChecker`**: Defines a health check mechanism to ensure the block is running correctly.\n",
    "    - **`autoscaler`**: Enables autoscaling for the block based on GPU utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat allocation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c448ca56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": {\n",
      "    \"data\": {\n",
      "      \"message\": \"task scheduled in background\",\n",
      "      \"task_id\": \"9e7034b8-5b39-4eb2-b956-4ae30a2a6eed\"\n",
      "    },\n",
      "    \"success\": true\n",
      "  },\n",
      "  \"success\": true,\n",
      "  \"task_id\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Now, let's allocate the block using `curl`.\n",
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7be93a",
   "metadata": {},
   "source": [
    "## 3. Checking Block Status and Metrics\n",
    "\n",
    "After allocating the block, it's important to check its status to ensure it's running correctly. AIOS provides a comprehensive **Metrics System** that exposes endpoints for health status, and resource consumption, giving you a complete picture of your block's operational state. The Metrics System is designed to be extensible, allowing you to define and expose custom metrics for your applications.\n",
    "\n",
    "For more information on the metrics system, see the documentation:\n",
    "- [Metrics System Overview-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#metrics-system)\n",
    "- [Custom Metrics-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#custom-metrics)\n",
    "\n",
    "Let's check the health status and metrics of our block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ddcf39",
   "metadata": {},
   "source": [
    "### Check Block Health"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca857d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"block_id\": \"magistral-small-2506-2\",\n",
      "  \"healthy\": true,\n",
      "  \"instances\": [\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"executor\",\n",
      "      \"reason\": \"executor instance\"\n",
      "    }\n",
      "  ],\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Health - This command verifies the health of the running service within the block.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/health/magistral-small-2506-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f3a4c",
   "metadata": {},
   "source": [
    "### Check Block Metrics (Before Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982f6ba",
   "metadata": {},
   "source": [
    "#### Core Metric Categories\n",
    "\n",
    "##### 🔧 **Runtime Metrics**\n",
    "- **CPU Usage**: Real-time CPU utilization percentage for the block container\n",
    "- **Memory Usage**: Current memory consumption and peak memory usage\n",
    "- **GPU Utilization**: GPU usage percentage and VRAM consumption (for GPU-enabled blocks)\n",
    "\n",
    "##### 📊 **Performance Metrics**\n",
    "- **Request Latency**: Average response times for inference requests\n",
    "- **Throughput**: Requests per second (RPS) and tokens per second (TPS)\n",
    "- **Queue Length**: Number of pending requests in the processing queue\n",
    "\n",
    "##### 🔄 **Operational Metrics**\n",
    "- **Request Count**: Total number of requests processed over time\n",
    "- **Error Rate**: Percentage of failed requests and error types\n",
    "\n",
    "##### 🏥 **Health Metrics**\n",
    "- **Uptime**: Total running time since last restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fd87f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   650  100   650    0     0  34722      0 --:--:-- --:--:-- --:--:-- 36111\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"magistral-small-2506-2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"magistral-small-2506-2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__magistral-small-2506-2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754410543.0228,\n",
      "                  \"tasks_processed_total\" : 0\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 25.0118050575256,\n",
      "               \"uptime_hours\" : 0.00694772362709045,\n",
      "               \"uptime_minutes\" : 0.416863417625427\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics before launching the app\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/magistral-small-2506-2 | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353f2800",
   "metadata": {},
   "source": [
    "## 4. Performing Inference\n",
    "\n",
    "Now that the block is running, let's perform an inference task. We'll use a Python script to send a request to the block via gRPC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "975e36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: grpcio-tools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: protobuf in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (6.31.1)\n",
      "Requirement already satisfied: setuptools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (from grpcio-tools) (80.9.0)\n"
     ]
    }
   ],
   "source": [
    "# First, let's install the necessary libraries for our gRPC client and import them.\n",
    "# We also need to add the `inference_client` directory to our Python path to import the generated gRPC files.\n",
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a9e7b7",
   "metadata": {},
   "source": [
    "Now, let's define a function to send an inference request to our block. This function will connect to the gRPC server, construct a request packet, and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abf243a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "    data = {\n",
    "        \"mode\": \"chat\",\n",
    "        \"message\": message,\n",
    "        \"gen_params\": generation_config\n",
    "    }\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id=block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        data=json.dumps(data),\n",
    "        ts=time.time()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        \n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(\"Data:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "             print(f\"Data: {response.data}\")\n",
    "\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ed3af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 11.61312198638916s\n",
      "Session ID: session_notebook_mistral-2\n",
      "Sequence No: 2\n",
      "Data:\n",
      "{\n",
      "  \"reply\": \"Mistral AI is a cutting-edge company based in Paris, France, developing large language models. While I don't have real-time information or access to specific details about Mistral AI's models, I can provide some general insights based on what is typically valued in language models and what Mistral AI might offer based on their mission and the AI landscape.\\n\\nHere are some potential key advantages of Mistral AI's language models compared to other LLMs:\\n\\n1. **Focus on Multilingualism**: Mistral AI emphasizes multilingual capabilities, which could mean their models are better at understanding and generating text in multiple languages, including less commonly supported ones.\\n\\n2. **Ethical and Responsible AI**: Mistral AI is committed to developing AI in an ethical and responsible manner. Their models might have better safeguards against bias, misinformation, and harmful content.\\n\\n3. **European Perspective**: Being based in Europe, Mistral AI's models might incorporate a more European-centric perspective, which could be valuable for applications in European markets.\\n\\n4. **Innovation in Architecture**: Mistral AI might be using innovative architectures or techniques that improve performance, efficiency, or other aspects of their models.\\n\\n5. **Customization and Fine-tuning**: They might offer better tools or services for customizing and fine-tuning their models for specific use cases.\\n\\n6. **Data Privacy**: As a European company, they might adhere to stricter data privacy standards, which could be appealing for certain applications.\\n\\n7. **Sustainability**: Mistral AI might be focusing on developing more energy-efficient models, which is increasingly important in the AI field.\\n\\n8. **Collaboration with Research Institutions**: Mistral AI collaborates with leading research institutions, which could lead to more advanced and well-tested models.\\n\\n9. **Open-source Contributions**: Mistral AI might be more open about sharing their research and models, fostering a more collaborative and transparent AI community.\\n\\n10. **Focus on Long-term Impact**: Mistral AI aims to have a long-term positive impact on society, which could translate into models that are more robust, adaptable, and beneficial for various applications.\\n\\nPlease note that these are potential advantages based on the information available about Mistral AI and general trends in the field. For the most accurate and up-to-date information, I would recommend checking Mistral AI's official website or contacting them directly.\"\n",
      "}\n",
      "Timestamp: 1754410813.8580995\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"magistral-small-2506-2\",\n",
    "    session_id=\"session_notebook_mistral-2\",\n",
    "    seq_no=2,\n",
    "    message=\"What are the key advantages of the Mistral AI language models compared to other LLMs?\",\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be4f71",
   "metadata": {},
   "source": [
    "## 4.a Block Metrics After Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36c7087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3152  100  3152    0     0  95454      0 --:--:-- --:--:-- --:--:-- 98500\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"magistral-small-2506-2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"magistral-small-2506-2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0.000179052352905273\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__magistral-small-2506-2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754410543.0228,\n",
      "                  \"tasks_processed_total\" : 2\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 280.064146280289,\n",
      "               \"uptime_hours\" : 0.0777955961889691,\n",
      "               \"uptime_minutes\" : 4.66773577133814\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"magistral-small-2506-2\",\n",
      "               \"end_to_end_count_total\" : 0,\n",
      "               \"end_to_end_fps\" : 0,\n",
      "               \"end_to_end_latency\" : 0,\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 1.16,\n",
      "                     \"load1m\" : 2.53,\n",
      "                     \"load5m\" : 2,\n",
      "                     \"percent\" : 0.12\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 28468.31,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 90.58,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 53451.69,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 27954.31,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 74.33,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 53965.69,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 10.32,\n",
      "                     \"totalMem\" : 342406.93,\n",
      "                     \"usedMem\" : 35339.77\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-zwr9\",\n",
      "               \"llm_active_sessions\" : 0,\n",
      "               \"llm_cpu_utilization\" : 0,\n",
      "               \"llm_gpu_utilization\" : 0,\n",
      "               \"llm_inference_duration_seconds_bucket\" : 0,\n",
      "               \"llm_inference_duration_seconds_count\" : 0,\n",
      "               \"llm_inference_duration_seconds_sum\" : 0,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_memory_usage_bytes\" : 0,\n",
      "               \"llm_prompt_tokens_total\" : 0,\n",
      "               \"llm_prompts_total\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 0,\n",
      "               \"llm_tokens_generated_total\" : 0,\n",
      "               \"llm_tokens_per_second\" : 0,\n",
      "               \"nodeId\" : \"wc-gpu-node2\",\n",
      "               \"nodeKey\" : \"in-zwr9__magistral-small-2506-2\",\n",
      "               \"on_data_count_total\" : 0,\n",
      "               \"on_data_fps\" : 0,\n",
      "               \"on_data_latency\" : 0,\n",
      "               \"on_preprocess_count_total\" : 0,\n",
      "               \"on_preprocess_fps\" : 0,\n",
      "               \"on_preprocess_latency\" : 0,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"timestamp\" : 1754410813.05927,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics after inference\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/magistral-small-2506-2 | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7dba43",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat with Streamlit\n",
    "\n",
    "To provide an interactive way to test the model, we will launch a Streamlit application. The following cell will import the necessary function and launch the app, providing a public URL for you to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79eb3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok nest_asyncio websockets > /dev/null\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utils' to the path to find the inference_client\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "sys.path.append(os.path.abspath('../utils/streamlit_app'))\n",
    "\n",
    "from utils import run_streamlit_direct\n",
    "\n",
    "# Define the block_id and grpc_server_address\n",
    "BLOCK_ID = \"magistral-small-2506-2\"\n",
    "GRPC_SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "streamlit_url = run_streamlit_direct(BLOCK_ID, GRPC_SERVER_ADDRESS, port=8501)\n",
    "print(f\"Streamlit App URL: {streamlit_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb68ff",
   "metadata": {},
   "source": [
    "You can now open the URL above in your browser to interact with the model. Once you are done, you can proceed to the cleanup steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a562ede",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up\n",
    "\n",
    "After you are finished with the block, it is important to deallocate it to free up resources. You can also deregister the asset if you no longer need it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3d2cf",
   "metadata": {},
   "source": [
    "### Deallocate the Block\n",
    "This command will stop the running block and release all associated resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5b5c49",
   "metadata": {},
   "source": [
    "`K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccbe6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   105  100    67  100    38     12      7  0:00:05  0:00:05 --:--:--    16\n",
      "{\n",
      "   \"data\" : {\n",
      "      \"data\" : \"Action performed\",\n",
      "      \"success\" : true\n",
      "   },\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Show the Block in K8s first and then show after deletion!\n",
    "!curl -X POST http://MANAGEMENTMASTER:30600/controller/removeBlock/gcp-cluster-2 -H \"Content-Type: application/json\" -d '{\"block_id\": \"magistral-small-2506-2\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141054b",
   "metadata": {},
   "source": [
    "### Deregister the Asset\n",
    "If you no longer need the asset in the registry, you can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de2758e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   125  100    64  100    61   7875   7506 --:--:-- --:--:-- --:--:-- 15625\n",
      "{\n",
      "   \"error\" : false,\n",
      "   \"payload\" : {\n",
      "      \"acknowledged\" : true,\n",
      "      \"deletedCount\" : 1\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/unregisterComponent -H \"Content-Type: application/json\" -d '{\"uri\": \"model.magistral-small-2506-llama_cpp2:1.0.0-stable\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9608f44",
   "metadata": {},
   "source": [
    "## More models sample notebooks can be found [here](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/tree/main/video_tutorial_series/02_more_models_llama_cpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
