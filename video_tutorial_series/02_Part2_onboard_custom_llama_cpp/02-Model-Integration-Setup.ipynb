{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d7675a9",
   "metadata": {},
   "source": [
    "# AIOS Model Integration Setup\n",
    "\n",
    "## Setting Up Your Model Integration Environment\n",
    "\n",
    "Welcome to this tutorial on AIOS Model Onboarding! In this notebook, we'll walk through the essential steps to set up your development environment for integrating models with the AIOS platform.\n",
    "\n",
    "> **Important Note:** This notebook is designed to be run **inside a Docker container**. The tutorial assumes that you will execute the cells within a container where the AIOS environment is already set up. The next section provides the `docker run` command to start this container.\n",
    "\n",
    "### What You'll Learn:\n",
    "- üê≥ How to set up and run a Docker container for AIOS development.\n",
    "- üèóÔ∏è Building a complete AIOS model integration within the container.\n",
    "- üß™ Testing your model integration using the AIOS testing utilities.\n",
    "- üì¶ Creating a production-ready Dockerfile for your custom model.\n",
    "\n",
    "### Prerequisites:\n",
    "Before starting this tutorial, ensure you've completed:\n",
    "- ‚úÖ **Tutorial 1**: Prerequisites & Setup (This includes setting up your local directory and downloading the necessary models).\n",
    "- ‚úÖ Docker installed and running on your machine.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44fc3e0",
   "metadata": {},
   "source": [
    "## 2.1. Docker Container Setup for Testing\n",
    "\n",
    "Since this notebook requires AIOS components to run properly, we need to execute it inside a Docker container with the necessary dependencies. We'll use the llama_cpp base image and mount our workspace.\n",
    "\n",
    "### Prerequisites\n",
    "- Docker installed and running\n",
    "- llama_cpp Docker image available (from previous tutorials)\n",
    "- Workspace directory accessible\n",
    "\n",
    "### Container Creation Command\n",
    "\n",
    "Run the following command to create and start a container with the necessary setup:\n",
    "\n",
    "```bash\n",
    "# Create and run container with workspace mounted\n",
    "docker run -it --rm \\\n",
    "  --name aios-model-integration \\\n",
    "  --gpus all \\\n",
    "  -v /home/user/local_files:/workspace \\\n",
    "  -v /home/user/local_files/models:/models \\\n",
    "  -w /workspace/documentation/video_tutorial_series/model_integration \\\n",
    "  -p 8888:8888 \\\n",
    "  aios_llama_cpp:v1-gpu \\\n",
    "  bash\n",
    "```\n",
    "\n",
    "### Alternative: Run with Jupyter Support\n",
    "\n",
    "If you want to run this notebook interactively:\n",
    "\n",
    "```bash\n",
    "# Run container with Jupyter notebook support\n",
    "docker run -it --rm \\\n",
    "  --name aios-model-integration-jupyter \\\n",
    "  --gpus all \\\n",
    "  -v /home/user/workspace:/workspace \\\n",
    "  -v /home/user/workspace/models:/models \\\n",
    "  -w /workspace \\\n",
    "  -p 8888:8888 \\\n",
    "  aios_llama_cpp:v1-gpu \\\n",
    "  jupyter notebook --ip=0.0.0.0 --port=8888 --no-browser --allow-root\n",
    "```\n",
    "\n",
    "### Container Environment Setup\n",
    "\n",
    "Once inside the container, set up the environment:\n",
    "\n",
    "```bash\n",
    "# Navigate to the tutorial directory\n",
    "cd /workspace/documentation/video_tutorial_series/model_integration\n",
    "\n",
    "# Install additional dependencies if needed\n",
    "pip install notebook ipykernel\n",
    "\n",
    "# Verify AIOS components are available\n",
    "python -c \"from aios_instance import TestContext, BlockTester; print('‚úÖ AIOS components ready')\"\n",
    "```\n",
    "\n",
    "### Volume Mounts Explained\n",
    "\n",
    "- `/workspace` - Main workspace containing tutorial files and code\n",
    "- `/models` - Dedicated models directory for downloaded models\n",
    "- Working directory set to tutorial location\n",
    "- Port 8888 exposed for Jupyter notebook access\n",
    "- GPU access enabled with `--gpus all`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127255c8",
   "metadata": {},
   "source": [
    "## 2.2. Building AIOS Integration Inside Container\n",
    "\n",
    "Now we'll build a comprehensive AIOS model integration that works inside our Docker container environment. This implementation follows the production-ready patterns and can be tested immediately.\n",
    "\n",
    "### Key Components:\n",
    "- **SimpleLlamaBlock**: Main class for model integration\n",
    "- **Container-Compatible Paths**: Proper path handling for Docker environment\n",
    "- **Volume-Mounted Models**: Access to models through container volumes\n",
    "- **AIOS Testing Integration**: Full testing setup within container\n",
    "- **Production Patterns**: Based on actual AIOS implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f2af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install websockets huggingface_hub grpcio protobuf==3.20.0 redis boto3 kubernetes flask pynvml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87c1f91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Successfully imported AIOS integration components\n",
      "üîß Ready to build SimpleLlamaBlock class\n"
     ]
    }
   ],
   "source": [
    "# Essential imports for basic AIOS integration\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "# AIOS core components\n",
    "from aios_instance import PreProcessResult, OnDataResult, Block\n",
    "from aios_llama_cpp import LLAMAUtils\n",
    "\n",
    "# Hugging Face for model downloading\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(\"üì¶ Successfully imported AIOS integration components\")\n",
    "print(\"üîß Ready to build SimpleLlamaBlock class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159908c",
   "metadata": {},
   "source": [
    "### The `SimpleLlamaBlock` Class\n",
    "\n",
    "This is the main class for our AIOS block. It inherits from the base `Block` class and implements the core methods required for a functional AIOS block. This class will handle model loading, data processing, and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ea36d8",
   "metadata": {},
   "source": [
    "#### `__init__(self, context)`\n",
    "\n",
    "The constructor for our `SimpleLlamaBlock`. This method is called when the block is first initialized. It's responsible for loading the model, setting up any necessary configurations, and preparing the block for inference. The `context` object provides access to block-specific information, such as initialization data and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59a84324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è SimpleLlamaBlock class created with __init__ method\n"
     ]
    }
   ],
   "source": [
    "# Simple AIOS Block Class - Basic Structure\n",
    "class SimpleLlamaBlock:\n",
    "    \"\"\"\n",
    "    Simple AIOS Block for LLaMA model integration\n",
    "    Following the basic AIOS patterns from prerequisites tutorial\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context):\n",
    "        \"\"\"Initialize the block with context and load model\"\"\"\n",
    "        self.context = context\n",
    "        \n",
    "        # Get model name from initialization data\n",
    "        init_data = context.block_init_data or {}\n",
    "        self.model_name = init_data.get(\"model_name\")\n",
    "        if not self.model_name:\n",
    "            raise ValueError(\"Missing 'model_name' in block_init_data\")\n",
    "            \n",
    "        # Set up basic configuration\n",
    "        self.model_path = context.common_path\n",
    "        \n",
    "        # Initialize LLaMA utilities\n",
    "        self.llama = LLAMAUtils(\n",
    "            model_path=f\"{self.model_path}/{self.model_name}\",\n",
    "            use_gpu=True\n",
    "        )\n",
    "        \n",
    "        # Load the model\n",
    "        self.llama.load_model()\n",
    "        print(f\"‚úÖ SimpleLlamaBlock initialized with model: {self.model_name}\")\n",
    "\n",
    "print(\"üèóÔ∏è SimpleLlamaBlock class created with __init__ method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5f0d5f",
   "metadata": {},
   "source": [
    "#### `on_preprocess(self, packet)`\n",
    "\n",
    "This method is called for each incoming packet of data. Its primary role is to prepare the data for the `on_data` method. This can include tasks like deserializing JSON, decoding base64 data, or any other preprocessing steps required by the model. It returns a `PreProcessResult` object that contains the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37da803b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Added on_preprocess method to SimpleLlamaBlock\n",
      "üéØ Features: JSON parsing, plain text support, error handling, session management\n"
     ]
    }
   ],
   "source": [
    "# Basic Input Preprocessing - on_preprocess method\n",
    "\n",
    "# Add on_preprocess method to SimpleLlamaBlock\n",
    "def on_preprocess(self, packet):\n",
    "    \"\"\"\n",
    "    Process incoming packets and prepare them for inference\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get data from packet\n",
    "        data = packet.data\n",
    "        \n",
    "        # Parse JSON if it's a string\n",
    "        if isinstance(data, str):\n",
    "            try:\n",
    "                data = json.loads(data)\n",
    "            except:\n",
    "                # Keep as plain string\n",
    "                pass\n",
    "        \n",
    "        # Create preprocessed result\n",
    "        result = PreProcessResult(\n",
    "            packet=packet,\n",
    "            extra_data={\"input\": data},\n",
    "            session_id=packet.session_id\n",
    "        )\n",
    "        \n",
    "        return True, [result]\n",
    "        \n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "# Add the method to SimpleLlamaBlock class\n",
    "SimpleLlamaBlock.on_preprocess = on_preprocess\n",
    "\n",
    "print(\"üîÑ Added on_preprocess method to SimpleLlamaBlock\")\n",
    "print(\"üéØ Features: JSON parsing, plain text support, error handling, session management\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd95453",
   "metadata": {},
   "source": [
    "#### `on_data(self, preprocessed_entry, is_ws)`\n",
    "\n",
    "This is the core method where the actual model inference happens. It takes the preprocessed data from `on_preprocess` and feeds it to the model. The `is_ws` parameter indicates if the request came from a WebSocket connection, which is useful for streaming responses. The method returns an `OnDataResult` object containing the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8c7f1055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Added on_data method to SimpleLlamaBlock\n",
      "üöÄ Features: Text generation, multiple input formats, error handling, clean response format\n"
     ]
    }
   ],
   "source": [
    "# Basic Inference Logic - on_data method\n",
    "def on_data(self, preprocessed_entry, is_ws):\n",
    "    \"\"\"\n",
    "    Run model inference on preprocessed data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get input data\n",
    "        \n",
    "        input_data = preprocessed_entry.extra_data[\"input\"][\"inputs\"][0]\n",
    "        print(input_data)\n",
    "        # Simple text generation\n",
    "        if isinstance(input_data, str):\n",
    "            # Direct string input\n",
    "            response = self.llama.generate_text(input_data)\n",
    "            \n",
    "        elif isinstance(input_data, dict):\n",
    "            # Handle different input formats\n",
    "            if \"prompt\" in input_data:\n",
    "                response = self.llama.generate_text(input_data[\"prompt\"])\n",
    "            elif \"message\" in input_data:\n",
    "                response = self.llama.generate_text(input_data[\"message\"])\n",
    "            else:\n",
    "                response = \"No valid input found\"\n",
    "        else:\n",
    "            response = \"Invalid input format\"\n",
    "        print('response is ',response) \n",
    "        return True, OnDataResult(output={\"reply\": response})\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Block on_data failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False, str(e)\n",
    "\n",
    "# Add the method to SimpleLlamaBlock class\n",
    "SimpleLlamaBlock.on_data = on_data\n",
    "\n",
    "print(\"üß† Added on_data method to SimpleLlamaBlock\")\n",
    "print(\"üöÄ Features: Text generation, multiple input formats, error handling, clean response format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98f46a8",
   "metadata": {},
   "source": [
    "#### `management(self, action, data)`\n",
    "\n",
    "This method provides a way to send custom commands to the block. This can be used for a variety of tasks, such as reloading the model, getting status information, or any other custom management actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30ebb22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Added management methods to SimpleLlamaBlock\n",
      "üîß Features: Health monitoring, parameter updates, management commands, error handling\n"
     ]
    }
   ],
   "source": [
    "# Basic Health and Management Methods\n",
    "\n",
    "def management(self, action, data):\n",
    "    \"\"\"\n",
    "    Basic management operations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if action == \"info\":\n",
    "            return {\"model\": self.model_name, \"status\": \"running\"}\n",
    "        elif action == \"reset\":\n",
    "            return {\"message\": \"Reset completed\"}\n",
    "        else:\n",
    "            return {\"error\": f\"Unknown action: {action}\"}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Add all methods to SimpleLlamaBlock class\n",
    "SimpleLlamaBlock.management = management\n",
    "\n",
    "print(\"‚öôÔ∏è Added management methods to SimpleLlamaBlock\")\n",
    "print(\"üîß Features: Health monitoring, parameter updates, management commands, error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f04698",
   "metadata": {},
   "source": [
    "## 2.3. Basic Testing\n",
    "\n",
    "Now we'll create a simple test to verify our basic model integration works correctly.\n",
    "\n",
    "### Simple Testing Approach\n",
    "\n",
    "Instead of complex testing frameworks, we'll use a basic approach to test our AIOS block:\n",
    "\n",
    "- **Direct instantiation**: Create the block directly\n",
    "- **Mock data**: Use simple test data\n",
    "- **Basic validation**: Check that methods work as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d164f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIOS Testing Setup - Container Environment\n",
    "# Following test_llama_cpp.py patterns with container-appropriate paths\n",
    "\n",
    "from aios_instance import TestContext, BlockTester\n",
    "import time\n",
    "import pprint\n",
    "import os,sys\n",
    "\n",
    "# Create proper AIOS test context for container environment\n",
    "context = TestContext()\n",
    "\n",
    "# Container-appropriate paths\n",
    "context.common_path = \"/models\"  # Models mounted at /models in container\n",
    "context.instance_path = \"/workspace/\"\n",
    "\n",
    "# Verify container environment\n",
    "print(\"üê≥ Container Environment Check:\")\n",
    "print(f\"- Workspace path: {os.path.exists('/workspace')}\")\n",
    "print(f\"- Models path: {os.path.exists('/models')}\")\n",
    "print(f\"- Current working directory: {os.getcwd()}\")\n",
    "print(f\"- AIOS components: Available\" if 'aios_instance' in sys.modules else \"Not loaded\")\n",
    "\n",
    "# Configuration for LLaMA model\n",
    "llama_config = {\n",
    "    \"n_gpu_layers\": -1,         # Use all GPU layers\n",
    "    \"n_threads\": -1,            # Auto-detect threads\n",
    "    \"n_ctx\": 4096,             # Context size\n",
    "    \"seed\": 3407,               # Random seed\n",
    "    \"verbose\": True             # Enable logging\n",
    "}\n",
    "\n",
    "# Block initialization data - using container paths\n",
    "context.block_init_data = {\n",
    "    \"model_name\": \"gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf\",  # Will look in /models/google/gemma-2b-it\n",
    "}\n",
    "\n",
    "# Block settings optimized for container\n",
    "context.block_init_settings = {\n",
    "    \"use_gpu\": True,\n",
    "    \"gpu_id\": [0],\n",
    "    \"enable_metrics\": False,     # Keep metrics disabled for simple testing\n",
    "    \"model_config\": llama_config,\n",
    "    \"cleanup_enabled\": True,\n",
    "    \"cleanup_check_interval\": 10,\n",
    "    \"cleanup_session_timeout\": 30\n",
    "}\n",
    "\n",
    "# Generation parameters\n",
    "context.block_init_parameters = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 512,\n",
    "    \"top_p\": 0.95,\n",
    "}\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"min_p\": 0.01,\n",
    "    \"top_k\": 64,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "print(\"\\n‚úÖ AIOS test context configured for container\")\n",
    "print(f\"üìÅ Model path: {context.common_path}\")\n",
    "print(f\"ü§ñ Model name: {context.block_init_data['model_name']}\")\n",
    "print(f\"‚öôÔ∏è  Generation config: {generation_config}\")\n",
    "\n",
    "# Container-specific model check\n",
    "model_path = f\"{context.common_path}/{context.block_init_data['model_name']}\"\n",
    "model_exists = os.path.exists(model_path)\n",
    "print(f\"üìã Model availability: {'Available' if model_exists else 'Not found'}\")\n",
    "\n",
    "if model_exists:\n",
    "    print(\"‚úÖ Ready for full testing with downloaded models!\")\n",
    "    print(\"# tester = BlockTester.init_with_context(SimpleLlamaBlock, context)\")\n",
    "else:\n",
    "    print(\"üìé To download models in container:\")\n",
    "    print(\"huggingface-cli download google/gemma-2b-it --local-dir /models/google/gemma-2b-it\")\n",
    "\n",
    "print(\"- Full AIOS testing framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "135d2712",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aios_llama_cpp.library:\u001b[93mCleanup configuration: {'enabled': True, 'check_interval': 300, 'session_timeout': 3600}\u001b[0m\n",
      "INFO:aios_llama_cpp.library:Chat session cleanup thread started\n",
      "llama_model_load_from_file_impl: using device CUDA0 (NVIDIA A100-SXM4-80GB) - 39824 MiB free\n",
      "llama_model_load_from_file_impl: using device CUDA1 (NVIDIA A100-SXM4-80GB) - 40638 MiB free\n",
      "llama_model_loader: loaded meta data with 39 key-value pairs and 444 tensors from /models/gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma3\n",
      "llama_model_loader: - kv   1:                      gemma3.context_length u32              = 131072\n",
      "llama_model_loader: - kv   2:                         gemma3.block_count u32              = 34\n",
      "llama_model_loader: - kv   3:                    gemma3.embedding_length u32              = 2560\n",
      "llama_model_loader: - kv   4:                 gemma3.feed_forward_length u32              = 10240\n",
      "llama_model_loader: - kv   5:                gemma3.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv   6:             gemma3.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv   7:                gemma3.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv   8:              gemma3.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv   9:    gemma3.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                   gemma3.rope.scaling.type str              = linear\n",
      "llama_model_loader: - kv  11:                 gemma3.rope.scaling.factor f32              = 8.000000\n",
      "llama_model_loader: - kv  12:                      gemma3.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  13:            gemma3.attention.sliding_window u32              = 1024\n",
      "llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  19:                      tokenizer.ggml.tokens arr[str,262144]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üê≥ Running Actual AIOS Block Tests...\n",
      "‚úÖ Test payloads created successfully\n",
      "- Simple payload: 1 input(s)\n",
      "\n",
      "üîß Initializing AIOS Block Tester...\n",
      "Loading model from /models/gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf with config: {}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  20:                      tokenizer.ggml.scores arr[f32,262144]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  21:                  tokenizer.ggml.token_type arr[i32,262144]  = [3, 3, 3, 2, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  23:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {{ bos_token }} {%- if messages[0]['r...\n",
      "llama_model_loader: - kv  25:                 gemma3.mm.tokens_per_image u32              = 256\n",
      "llama_model_loader: - kv  26:         gemma3.vision.attention.head_count u32              = 16\n",
      "llama_model_loader: - kv  27: gemma3.vision.attention.layer_norm_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  28:                  gemma3.vision.block_count u32              = 27\n",
      "llama_model_loader: - kv  29:             gemma3.vision.embedding_length u32              = 1152\n",
      "llama_model_loader: - kv  30:          gemma3.vision.feed_forward_length u32              = 4304\n",
      "llama_model_loader: - kv  31:                   gemma3.vision.image_size u32              = 896\n",
      "llama_model_loader: - kv  32:                 gemma3.vision.num_channels u32              = 3\n",
      "llama_model_loader: - kv  33:                   gemma3.vision.patch_size u32              = 14\n",
      "llama_model_loader: - kv  34:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  35:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  36:           tokenizer.ggml.add_padding_token bool             = false\n",
      "llama_model_loader: - kv  37:           tokenizer.ggml.add_unknown_token bool             = false\n",
      "llama_model_loader: - kv  38:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - type  f32:  205 tensors\n",
      "llama_model_loader: - type  f16:    1 tensors\n",
      "llama_model_loader: - type q4_0:  238 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_0\n",
      "print_info: file size   = 2.93 GiB (6.49 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token: 256000 '<end_of_image>' is not marked as EOG\n",
      "load: control token: 255999 '<start_of_image>' is not marked as EOG\n",
      "load: control token:    105 '<start_of_turn>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 8\n",
      "load: token to piece cache size = 1.9446 MB\n",
      "print_info: arch             = gemma3\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 2560\n",
      "print_info: n_layer          = 34\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 1024\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 10240\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 1000000.0\n",
      "print_info: freq_scale_train = 0.125\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 4B\n",
      "print_info: model params     = 3.88 B\n",
      "print_info: general.name     = n/a\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 262144\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 106 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 248 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 106 '<end_of_turn>'\n",
      "print_info: max token length = 93\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  27 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  28 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  30 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  31 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  32 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  33 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  34 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (f16) (and 444 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/35 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  3002.65 MiB\n",
      "...........................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 512\n",
      "llama_context: n_ctx_per_seq = 512\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 1000000.0\n",
      "llama_context: freq_scale    = 0.125\n",
      "llama_context: n_ctx_per_seq (512) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     1.00 MiB\n",
      "create_memory: n_ctx = 512 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 512 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified: layer  26: skipped\n",
      "llama_kv_cache_unified: layer  27: skipped\n",
      "llama_kv_cache_unified: layer  28: skipped\n",
      "llama_kv_cache_unified: layer  29: dev = CPU\n",
      "llama_kv_cache_unified: layer  30: skipped\n",
      "llama_kv_cache_unified: layer  31: skipped\n",
      "llama_kv_cache_unified: layer  32: skipped\n",
      "llama_kv_cache_unified: layer  33: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    10.00 MiB\n",
      "llama_kv_cache_unified: size =   10.00 MiB (   512 cells,   5 layers,  1 seqs), K (f16):    5.00 MiB, V (f16):    5.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 512 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified: layer  26: dev = CPU\n",
      "llama_kv_cache_unified: layer  27: dev = CPU\n",
      "llama_kv_cache_unified: layer  28: dev = CPU\n",
      "llama_kv_cache_unified: layer  29: skipped\n",
      "llama_kv_cache_unified: layer  30: dev = CPU\n",
      "llama_kv_cache_unified: layer  31: dev = CPU\n",
      "llama_kv_cache_unified: layer  32: dev = CPU\n",
      "llama_kv_cache_unified: layer  33: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =    58.00 MiB\n",
      "llama_kv_cache_unified: size =   58.00 MiB (   512 cells,  29 layers,  1 seqs), K (f16):   29.00 MiB, V (f16):   29.00 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:      CUDA0 compute buffer size =  1797.00 MiB\n",
      "llama_context:  CUDA_Host compute buffer size =     7.01 MiB\n",
      "llama_context: graph nodes  = 1537\n",
      "llama_context: graph splits = 514 (with bs=512), 1 (with bs=1)\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_unknown_token': 'false', 'tokenizer.ggml.add_padding_token': 'false', 'tokenizer.ggml.add_eos_token': 'false', 'gemma3.vision.num_channels': '3', 'gemma3.rope.freq_base': '1000000.000000', 'gemma3.rope.scaling.type': 'linear', 'gemma3.attention.layer_norm_rms_epsilon': '0.000001', 'gemma3.vision.attention.layer_norm_epsilon': '0.000001', 'gemma3.attention.key_length': '256', 'gemma3.attention.head_count_kv': '4', 'gemma3.attention.head_count': '8', 'tokenizer.chat_template': '{{ bos_token }} {%- if messages[0][\\'role\\'] == \\'system\\' -%} {%- if messages[0][\\'content\\'] is string -%} {%- set first_user_prefix = messages[0][\\'content\\'] + \\'\\\\n\\' -%} {%- else -%} {%- set first_user_prefix = messages[0][\\'content\\'][0][\\'text\\'] + \\'\\\\n\\' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message[\\'role\\'] == \\'assistant\\') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message[\\'role\\'] -%} {%- endif -%} {{ \\'<start_of_turn>\\' + role + \\'\\\\n\\' + (first_user_prefix if loop.first else \"\") }} {%- if message[\\'content\\'] is string -%} {{ message[\\'content\\'] | trim }} {%- elif message[\\'content\\'] is iterable -%} {%- for item in message[\\'content\\'] -%} {%- if item[\\'type\\'] == \\'image\\' -%} {{ \\'<start_of_image>\\' }} {%- elif item[\\'type\\'] == \\'text\\' -%} {{ item[\\'text\\'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ \\'<end_of_turn>\\\\n\\' }} {%- endfor -%} {%- if add_generation_prompt -%} {{\\'<start_of_turn>model\\\\n\\'}} {%- endif -%}', 'gemma3.attention.value_length': '256', 'gemma3.context_length': '131072', 'tokenizer.ggml.add_bos_token': 'true', 'gemma3.feed_forward_length': '10240', 'general.architecture': 'gemma3', 'gemma3.vision.block_count': '27', 'gemma3.attention.sliding_window': '1024', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'gemma3.vision.patch_size': '14', 'tokenizer.ggml.bos_token_id': '2', 'gemma3.block_count': '34', 'gemma3.vision.attention.head_count': '16', 'gemma3.rope.scaling.factor': '8.000000', 'gemma3.vision.feed_forward_length': '4304', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.eos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '3', 'gemma3.embedding_length': '2560', 'general.file_type': '2', 'gemma3.mm.tokens_per_image': '256', 'tokenizer.ggml.pre': 'default', 'gemma3.vision.embedding_length': '1152', 'gemma3.vision.image_size': '896'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }} {%- if messages[0]['role'] == 'system' -%} {%- if messages[0]['content'] is string -%} {%- set first_user_prefix = messages[0]['content'] + '\\n' -%} {%- else -%} {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n' -%} {%- endif -%} {%- set loop_messages = messages[1:] -%} {%- else -%} {%- set first_user_prefix = \"\" -%} {%- set loop_messages = messages -%} {%- endif -%} {%- for message in loop_messages -%} {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%} {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }} {%- endif -%} {%- if (message['role'] == 'assistant') -%} {%- set role = \"model\" -%} {%- else -%} {%- set role = message['role'] -%} {%- endif -%} {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }} {%- if message['content'] is string -%} {{ message['content'] | trim }} {%- elif message['content'] is iterable -%} {%- for item in message['content'] -%} {%- if item['type'] == 'image' -%} {{ '<start_of_image>' }} {%- elif item['type'] == 'text' -%} {{ item['text'] | trim }} {%- endif -%} {%- endfor -%} {%- else -%} {{ raise_exception(\"Invalid content type\") }} {%- endif -%} {{ '<end_of_turn>\\n' }} {%- endfor -%} {%- if add_generation_prompt -%} {{'<start_of_turn>model\\n'}} {%- endif -%}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n",
      "INFO:aios_llama_cpp.library:Model loaded successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SimpleLlamaBlock initialized with model: gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf\n",
      "‚úÖ Block tester initialized successfully!\n",
      "\n",
      "üß™ Test 1: Simple Message Inference\n",
      "Input: Hello! Tell me about Deepseek LLM Model?\n",
      "{'message': 'Hello! Tell me about Deepseek LLM Model?', 'session_id': 'container_session_1', 'gen_params': {'temperature': 0.1, 'min_p': 0.01, 'top_k': 64, 'top_p': 0.95, 'max_tokens': 512}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =     251.54 ms\n",
      "llama_perf_context_print: prompt eval time =     251.23 ms /    12 tokens (   20.94 ms per token,    47.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3274.44 ms /    49 runs   (   66.83 ms per token,    14.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    3653.49 ms /    61 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response is  [{'id': 'cmpl-50991541-c927-452a-9148-e1d5465b28b8', 'object': 'text_completion', 'created': 1754462127, 'model': '/models/gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf', 'choices': [{'text': \"</h1>\\n<p>Deepseek LLM is a powerful and versatile large language model developed by Deepseek. Here's a breakdown of what makes it noteworthy:</p>\\n\\n<h2>Key Features and Capabilities</h2>\\n\\n*   **Mixture-of\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 50, 'total_tokens': 62}}]\n",
      "result_1 [{'reply': [{'id': 'cmpl-50991541-c927-452a-9148-e1d5465b28b8', 'object': 'text_completion', 'created': 1754462127, 'model': '/models/gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf', 'choices': [{'text': \"</h1>\\n<p>Deepseek LLM is a powerful and versatile large language model developed by Deepseek. Here's a breakdown of what makes it noteworthy:</p>\\n\\n<h2>Key Features and Capabilities</h2>\\n\\n*   **Mixture-of\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 50, 'total_tokens': 62}}]}]\n",
      "‚è±Ô∏è  Inference time: 3.66s\n",
      "‚úÖ Response: [{'id': 'cmpl-50991541-c927-452a-9148-e1d5465b28b8', 'object': 'text_completion', 'created': 1754462127, 'model': '/models/gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf', 'choices': [{'text': \"</h1>\\n<p>Deepseek LLM is a powerful and versatile large language model developed by Deepseek. Here's a breakdown of what makes it noteworthy:</p>\\n\\n<h2>Key Features and Capabilities</h2>\\n\\n*   **Mixture-of\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 12, 'completion_tokens': 50, 'total_tokens': 62}}]...\n",
      "\n",
      "üß™ Test 2: Management Operations\n",
      "Block Info: {'model': 'gemma-3-4b-it-qat-q4/gemma-3-4b-it-q4_0.gguf', 'status': 'running'}\n",
      "\n",
      "üìä Performance Summary:\n",
      "- Average inference time: 3.66s\n",
      "- Model status: running\n",
      "\n",
      "üéØ Block testing result: SUCCESS\n",
      "\n",
      "üöÄ Real AIOS block testing completed!\n"
     ]
    }
   ],
   "source": [
    "# Container-Based Test Examples - Following AIOS Testing Patterns\n",
    "\n",
    "# Example test payloads optimized for container environment\n",
    "def create_container_test_payloads():\n",
    "    \"\"\"\n",
    "    Create test payloads optimized for container environment\n",
    "    \"\"\"\n",
    "    \n",
    "    # Simple text message test\n",
    "    simple_payload = {\n",
    "        \"inputs\": [{\n",
    "            \"message\": \"Hello! Tell me about Deepseek LLM Model?\",\n",
    "            \"session_id\": \"container_session_1\",\n",
    "            \"gen_params\": generation_config\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return simple_payload\n",
    "\n",
    "def run_actual_block_tests():\n",
    "    \"\"\"\n",
    "    Actually execute the SimpleLlamaBlock with test payloads\n",
    "    \"\"\"\n",
    "    print(\"üê≥ Running Actual AIOS Block Tests...\")\n",
    "    \n",
    "    try:\n",
    "        # Create test payloads\n",
    "        simple_payload = create_container_test_payloads()\n",
    "        \n",
    "        print(\"‚úÖ Test payloads created successfully\")\n",
    "        print(f\"- Simple payload: {len(simple_payload['inputs'])} input(s)\")\n",
    "\n",
    "        # Check if models are available\n",
    "        model_path = f\"{context.common_path}/{context.block_init_data['model_name']}\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"\\n‚ö†Ô∏è  Models not found. Download first:\")\n",
    "            print(f\"huggingface-cli download {context.block_init_data['model_name']} --local-dir {model_path}\")\n",
    "            return False\n",
    "            \n",
    "        # Initialize the block tester\n",
    "        print(\"\\nüîß Initializing AIOS Block Tester...\")\n",
    "        tester = BlockTester.init_with_context(SimpleLlamaBlock, context)\n",
    "        print(\"‚úÖ Block tester initialized successfully!\")\n",
    "        \n",
    "        # Test 1: Simple message inference\n",
    "        print(\"\\nüß™ Test 1: Simple Message Inference\")\n",
    "        print(f\"Input: {simple_payload['inputs'][0]['message']}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        result_1 = tester.run(simple_payload)\n",
    "        elapsed_1 = time.time() - start_time\n",
    "        print('result_1',result_1)\n",
    "        print(f\"‚è±Ô∏è  Inference time: {elapsed_1:.2f}s\")\n",
    "        if result_1 and len(result_1) > 0:\n",
    "            print(f\"‚úÖ Response: {result_1[0].get('reply', 'No reply found')[:100]}...\")\n",
    "        else:\n",
    "            print(\"‚ùå No response received\")\n",
    "\n",
    "        \n",
    "        # Test 2: Management commands\n",
    "        print(\"\\nüß™ Test 2: Management Operations\")\n",
    "        info_result = tester.block_instance.management(\"info\", {})\n",
    "        print(f\"Block Info: {info_result}\")\n",
    "        \n",
    "        # Performance summary\n",
    "        avg_time = elapsed_1\n",
    "        print(f\"\\nüìä Performance Summary:\")\n",
    "        print(f\"- Average inference time: {avg_time:.2f}s\")\n",
    "        print(f\"- Model status: {info_result.get('status', 'unknown')}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Block testing failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Execute actual block tests\n",
    "test_result = run_actual_block_tests()\n",
    "print(f\"\\nüéØ Block testing result: {'SUCCESS' if test_result else 'FAILED'}\")\n",
    "print(\"\\nüöÄ Real AIOS block testing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619c211",
   "metadata": {},
   "source": [
    "\n",
    "### Why Test Your Integration?\n",
    "\n",
    "üõ†Ô∏è **Catch Issues Early**: Identify and fix problems before deployment\n",
    "‚úÖ **Ensure Functionality**: Verify that your integration behaves as expected\n",
    "üìà **Performance Checks**: Ensure your integration meets performance benchmarks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad3626",
   "metadata": {},
   "source": [
    "## 2.4 Summary of Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89770ac1",
   "metadata": {},
   "source": [
    "Sample Full file save it as main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a897b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple AIOS Block Implementation\n",
    "Based on AIOS tutorial patterns and test_llama_cpp.py\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "# AIOS core components\n",
    "from aios_instance import PreProcessResult, OnDataResult, Block\n",
    "from aios_llama_cpp import LLAMAUtils\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "class SimpleLlamaBlock:\n",
    "    \"\"\"\n",
    "    Simple AIOS Block for LLaMA model integration\n",
    "    Following AIOS patterns with all essential methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, context):\n",
    "        \"\"\"Initialize the block with context and load model\"\"\"\n",
    "        self.context = context\n",
    "        init_data = context.block_init_data or {}\n",
    "        \n",
    "        self.model_name = init_data.get(\"model_name\")\n",
    "        if not self.model_name:\n",
    "            raise ValueError(\"Missing model_name in block_init_data\")\n",
    "            \n",
    "        # Set up model path and configuration\n",
    "        self.model_path = context.common_path\n",
    "        \n",
    "        # Get model configuration from settings\n",
    "        settings = getattr(context, 'block_init_settings', {})\n",
    "        model_config = settings.get('model_config', {})\n",
    "        \n",
    "        # Initialize LLaMA utilities\n",
    "        self.llama = LLAMAUtils(\n",
    "            model_path=f\"{self.model_path}/{self.model_name}\",\n",
    "            use_gpu=settings.get('use_gpu', True),\n",
    "            **model_config\n",
    "        )\n",
    "        \n",
    "        # Load the model\n",
    "        self.llama.load_model()\n",
    "        print(f\"‚úÖ SimpleLlamaBlock initialized with model: {self.model_name}\")\n",
    "    \n",
    "    def on_preprocess(self, packet):\n",
    "        \"\"\"Process incoming packets and prepare them for inference\"\"\"\n",
    "        try:\n",
    "            data = packet.data\n",
    "            if isinstance(data, str):\n",
    "                try:\n",
    "                    data = json.loads(data)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return True, [PreProcessResult(\n",
    "                packet=packet,\n",
    "                extra_data={\"input\": data},\n",
    "                session_id=packet.session_id\n",
    "            )]\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "    \n",
    "    def on_data(self, preprocessed_entry, is_ws=False):\n",
    "        \"\"\"Run model inference on preprocessed data\"\"\"\n",
    "        try:\n",
    "            input_data = preprocessed_entry.extra_data[\"input\"]\n",
    "            \n",
    "            # Handle different input formats\n",
    "            if isinstance(input_data, str):\n",
    "                response = self.llama.generate_text(input_data)\n",
    "            elif isinstance(input_data, dict):\n",
    "                if \"inputs\" in input_data:\n",
    "                    # Handle batch inputs\n",
    "                    inputs = input_data[\"inputs\"][0] if input_data[\"inputs\"] else {}\n",
    "                    if \"message\" in inputs:\n",
    "                        response = self.llama.generate_text(inputs[\"message\"])\n",
    "                    elif \"messages\" in inputs:\n",
    "                        # Handle chat format\n",
    "                        messages = inputs[\"messages\"]\n",
    "                        if messages and \"content\" in messages[-1]:\n",
    "                            content = messages[-1][\"content\"]\n",
    "                            if isinstance(content, list) and content:\n",
    "                                text_content = next((c[\"text\"] for c in content if c.get(\"type\") == \"text\"), \"\")\n",
    "                                response = self.llama.generate_text(text_content)\n",
    "                            else:\n",
    "                                response = self.llama.generate_text(str(content))\n",
    "                        else:\n",
    "                            response = \"No valid message content found\"\n",
    "                    else:\n",
    "                        response = \"No valid input format found\"\n",
    "                elif \"prompt\" in input_data:\n",
    "                    response = self.llama.generate_text(input_data[\"prompt\"])\n",
    "                else:\n",
    "                    response = \"No valid input format found\"\n",
    "            else:\n",
    "                response = \"Invalid input format\"\n",
    "                \n",
    "            return True, OnDataResult(output={\"reply\": response})\n",
    "        except Exception as e:\n",
    "            return False, str(e)\n",
    "    \n",
    "    \n",
    "    def management(self, action, data):\n",
    "        \"\"\"Handle management operations\"\"\"\n",
    "        try:\n",
    "            if action == \"info\":\n",
    "                return {\n",
    "                    \"model\": self.model_name,\n",
    "                    \"status\": \"running\",\n",
    "                    \"health\": self.health()\n",
    "                }\n",
    "            elif action == \"reset\":\n",
    "                return {\"message\": \"Reset completed\"}\n",
    "            elif action == \"reload_model\":\n",
    "                # Reload model if path provided\n",
    "                if \"model_path\" in data:\n",
    "                    return {\"message\": f\"Model reload requested: {data['model_path']}\"}\n",
    "                return {\"message\": \"Model reload completed\"}\n",
    "            else:\n",
    "                return {\"error\": f\"Unknown action: {action}\"}\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Entry point\n",
    "if __name__ == \"__main__\":\n",
    "    block = Block(SimpleLlamaBlock)\n",
    "    block.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7a2d5",
   "metadata": {},
   "source": [
    "Add Requirements.txt and keep the test files handy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdc01f5",
   "metadata": {},
   "source": [
    "### Dockerfile Creation\n",
    "In the next section, we'll cover how to create a production-ready Dockerfile for your AIOS model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "530ae397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dockerfile created successfully!\n",
      "\n",
      "üìã Dockerfile Contents:\n",
      "# Simple AIOS Model Integration Dockerfile\n",
      "# Based on existing AIOS patterns with llama_cpp_python_base:v1 base image\n",
      "\n",
      "# Use llama_cpp_python_base as the base image (contains AIOS components)\n",
      "FROM llama_cpp_python_base:v1\n",
      "\n",
      "# Clean up any existing app directory\n",
      "RUN rm -rf /app\n",
      "\n",
      "# Set up working directory using FOLDER_NAME argument\n",
      "ARG FOLDER_NAME=aios_simple_llama\n",
      "WORKDIR /${FOLDER_NAME}\n",
      "\n",
      "# Copy all project files to the container\n",
      "COPY . /${FOLDER_NAME}/\n",
      "\n",
      "# Install Python dependencies\n",
      "RUN pip3 install -r requirements.txt\n",
      "\n",
      "# Set the entrypoint to run our AIOS block\n",
      "ENTRYPOINT [\"python3\", \"-u\", \"main.py\"]\n",
      "\n",
      "\n",
      "‚úÖ .dockerignore created successfully!\n",
      "\n",
      "üìã .dockerignore Contents:\n",
      "# Docker ignore file for AIOS integration\n",
      "__pycache__/\n",
      "*.pyc\n",
      "*.pyo\n",
      "*.pyd\n",
      ".Python\n",
      "*.so\n",
      ".git/\n",
      ".gitignore\n",
      "README.md\n",
      "*.md\n",
      ".vscode/\n",
      ".idea/\n",
      "*.log\n",
      "tests/\n",
      ".pytest_cache/\n",
      "models/\n",
      "*.ipynb\n",
      ".ipynb_checkpoints/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Production Dockerfile\n",
    "# Simple Dockerfile for AIOS model integration deployment\n",
    "\n",
    "dockerfile_content = '''# Simple AIOS Model Integration Dockerfile\n",
    "# Based on existing AIOS patterns with llama_cpp_python_base:v1 base image\n",
    "\n",
    "# Use llama_cpp_python_base as the base image (contains AIOS components)\n",
    "FROM llama_cpp_python_base:v1\n",
    "\n",
    "# Clean up any existing app directory\n",
    "RUN rm -rf /app\n",
    "\n",
    "# Set up working directory using FOLDER_NAME argument\n",
    "ARG FOLDER_NAME=aios_simple_llama\n",
    "WORKDIR /${FOLDER_NAME}\n",
    "\n",
    "# Copy all project files to the container\n",
    "COPY . /${FOLDER_NAME}/\n",
    "\n",
    "# Install Python dependencies\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "# Set the entrypoint to run our AIOS block\n",
    "ENTRYPOINT [\"python3\", \"-u\", \"main.py\"]\n",
    "'''\n",
    "\n",
    "# Create the Dockerfile\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile created successfully!\")\n",
    "print(\"\\nüìã Dockerfile Contents:\")\n",
    "print(dockerfile_content)\n",
    "\n",
    "# Also create a simple .dockerignore file\n",
    "dockerignore_content = '''# Docker ignore file for AIOS integration\n",
    "__pycache__/\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    "*.so\n",
    ".git/\n",
    ".gitignore\n",
    "README.md\n",
    "*.md\n",
    ".vscode/\n",
    ".idea/\n",
    "*.log\n",
    "tests/\n",
    ".pytest_cache/\n",
    "models/\n",
    "*.ipynb\n",
    ".ipynb_checkpoints/\n",
    "'''\n",
    "\n",
    "with open('.dockerignore', 'w') as f:\n",
    "    f.write(dockerignore_content)\n",
    "\n",
    "print(\"\\n‚úÖ .dockerignore created successfully!\")\n",
    "print(\"\\nüìã .dockerignore Contents:\")\n",
    "print(dockerignore_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704b4e6",
   "metadata": {},
   "source": [
    "### Testing Steps\n",
    "\n",
    "1. **Build the Docker Image**: Create a Docker image from your Dockerfile.\n",
    "2. **Run the Docker Container**: Start a container from your image with --entrypoint=/bin/bash\n",
    "3. **Execute Test Cases**: Run your test suite inside the running container again.\n",
    "4. **Check Logs and Metrics**: Monitor logs and performance metrics for anomalies.\n",
    "5. **Iterate**: Fix any issues and retest until you achieve the desired stability and performance.\n",
    "\n",
    "\n",
    "Basic testing is a crucial step to ensure your AIOS model integration is robust and reliable. By following the steps outlined in this section, you can identify and fix issues early, ensuring a smoother deployment process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a84e0b0",
   "metadata": {},
   "source": [
    "## 2.5. Conclusion and Next Steps\n",
    "\n",
    "### What We've Built:\n",
    "\n",
    "‚úÖ **Container-Ready AIOS Integration**: Complete model integration optimized for Docker containers\n",
    "‚úÖ **Simple Testing Approach**: Basic inference testing without complex frameworks\n",
    "‚úÖ **GPU-Accelerated Environment**: Full GPU access within containerized environment\n",
    "‚úÖ **Volume-Mounted Storage**: Proper data persistence and model storage\n",
    "‚úÖ **Production Patterns**: AIOS-standard implementation with essential methods\n",
    "\n",
    "### Container Architecture:\n",
    "\n",
    "| Component | Host Path | Container Path | Purpose |\n",
    "|-----------|-----------|----------------|----------|\n",
    "| Workspace | `/home/user/local_files` | `/workspace` | Code and notebooks |\n",
    "| Models | `/home/user/local_files/models` | `/models` | Model storage |\n",
    "| GPU | Host GPU | Container GPU | Accelerated inference |\n",
    "\n",
    "### Core AIOS Methods:\n",
    "\n",
    "| Method | Purpose | Implementation |\n",
    "|--------|---------|----------------|\n",
    "| `__init__` | Initialize block and load model | Context setup, model loading |\n",
    "| `on_preprocess` | Process incoming packets | JSON parsing, data extraction |\n",
    "| `on_data` | Run model inference | Text generation with multiple formats |\n",
    "| `management` | Handle management commands | Info and reset operations |\n",
    "\n",
    "### Basic Testing Approach:\n",
    "\n",
    "‚úÖ **Simple Payloads**: Basic message and prompt testing\n",
    "‚úÖ **Direct Inference**: Straightforward model testing\n",
    "‚úÖ **Health Checks**: Basic status monitoring\n",
    "‚úÖ **Container Environment**: Docker-based testing setup\n",
    "\n",
    "### Generated Files:\n",
    "\n",
    "```\n",
    "basic-aios-project/\n",
    "‚îú‚îÄ‚îÄ main.py              # Complete AIOS block implementation\n",
    "‚îú‚îÄ‚îÄ requirements.txt     # Dependencies\n",
    "‚îú‚îÄ‚îÄ setup_guide.md       # Setup instructions\n",
    "‚îú‚îÄ‚îÄ test_basic.py        # Basic environment testing\n",
    "‚îî‚îÄ‚îÄ models/              # Model directory\n",
    "```\n",
    "\n",
    "### Dockerfile Creation:\n",
    "\n",
    "To containerize the AIOS integration, a Dockerfile is created in the project root. This file defines the container environment, including the base image, working directory, and commands to install dependencies and copy project files.\n",
    "\n",
    "**Sample Dockerfile:**\n",
    "\n",
    "```\n",
    "# Use the official Python image from the Docker Hub\n",
    "FROM llama_cpp_python_base:v1\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the requirements file into the container\n",
    "COPY requirements.txt .\n",
    "\n",
    "# Install the Python dependencies\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the rest of the project files into the container\n",
    "COPY . .\n",
    "\n",
    "# Command to run the application\n",
    "CMD [\"python\", \"main.py\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Success!** You now have a simple, working AIOS model integration ready for container deployment! üöÄüê≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b7dc0",
   "metadata": {},
   "source": [
    "For registering,allocating and inferencing this model in AIOS Ecosystem , refer this tutorial - [Model Onboarding](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/blob/main/video_tutorial_series/02_Part1_onboard_gemma3_llama_cpp/Model-Onboarding.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
