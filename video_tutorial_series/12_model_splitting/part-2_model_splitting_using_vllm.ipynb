{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f846f30",
   "metadata": {},
   "source": [
    "## üß† Model Splitting Across Nodes in Grid Using Thirdparty Inference Service like vLLM(with Ray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337c7d7",
   "metadata": {},
   "source": [
    "\n",
    "- Author: Shridhar Kini (Profile)\n",
    "- To Securely Run: `jupyter notebook password` to generate onetime password for secure access\n",
    "- To Run: `jupyter notebook --allow-root  --port 9999 --ip=0.0.0.0 part-2_model_splitting_using_vllm.ipynb`\n",
    "- To Clear Outputs: Use `jupyter nbconvert --clear-output --inplace part-2_model_splitting_using_vllm.ipynb`\n",
    "\n",
    "This notebook provides an overview of Model Splitting across Nodes in AIOS using 3rd party systems like vLLM inference server using init containers concept. This Demo is for the users who has bigger models which cannot be fit in single GPU or Node. Splitting Models with the helper code from **init_container** using vLLM inference server is shown here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339b4b",
   "metadata": {},
   "source": [
    "**To  GET PORT MAPPING wrt to Service**([Doc](https://docs.aigr.id/installation/installation/#deploying-registry-services))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395268e2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# üîß Configuration Setup - Run this cell first to set up shared variables\n",
    "import os\n",
    "\n",
    "# Set configuration variables that will be available across all cells\n",
    "GATEWAY_URL = \"MANAGEMENTMASTER:30600\"\n",
    "CLUSTER_ID = \"gcp-cluster-2\"\n",
    "GLOBAL_CLUSTER_METRICS_DB = \"MANAGEMENTMASTER:30202\"\n",
    "GLOBAL_BLOCK_METRICS_DB = \"MANAGEMENTMASTER:30201\"\n",
    "PARSER_URL = \"MANAGEMENTMASTER:30501\"\n",
    "GLOBAL_CLUSTER_DB = \"MANAGEMENTMASTER:30101\"\n",
    "GLOBAL_TASK_DB_SERVICE = \"MANAGEMENTMASTER:30108\"\n",
    "COMPONENT_REGISTRY_SERVICE = \"MANAGEMENTMASTER:30112\"\n",
    "GLOBAL_BLOCKDB_SERVICE = \"MANAGEMENTMASTER:30100\"\n",
    "#SERVER_URL = \"10.10.10.10:5000\"  # For other API calls\n",
    "\n",
    "# Set environment variables for bash cells\n",
    "os.environ['GATEWAY_URL'] = GATEWAY_URL\n",
    "os.environ['CLUSTER_ID'] = CLUSTER_ID\n",
    "os.environ['GLOBAL_CLUSTER_METRICS_DB'] = GLOBAL_CLUSTER_METRICS_DB\n",
    "os.environ['GLOBAL_BLOCK_METRICS_DB'] = GLOBAL_BLOCK_METRICS_DB\n",
    "os.environ['PARSER_URL'] = PARSER_URL\n",
    "os.environ['GLOBAL_CLUSTER_DB'] = GLOBAL_CLUSTER_DB\n",
    "os.environ['GLOBAL_TASK_DB_SERVICE'] = GLOBAL_TASK_DB_SERVICE\n",
    "os.environ['COMPONENT_REGISTRY_SERVICE'] = COMPONENT_REGISTRY_SERVICE\n",
    "os.environ['GLOBAL_BLOCKDB_SERVICE'] = GLOBAL_BLOCKDB_SERVICE\n",
    "#os.environ['SERVER_URL'] = SERVER_URL\n",
    "\n",
    "print(\"‚úÖ Configuration variables set:\")\n",
    "print(f\"   ‚Ä¢ GATEWAY_URL: {GATEWAY_URL}\")\n",
    "print(f\"   ‚Ä¢ CLUSTER_ID: {CLUSTER_ID}\")\n",
    "print(f\"   ‚Ä¢ GLOBAL_CLUSTER_METRICS_DB: {GLOBAL_CLUSTER_METRICS_DB}\")\n",
    "print(\"\\nüìù These variables are now available in both Python and bash cells!\")\n",
    "print(\"   - In Python: use GATEWAY_URL, CLUSTER_ID, GLOBAL_CLUSTER_METRICS_DB PARSER_URL GLOBAL_CLUSTER_DB GLOBAL_TASK_DB_SERVICE\")\n",
    "print(\"   - In bash: use $GATEWAY_URL, $CLUSTER_ID, $GLOBAL_CLUSTER_METRICS_DB $PARSER_URL $GLOBAL_CLUSTER_DB $GLOBAL_TASK_DB_SERVICE\")\n",
    "os.system('echo $GATEWAY_URL')\n",
    "os.system('echo $CLUSTER_ID')\n",
    "os.system('echo $GLOBAL_CLUSTER_METRICS_DB')\n",
    "os.system('echo $PARSER_URL')\n",
    "os.system('echo $GLOBAL_CLUSTER_DB')\n",
    "os.system('echo $GLOBAL_TASK_DB_SERVICE')\n",
    "os.system('echo $COMPONENT_REGISTRY_SERVICE')\n",
    "os.system('echo $GLOBAL_BLOCKDB_SERVICE')\n",
    "os.system('echo $GLOBAL_BLOCK_METRICS_DB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde50d97",
   "metadata": {},
   "source": [
    "### Code Sample for Splitting and Generating Response ([Code](Part-2/init_container))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c73dbd",
   "metadata": {},
   "source": [
    "##### Main Codes\n",
    "- [init_container/main.py](Part-2/init_container/main.py) - For generating Deployment and Service File of vLLM Container and deploy it with LeaderWorker strategy\n",
    "    - Leader and Worker:\n",
    "        - One Leader pod which initializes Ray Cluster and vLLM Inference Server and N-1 workers pods which joins the Ray Cluster created by Leader pod\n",
    "        - perfect for distributed applications like vLLM that use a central coordinator (the leader) and several computation nodes (the workers)\n",
    "        - Leader Service is created with Http port(for vLLM inference) and Ray Port(for Coordinating).\n",
    "    - Interacts with K8s cluster using K8s Python Client\n",
    "- [init_container_sdk/sdk.py](Part-2/init_container/init_container_sdk/sdk.py) - SDK for interfacing init container with AIOS system.\n",
    "- [vllm_client/main.py](Part-2/block/vllm-client/main.py) - Proxy Block which can interact with leader of vLLM replicas. \n",
    "    - It can hold chat history kind of logic\n",
    "    - It can do inference with Splitted Model Rank-0\n",
    "    - It can be extended to do other tasks like Metrics, Logging, etc\n",
    "    - It can talk to any other API of vLLM if needed.\n",
    "\n",
    "##### Important Changes from AIOS Flow\n",
    "- [component.json](Part-2/component.json): `\"componentMode\": \"aios\"`   to `\"componentMode\": \"third_party\",\n",
    "        \"initContainer\": {\n",
    "            \"image\": \"MANAGEMENTMASTER:31280/third-party/vllm\"\n",
    "        }`\n",
    "- In [resource allocator policy](Part-2/policies/code/function.py):\n",
    "            `if input_data['action'] == 'third_party_allocate':\n",
    "                logging.info(f\"parameters={self.parameters}\")\n",
    "\n",
    "                if 'third_party_allocation_data' in self.parameters:\n",
    "                    return self.parameters['third_party_allocation_data']`\n",
    "- init container :\n",
    "    - creates vLLM container as initialization for model split using Ray backend. Post vllm container starts,  block container(instance of proxy block) will be created.\n",
    "    - any other 3rd party/backend can be initialized like this.\n",
    "    - command for leader:\n",
    "        - `bash /vllm-workspace/examples/online_serving/multi-node-serving.sh leader --ray_cluster_size={self.replicas}`\n",
    "            - script initializes distributed environment\n",
    "            - leader: This argument tells the script to configure this pod as the head node of a Ray cluster. Ray is the underlying framework vLLM uses to coordinate tasks across multiple GPUs and nodes\n",
    "            - --ray_cluster_size=$(self.replicas): This tells the Ray head node how many total nodes (pods) to expect in its cluster. $(self.replicas) value is equal to the size field (e.g., 4), so Ray knows to wait for 1 leader and 3 workers to join.\n",
    "        - `python3 -m vllm.entrypoints.openai.api_server --port {self.api_port} --model {self.model} --max-model-len {self.max_model_len} --tensor-parallel-size {self.tensor_parallel} --pipeline-parallel-size {self.pipeline_parallel}`\n",
    "            - After the Ray cluster is initialized, this command starts the main vLLM application. \n",
    "            - This process acts as the API endpoint for user requests. It receives prompts, forwards them to the distributed Ray cluster for processing by the LLM, and returns the generated text.\n",
    "            - It listens on the specified --port.\n",
    "            - --tensor-parallel-size and --pipeline-parallel-size are key vLLM arguments that define how the model is split across all available GPUs in the cluster for efficient, parallel processing.\n",
    "    - command for worker:\n",
    "        - `bash /vllm-workspace/examples/online_serving/multi-node-serving.sh worker --ray_address={ray_address}`\n",
    "            - this command configures the pod as a worker node in the Ray cluster.\n",
    "            - This argument tells the script to join an existing Ray cluster instead of starting a new one.\n",
    "            - --ray_address=$(ray_address): This is the most important part. ray_address is leader_service_name.namespace.svc.cluster.local resolves to the internal IP address and port of the leader pod. This is how the worker knows exactly where to find and connect to the Ray head node, allowing it to join the cluster and start receiving computation tasks.\n",
    "    - replicas - size vs tensor_parallel_size - pipeline_parallel_size:\n",
    "        - Kubernetes (replicas, size) is responsible for providing the infrastructure (the pods and GPUs).\n",
    "        - vLLM (--tensor-parallel-size, --pipeline-parallel-size) is responsible for using that infrastructure to run the model.\n",
    "        - **Summary Analogy**\n",
    "            - size: Determines how many workers are on your project team to complete one large task.\n",
    "            - replicas: The number of independent project teams you are running simultaneously to handle more overall work.\n",
    "            - --tensor-parallel-size & --pipeline-parallel-size: The internal work plan that dictates how your project team organizes itself to complete that task. The number of people required by the plan must match the team size.\n",
    "            \n",
    "            - total pods: replicas x size  (equal to Total number of GPUs available)\n",
    "            - size: --tensor-parallel-size x --pipeline-parallel-size \n",
    "            - if replicas=3 and `--tensor-parallel-size=2 and --pipeline-parallel-size=4`\n",
    "                - What you get is:\n",
    "                    - Three completely separate vLLM deployments.\n",
    "                    - Each deployment consists of 8 pods (1 leader, 7 workers).\n",
    "                    - Each 8-pod deployment runs its own model instance using 2-way tensor and 4-way pipeline parallelism.\n",
    "                    - You would have a total of 3 * 8 = 24 pods running in your cluster.\n",
    "- Proxy Block for: \n",
    "    - Proxy Block can do inference with Splitted Model Rank-0\n",
    "    - Proxy Block can hold chat history kind of logic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e48f2",
   "metadata": {},
   "source": [
    "##### To Build the init container images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852d7e8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash Part-2/init_container/build_docker.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bab05a",
   "metadata": {},
   "source": [
    "##### Push the built docker to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b87ee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker push MANAGEMENTMASTER:31280/third-party/vllm:demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ef624f",
   "metadata": {},
   "source": [
    "##### To Build the block container image (Proxy Container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef24b61",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash Part-2/block/vllm-client/build_docker.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91208c0",
   "metadata": {},
   "source": [
    "##### Push the built docker to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcca88",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker push MANAGEMENTMASTER:31280/example/vllm-client:demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5e8ec",
   "metadata": {},
   "source": [
    "##### **Register the Blocks Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a01929",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$COMPONENT_REGISTRY_SERVICE/api/registerComponent \\\n",
    " -d @Part-2/component.json \\\n",
    " -H \"Content-Type: application/json\" | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd8bd1",
   "metadata": {},
   "source": [
    "##### **UnRegister the Block Component**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b54cc61",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$COMPONENT_REGISTRY_SERVICE/api/unregisterComponent \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"uri\":\"model.vllm-runner-demo:1.0.0-stable\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abe3522",
   "metadata": {},
   "source": [
    "##### **Deploy the Proxy Block(With its init container)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a1918",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST -d @Part-2/block.json \\\n",
    " -H \"Content-Type: application/json\" \\\n",
    "  http://$PARSER_URL/api/createBlock | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9def8",
   "metadata": {},
   "source": [
    "##### To check the k8s pod(to be done in target cluster Controle Plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73194c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "kubectl get pods -n vllm-blocks #for vLLM Ray Cluster\n",
    "kubectl get pods -n default  #for init container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b806c",
   "metadata": {},
   "source": [
    "##### To get the log of vLLM pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0bd05",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl logs -f vllm-0 -n vllm-blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63e487",
   "metadata": {},
   "source": [
    "##### To get the log of proxy Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84faac",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "kubectl get pods -n blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cd1ee",
   "metadata": {},
   "source": [
    "##### **Create Inference Server For The Cluster**\n",
    "- Run this commands in your cluster node(like master node)\n",
    "    - `kubectl create namespace inference-server`\n",
    "    - `kubectl create -f inference_server/inference_server.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18220825",
   "metadata": {},
   "source": [
    "##### **Do the Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b27425",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST  http://CLUSTER2_MASTER_IP:31504/v1/infer \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "  \"model\": \"vllm-block-demo-1\",\n",
    "  \"session_id\": \"session-4\",\n",
    "  \"seq_no\": 23,\n",
    "  \"data\": {\n",
    "    \"mode\": \"completions\",\n",
    "    \"generation_config\": {\n",
    "                \"max_tokens\": 512,\n",
    "                \"top_k\": 50,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 1.0\n",
    "            },\n",
    "    \"message\": \"Give me code for adding two integers list element wise in c++\",\n",
    "    \"system_message\": \"You are a helpful assistant that provides code examples.\"\n",
    "  },\n",
    "  \"graph\": {},\n",
    "  \"files\": {},\n",
    "  \"selection_query\": {\n",
    "    \n",
    "  }\n",
    "}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db2520",
   "metadata": {},
   "source": [
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfebfe7",
   "metadata": {},
   "source": [
    "##### Delete the Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691240b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$GATEWAY_URL/controller/removeBlock/gcp-cluster-2 \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"block_id\": \"vllm-block-demo-1\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f071bac-11c4-41ea-b0a8-fcb46b166e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30fdfab-4e07-477c-ae8b-0cdde9e37c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
