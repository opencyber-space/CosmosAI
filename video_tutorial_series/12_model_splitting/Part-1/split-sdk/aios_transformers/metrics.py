import time

class LLMMetrics:
    def __init__(self, metrics, block_id=None):
        self.metrics = metrics
        self.block_id = block_id
        self._register_llm_metrics()

    def _register_llm_metrics(self):
        self.metrics.register_counter("llm_prompts_total", "Total number of prompts processed")
        self.metrics.register_counter("llm_tokens_generated_total", "Total number of tokens generated by the LLM")
        self.metrics.register_counter("llm_prompt_tokens_total", "Total number of prompt tokens received")
        self.metrics.register_gauge("llm_active_sessions", "Current number of active LLM chat sessions")
        self.metrics.register_histogram("llm_inference_duration_seconds", "Duration of LLM inference in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10])
        self.metrics.register_histogram("llm_time_to_first_token_seconds", "Time to First Token (TTFT) in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1, 2])
        self.metrics.register_histogram("llm_time_per_output_token_seconds", "Time Per Output Token (TPOT) in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5])
        self.metrics.register_gauge("llm_tokens_per_second", "Number of tokens generated per second")
        self.metrics.register_gauge("llm_cpu_utilization", "CPU utilization percentage during inference")
        self.metrics.register_gauge("llm_gpu_utilization", "GPU utilization percentage during inference")
        self.metrics.register_gauge("llm_memory_usage_bytes", "Memory usage in bytes during inference")
        self.metrics.register_counter("llm_inference_errors_total", "Total number of inference errors")

    def log_prompt(self, prompt_token_count: int):
        self.metrics.increment_counter("llm_prompts_total")
        self._increment_counter_by("llm_prompt_tokens_total", prompt_token_count)

    def log_response(self, generated_token_count: int):
        self._increment_counter_by("llm_tokens_generated_total", generated_token_count)

    def observe_inference_time(self, start_time: float):
        elapsed = time.time() - start_time
        self.metrics.observe_histogram("llm_inference_duration_seconds", elapsed)

    def observe_time_to_first_token(self, start_time: float):
        ttft = time.time() - start_time
        self.metrics.observe_histogram("llm_time_to_first_token_seconds", ttft)

    def observe_time_per_output_token(self, start_time: float, token_count: int):
        if token_count > 0:
            tp = (time.time() - start_time) / token_count
            self.metrics.observe_histogram("llm_time_per_output_token_seconds", tp)

    def update_tokens_per_second(self, tokens_generated: int, duration_seconds: float):
        if duration_seconds > 0:
            tps = tokens_generated / duration_seconds
            self.metrics.set_gauge("llm_tokens_per_second", tps)

    def update_resource_utilization(self, cpu_percent: float = None, gpu_percent: float = None, memory_bytes: int = None):
        if cpu_percent is not None:
            self.metrics.set_gauge("llm_cpu_utilization", cpu_percent)
        if gpu_percent is not None:
            self.metrics.set_gauge("llm_gpu_utilization", gpu_percent)
        if memory_bytes is not None:
            self.metrics.set_gauge("llm_memory_usage_bytes", memory_bytes)

    def increment_inference_errors(self):
        self.metrics.increment_counter("llm_inference_errors_total")

    def increase_active_sessions(self):
        self._add_to_gauge("llm_active_sessions", 1)

    def decrease_active_sessions(self):
        self._add_to_gauge("llm_active_sessions", -1)

    # Helpers
    def _increment_counter_by(self, name: str, value: int):
        metric = self.metrics.metrics.get(name)
        if metric:
            metric.inc(value)

    def _add_to_gauge(self, name: str, delta: int):
        metric = self.metrics.metrics.get(name)
        if metric:
            current = metric._value.get()
            metric.set(current + delta)
