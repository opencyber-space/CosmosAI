{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f846f30",
   "metadata": {},
   "source": [
    "## üß† Model Splitting Across Nodes in AIOS Using Pytorch & Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0337c7d7",
   "metadata": {},
   "source": [
    "\n",
    "- Author: Shridhar Kini (Profile)\n",
    "- To Securely Run: `jupyter notebook password` to generate onetime password for secure access\n",
    "- To Run: `jupyter notebook --allow-root  --port 9999 --ip=0.0.0.0 part-1_model_splitting_using_pytorch.ipynb`\n",
    "- To Clear Outputs: Use `jupyter nbconvert --clear-output --inplace part-1_model_splitting_using_pytorch.ipynb`\n",
    "\n",
    "This notebook provides an overview of Model Splitting across Nodes in AIOS using Library support from Pytorch and Transformers. This Demo is for the users who has bigger models which cannot be fit in single GPU or Node. Splitting Models with the helper code of **split-sdk**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7459fe4b",
   "metadata": {},
   "source": [
    "### ‚õ©Ô∏è**I. Model Splitting (Parallelism) Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb373",
   "metadata": {},
   "source": [
    "#### Tensor Parallelism(TP)\n",
    "- Split a single operation(like matmul) across multiple devices\n",
    "    - Large Matrix multiplication op is split across GPUs\n",
    "        - Like Feed Forward Network and Attention Blocks\n",
    "        - Example: weight matrix W can be split by its columns [W_1,W_2,W_3,W_4] across 4 GPUs\n",
    "            - Each GPU multiplies input X with W_i to Give Y_i\n",
    "            - Use high-speed communication collective (like all-gather) to collect Y_i\n",
    "            - Form the final Output\n",
    "            - All GPUs are active on the same layer(W) at same time\n",
    "    - Pros ‚úÖ:\n",
    "        - High GPU Utilization: Keeps all participating GPUs busy, avoiding the idle \"bubbles\" seen in pipeline parallelism.\n",
    "        - Low Latency: Excellent for inference, as it speeds up the forward pass of individual large layers.\n",
    "    - Cons ‚ùå:\n",
    "        - High Communication Overhead: Requires very frequent, high-bandwidth communication     between GPUs. It is almost exclusively used within a single node connected by ultra-fast interconnects like NVIDIA's NVLink.\n",
    "        - Limited Scalability: Does not scale well beyond a small number of GPUs (typically 8 or 16) on a single server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551cd42e",
   "metadata": {},
   "source": [
    "#### Pipeline Parallelism (PP)\n",
    "- Splits the entire model into sequential chunks of layers (stages) and places each chunk on a different GPU.\n",
    "    - Similar to Assembly Line\n",
    "        - Each GPU processes a different micro-batch of data at the same time\n",
    "        - Output of one stage (x number of layers) from one GPU is the input to the next stage\n",
    "        - When one GPU finishes processing its micro-batch, it sends the output to the next GPU and starts processing the next micro-batch\n",
    "    - Pros ‚úÖ:\n",
    "        - Lower Communication Volume: Only needs to pass activations between adjacent GPUs, which is less data-intensive than TP's constant weight syncing.\n",
    "        - Scales Across Nodes: Can work effectively across multiple servers connected by slower networking like Ethernet, making it ideal for scaling to a huge number of GPUs.\n",
    "    - Cons ‚ùå:\n",
    "        - The \"Pipeline Bubble\": It's hard to keep the pipeline perfectly full. The first GPU will be idle at the end of a batch, and the last GPU is idle at the beginning, leading to wasted compute cycles.\n",
    "        - Load Balancing is Hard: The layers must be carefully divided so that each GPU has roughly the same amount of work to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bbe3e",
   "metadata": {},
   "source": [
    "#### Fully Sharded Data Parallelism (FSDP)\n",
    "- Mainly used for Training as it can shard the model weight, gradients and optimizers state across GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e4a953",
   "metadata": {},
   "source": [
    "### Concept of RANK, LOCAL_RANK, WORLD_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad22110",
   "metadata": {},
   "source": [
    "Think of your multi-node setup as a team of workers assigned to a large project. Each worker (GPU) has a specific role (rank) and works on a portion of the task (data parallelism).\n",
    "- `World Size`: This is the total number of workers (processes/GPUs) in your team. \n",
    "    - For example, if you have 2 nodes with 8 GPUs each, your World Size is 2 times 8 = 16.\n",
    "- `Rank`: This is the unique identifier for each worker in the team. \n",
    "    - It helps in distinguishing between different workers. \n",
    "    - For instance, in a setup with 16 GPUs, their ranks would be from 0 to 15.\n",
    "- `Local Rank`: Rank of a worker within its local node.\n",
    "    - If each node has 8 GPUs, then\n",
    "        - the local ranks for the GPUs in a single node would be from 0 to 7."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75339b4b",
   "metadata": {},
   "source": [
    "**To  GET PORT MAPPING wrt to Service**([Doc](https://docs.aigr.id/installation/installation/#deploying-registry-services))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395268e2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# üîß Configuration Setup - Run this cell first to set up shared variables\n",
    "import os\n",
    "\n",
    "# Set configuration variables that will be available across all cells\n",
    "GATEWAY_URL = \"MANAGEMENTMASTER:30600\"\n",
    "CLUSTER_ID = \"gcp-cluster-2\"\n",
    "GLOBAL_CLUSTER_METRICS_DB = \"MANAGEMENTMASTER:30202\"\n",
    "GLOBAL_BLOCK_METRICS_DB = \"MANAGEMENTMASTER:30201\"\n",
    "PARSER_URL = \"MANAGEMENTMASTER:30501\"\n",
    "GLOBAL_CLUSTER_DB = \"MANAGEMENTMASTER:30101\"\n",
    "GLOBAL_TASK_DB_SERVICE = \"MANAGEMENTMASTER:30108\"\n",
    "COMPONENT_REGISTRY_SERVICE = \"MANAGEMENTMASTER:30112\"\n",
    "GLOBAL_BLOCKDB_SERVICE = \"MANAGEMENTMASTER:30100\"\n",
    "GCP_CLUSTER_2_INFERENCE_SERVER = \"CLUSTER2_MASTER_IP:31504\"\n",
    "#SERVER_URL = \"10.10.10.10:5000\"  # For other API calls\n",
    "\n",
    "# Set environment variables for bash cells\n",
    "os.environ['GATEWAY_URL'] = GATEWAY_URL\n",
    "os.environ['CLUSTER_ID'] = CLUSTER_ID\n",
    "os.environ['GLOBAL_CLUSTER_METRICS_DB'] = GLOBAL_CLUSTER_METRICS_DB\n",
    "os.environ['GLOBAL_BLOCK_METRICS_DB'] = GLOBAL_BLOCK_METRICS_DB\n",
    "os.environ['PARSER_URL'] = PARSER_URL\n",
    "os.environ['GLOBAL_CLUSTER_DB'] = GLOBAL_CLUSTER_DB\n",
    "os.environ['GLOBAL_TASK_DB_SERVICE'] = GLOBAL_TASK_DB_SERVICE\n",
    "os.environ['COMPONENT_REGISTRY_SERVICE'] = COMPONENT_REGISTRY_SERVICE\n",
    "os.environ['GLOBAL_BLOCKDB_SERVICE'] = GLOBAL_BLOCKDB_SERVICE\n",
    "os.environ['GCP_CLUSTER_2_INFERENCE_SERVER'] = GCP_CLUSTER_2_INFERENCE_SERVER\n",
    "#os.environ['SERVER_URL'] = SERVER_URL\n",
    "\n",
    "print(\"‚úÖ Configuration variables set:\")\n",
    "print(f\"   ‚Ä¢ GATEWAY_URL: {GATEWAY_URL}\")\n",
    "print(f\"   ‚Ä¢ CLUSTER_ID: {CLUSTER_ID}\")\n",
    "print(f\"   ‚Ä¢ GLOBAL_CLUSTER_METRICS_DB: {GLOBAL_CLUSTER_METRICS_DB}\")\n",
    "print(\"\\nüìù These variables are now available in both Python and bash cells!\")\n",
    "print(\"   - In Python: use GATEWAY_URL, CLUSTER_ID, GLOBAL_CLUSTER_METRICS_DB PARSER_URL GLOBAL_CLUSTER_DB GLOBAL_TASK_DB_SERVICE\")\n",
    "print(\"   - In bash: use $GATEWAY_URL, $CLUSTER_ID, $GLOBAL_CLUSTER_METRICS_DB $PARSER_URL $GLOBAL_CLUSTER_DB $GLOBAL_TASK_DB_SERVICE\")\n",
    "os.system('echo $GATEWAY_URL')\n",
    "os.system('echo $CLUSTER_ID')\n",
    "os.system('echo $GLOBAL_CLUSTER_METRICS_DB')\n",
    "os.system('echo $PARSER_URL')\n",
    "os.system('echo $GLOBAL_CLUSTER_DB')\n",
    "os.system('echo $GLOBAL_TASK_DB_SERVICE')\n",
    "os.system('echo $COMPONENT_REGISTRY_SERVICE')\n",
    "os.system('echo $GLOBAL_BLOCKDB_SERVICE')\n",
    "os.system('echo $GLOBAL_BLOCK_METRICS_DB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde50d97",
   "metadata": {},
   "source": [
    "### Code Sample for Splitting and Generating Response ([Code](model_splitting/split-sdk))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c73dbd",
   "metadata": {},
   "source": [
    "##### Main Codes\n",
    "- aios_transformers/apis.py - Exposes the API for model inferencing\n",
    "- aios_transformers/sdk.py - Contains the core logic for model splitting and distributed inference\n",
    "- aios_transformers/metrics.py - Helper for metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e48f2",
   "metadata": {},
   "source": [
    "##### To Build the Model Splitter Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852d7e8",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "bash Part-1/split-sdk/build_docker.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bab05a",
   "metadata": {},
   "source": [
    "##### Push the built docker to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961b87ee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker push MANAGEMENTMASTER:31280/example/split-runner:demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc5e8ec",
   "metadata": {},
   "source": [
    "##### **Deploy the Model Splitter for inferencing**\n",
    "**To Know more about these API's Please check the Link**([Link](https://github.com/OpenCyberspace/AIGr.id/blob/main/services/applications/model_splits/core/apis.py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a01929",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://MANAGEMENTMASTER:30160/splits/create \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "  \"rank_0_cluster_id\": \"gcp-cluster-2\",\n",
    "  \"cluster_id\": [\n",
    "    \"gcp-cluster-2\"\n",
    "  ],\n",
    "  \"deployment_name\": \"phi-128k-2\",\n",
    "  \"nnodes\": 4,\n",
    "  \"common_params\": {\n",
    "    \"model_name\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    \"image\": \"MANAGEMENTMASTER:31280/example/split-runner:demo\",\n",
    "    \"master_port\": 3000\n",
    "  },\n",
    "  \"per_rank_params\": [\n",
    "    {\n",
    "      \"rank\": 0,\n",
    "      \"node_id\": \"wc-gpu-node2\",\n",
    "      \"nccl_socket_ifname\": \"eth0\",\n",
    "      \"nvidia_visible_devices\": \"0\",\n",
    "      \"cluster_id\": \"gcp-cluster-2\",\n",
    "      \"cuda_visible_devices\": \"0\"\n",
    "    },\n",
    "    {\n",
    "      \"rank\": 1,\n",
    "      \"node_id\": \"wc-gpu-node2\",\n",
    "      \"nccl_socket_ifname\": \"eth0\",\n",
    "      \"nvidia_visible_devices\": \"0,1\",\n",
    "      \"cuda_visible_devices\": \"1\",\n",
    "      \"cluster_id\": \"gcp-cluster-2\"\n",
    "    },\n",
    "    {\n",
    "      \"rank\": 2,\n",
    "      \"node_id\": \"wc-gpu-node3\",\n",
    "      \"nccl_socket_ifname\": \"eth0\",\n",
    "      \"nvidia_visible_devices\": \"0\",\n",
    "      \"cuda_visible_devices\": \"0\",\n",
    "      \"cluster_id\": \"gcp-cluster-2\"\n",
    "    },\n",
    "    {\n",
    "      \"rank\": 3,\n",
    "      \"node_id\": \"wc-gpu-node3\",\n",
    "      \"nccl_socket_ifname\": \"eth0\",\n",
    "      \"nvidia_visible_devices\": \"1\",\n",
    "      \"cuda_visible_devices\": \"0,1\",\n",
    "      \"cluster_id\": \"gcp-cluster-2\"\n",
    "    }\n",
    "  ],\n",
    "  \"multi_cluster\": false,\n",
    "  \"platform\": \"torch\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d9def8",
   "metadata": {},
   "source": [
    "##### To check the k8s pod(to be done in target cluster Controle Plane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb73194c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "kubectl get pods -n splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb12a1",
   "metadata": {},
   "source": [
    "#### **Create Proxy-AIOS-Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb90e4a",
   "metadata": {},
   "source": [
    "- Proxy Block can do inference with Splitted Model Rank-0\n",
    "- Proxy Block can hold chat history kind of logic\n",
    "- Proxy Block uses internal namespace based URL for inference. If not we need to create ClusterIP to NodePort for Model Split Pod Service\n",
    "    - f\"{deploymentNameOfSplittingService}-rank-master.svc.cluster.local:8080/generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d358474",
   "metadata": {},
   "source": [
    "##### Build Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491e91ee",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "bash Part-1/block/block-client/build_docker.bash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d260cdb6",
   "metadata": {},
   "source": [
    "##### Push the Docker Image to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f9d2a7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker push MANAGEMENTMASTER:31280/pytorch-split-client:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e1f90",
   "metadata": {},
   "source": [
    "##### Register the component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4659e3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$COMPONENT_REGISTRY_SERVICE/api/registerComponent \\\n",
    " -d @Part-1/block/component.json \\\n",
    " -H \"Content-Type: application/json\" | json_pp\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8882b447",
   "metadata": {},
   "source": [
    "##### Unregister Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c848ef",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$COMPONENT_REGISTRY_SERVICE/api/unregisterComponent \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"uri\":\"model.pytorch-runner:1.0.0-stable\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4628464f",
   "metadata": {},
   "source": [
    "##### Deploy the Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aef556",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST -d @Part-1/block/block.json \\\n",
    " -H \"Content-Type: application/json\" \\\n",
    "  http://$PARSER_URL/api/createBlock | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04b806c",
   "metadata": {},
   "source": [
    "##### To get the log of Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0bd05",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "kubectl logs -f pytorch-block-3-in-v66d-d57fc6d5c-v9x8j -n blocks instance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30cd1ee",
   "metadata": {},
   "source": [
    "##### **Create Inference Server For The Cluster**\n",
    "- Run this commands in your cluster node(like master node)\n",
    "    - `kubectl create namespace inference-server`\n",
    "    - `kubectl create -f inference_server/inference_server.yaml`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18220825",
   "metadata": {},
   "source": [
    "##### **Do the Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b27425",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST  http://CLUSTER2_MASTER_IP:31504/v1/infer \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "  \"model\": \"pytorch-block-3\",\n",
    "  \"session_id\": \"session-3\",\n",
    "  \"seq_no\": 20,\n",
    "  \"data\": {\n",
    "    \"mode\": \"completions\",\n",
    "    \"generation_config\": {\n",
    "                \"max_new_tokens\": 1024,\n",
    "                \"do_sample\": false,\n",
    "                \"top_k\": 50,\n",
    "                \"top_p\": 0.95,\n",
    "                \"temperature\": 1.0\n",
    "            },\n",
    "    \"message\": \"Give me code for adding two integers list element wise in python\",\n",
    "    \"system_message\": \"You are a helpful assistant that provides code examples.\"\n",
    "  },\n",
    "  \"graph\": {},\n",
    "  \"selection_query\": {\n",
    "    \n",
    "  }\n",
    "}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db2520",
   "metadata": {},
   "source": [
    "#### Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b55555",
   "metadata": {},
   "source": [
    "##### Delete Model Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc0665",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X DELETE http://MANAGEMENTMASTER:30160/splits/delete/phi-128k-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfebfe7",
   "metadata": {},
   "source": [
    "##### Delete the Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c691240b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "curl -X POST http://$GATEWAY_URL/controller/removeBlock/gcp-cluster-2 \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"block_id\": \"pytorch-block-3\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f071bac-11c4-41ea-b0a8-fcb46b166e04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
