{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4ce89ae",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Model Onboarding in AIOS\n",
    "\n",
    "Welcome to this guide on onboarding a model to the OpenOS/AIGr.id platform. This notebook serves as a standalone tutorial that walks you through every step of the process, from registering your model as a digital asset to running inference and managing its lifecycle. We will use the `gemma3_llama_cpp` model as our primary example.\n",
    "\n",
    "## The AIOS Ecosystem: A Brief Overview\n",
    "- [AIOS Ecosystem Architecture](https://docs.aigr.id/assets/aios-all-arch.drawio.png)\n",
    "Before we dive in, it's helpful to understand the key components of the AIOS ecosystem. AIOS is designed as a decentralized, modular, and extensible platform for AI development and deployment. Its architecture consists of several core services that work together to manage the lifecycle of AI models and applications. These include:\n",
    "\n",
    "- **Asset DB Registry**: A central catalog for discovering and managing versioned, runnable software components (Assets).\n",
    "- **Resource Allocator**: Responsible for scheduling and allocating resources for running Assets on the network of clusters.\n",
    "- **Cluter Controller**: Manages the lifecycle of a Block, which is a running instance of an Asset.\n",
    "- **Metrics System**: A comprehensive system for monitoring the health, status, and performance of Blocks.\n",
    "\n",
    "This guide will touch on each of these components as we walk through the model onboarding process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c22a8",
   "metadata": {},
   "source": [
    "## Onboarding Process Overview\n",
    "\n",
    "In this guide, we will cover the following steps in detail:\n",
    "\n",
    "1. **Registering an Asset**: We will create a `component.json` file that describes our model and register it with the AIOS Component Registry.\n",
    "2. **Allocating a Block**: We will define the block's configuration in an `allocation.json` file and allocate a block using the AIOS API.\n",
    "3. **Checking Block Status & Metrics**: We will use `curl` commands to check the block's status, health, and metrics.\n",
    "4. **Performing Inference**: We will write a Python script to send an inference request to the block via gRPC.\n",
    "5. **Chat UI with streaming**: We will launch an interactive chat interface to demonstrate streaming capabilities.\n",
    "6. **Cleaning Up**: We will deallocate the block and deregister the asset when done.\n",
    "\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e450032f",
   "metadata": {},
   "source": [
    "\n",
    "The following image provides a high-level overview of the entire model onboarding process, from defining the asset to monitoring the running service.\n",
    "\n",
    "<!-- ![Model Onboarding Process](block_onboarding.gif) -->\n",
    "<img src=\"onboarding.png\" alt=\"Custom Autoscaler\" width=\"1000\" height=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcb97fc-c9cc-4d27-8f72-985ebea16335",
   "metadata": {},
   "source": [
    "### A note on using prebuilt docker images. \n",
    "Below docker images can be a quick start for testing the features of AIOS. For custom model onboarding refer \n",
    "    - [Custom Model Onboarding](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/llm-docs/llm-method-1.md)\n",
    "- `MANAGEMENTMASTER:31280/llama4-scout-17b:v1`\n",
    "- `MANAGEMENTMASTER:31280/gemma3-27b:v1`\n",
    "- `MANAGEMENTMASTER:31280/deepseek-r1-distill-70b:v1`\n",
    "- `MANAGEMENTMASTER:31280/magistral-small-2506-llama-cpp:v1`\n",
    "- `MANAGEMENTMASTER:31280/qwen-3-32b-llama-cpp:v1`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5fcb8",
   "metadata": {},
   "source": [
    "## 1. Registering an Asset\n",
    "\n",
    "First, we need to define our model as an **Asset**. An asset is a static, versioned, and runnable software component that is registered in the AIOS **Asset DB Registry**. The registry acts as a central catalog, allowing developers to discover, share, and reuse assets across the ecosystem. By registering an asset, you are making it available for deployment on the AIOS network.\n",
    "\n",
    "For a deeper dive into how the asset registry works, you can refer to the official documentation:\n",
    "- [Asset DB Registry Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/assets-db-registry/assets-db-registry.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f3a79",
   "metadata": {},
   "source": [
    "Let's look at the `component.json` file for `gemma3_llama_cpp`. This file is the asset's manifest, containing all the essential information AIOS needs. It defines:\n",
    "- **`componentId`**: The unique name, version, and release tag for the asset.\n",
    "- **`componentType`**: Specifies that this is a `model` asset.\n",
    "- **`containerRegistryInfo`**: Points to the container image (`gemma3-27b:v1`) and includes metadata like the author and a description.\n",
    "- **`componentMetadata`**: A rich set of details including the model's use case (`chat-completion, multi-modal`), hardware requirements (`gpu`), performance benchmarks (`MMLU`, `HellaSwag`), and known limitations.\n",
    "- **`componentInitData`**: Specifies the default model files to be loaded, such as the GGUF file for the model (`gemma-3-27b-it-UD-Q8_K_XL.gguf`) and the CLIP model for multi-modal capabilities.\n",
    "\n",
    "This file effectively serves as a comprehensive \"passport\" for the model within the AIOS ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41443d63-f74a-4ca2-b9db-c10c347879a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"componentId\": {\n",
      "    \"name\": \"gemma3-27b\",\n",
      "    \"version\": \"1.0.0\",\n",
      "    \"releaseTag\": \"stable\"\n",
      "  },\n",
      "  \"componentType\": \"model\",\n",
      "  \"containerRegistryInfo\": {\n",
      "    \"containerImage\": \"MANAGEMENTMASTER:31280/gemma3-27b:v1\",\n",
      "    \"containerRegistryId\": \"MANAGEMENTMASTER:31280/gemma3-27b:v1\",\n",
      "    \"containerImageMetadata\": {\n",
      "      \"author\": \"llm-team\",\n",
      "      \"description\": \"AIOS block for chat and multi-modal inference using llama-cpp-python with Gemma-3-27B and CLIP support\"\n",
      "    },\n",
      "    \"componentMode\": \"aios\"\n",
      "  },\n",
      "  \"componentMetadata\": {\n",
      "    \"usecase\": \"chat-completion, multi-modal\",\n",
      "    \"framework\": \"llama-cpp-python\",\n",
      "    \"hardware\": \"gpu\",\n",
      "    \"supports_local_models\": true,\n",
      "    \"model_path_resolution\": \"automatic\",\n",
      "    \"supports_quantization\": true,\n",
      "    \"quantization_methods\": [\n",
      "      \"4bit\",\n",
      "      \"8bit\",\n",
      "      \"fp16\",\n",
      "      \"fp8\"\n",
      "    ],\n",
      "    \"provider\": {\n",
      "      \"name\": \"AIOS-Internal\",\n",
      "      \"modelIdentifier\": \"gemma3-27b-v1\"\n",
      "    },\n",
      "    \"architecture\": {\n",
      "      \"contextLength\": 8192,\n",
      "      \"parameterCountB\": 27\n",
      "    },\n",
      "    \"capabilities\": {\n",
      "      \"supportsStreaming\": true\n",
      "    },\n",
      "    \"dataUsagePolicy\": {\n",
      "      \"usedForTraining\": false,\n",
      "      \"policyStatement\": \"User data is not used for training or improving this model.\"\n",
      "    },\n",
      "    \"biasAndFairness\": {\n",
      "      \"assessmentSummary\": \"Model has undergone internal safety evaluations. Like all LLMs, it may reflect biases present in its training data.\",\n",
      "      \"knownBiases\": [\n",
      "        \"potential for generating stereotypical content\",\n",
      "        \"English-language cultural bias\"\n",
      "      ]\n",
      "    },\n",
      "    \"knownLimitations\": \"The model's knowledge cutoff is mid-2024. It may not be aware of events after that date and can sometimes produce plausible but incorrect information.\",\n",
      "    \"trainingDetails\": {\n",
      "      \"trainingData\": {\n",
      "        \"datasetName\": \"Mixed Web and Synthetic Data\",\n",
      "        \"datasetSize\": \"Not Disclosed\",\n",
      "        \"dataPreprocessing\": \"Extensive filtering for safety, quality, and removal of personal information.\"\n",
      "      },\n",
      "      \"trainingProcedure\": {\n",
      "        \"trainingFramework\": \"JAX\",\n",
      "        \"hyperparameters\": {\n",
      "          \"learningRate\": \"Not Disclosed\",\n",
      "          \"batchSize\": \"Not Disclosed\",\n",
      "          \"optimizer\": \"Not Disclosed\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"evaluation\": {\n",
      "      \"benchmarks\": {\n",
      "        \"MMLU\": {\n",
      "          \"metric\": \"5-shot accuracy\",\n",
      "          \"value\": 82.3\n",
      "        },\n",
      "        \"HellaSwag\": {\n",
      "          \"metric\": \"10-shot\",\n",
      "          \"value\": 90.1\n",
      "        },\n",
      "        \"ARC-Challenge\": {\n",
      "          \"metric\": \"25-shot\",\n",
      "          \"value\": 95.2\n",
      "        }\n",
      "      },\n",
      "      \"evaluationData\": {\n",
      "        \"datasetSummary\": \"Evaluated on a broad range of academic benchmarks covering reasoning, math, and coding.\"\n",
      "      }\n",
      "    }\n",
      "  },\n",
      "  \"componentInitData\": {\n",
      "    \"model_name\": \"gemma-3-27b-it-UD-Q8_K_XL/gemma-3-27b-it-UD-Q8_K_XL.gguf\",\n",
      "    \"clip_model_name\": \"gemma-3-27b-it-UD-Q8_K_XL/mmproj-F16.gguf\"\n",
      "  },\n",
      "  \"componentInitParametersProtocol\": {\n",
      "    \"temperature\": {\n",
      "      \"type\": \"number\",\n",
      "      \"description\": \"Sampling temperature\",\n",
      "      \"min\": 0.0,\n",
      "      \"max\": 1.0\n",
      "    }\n",
      "  },\n",
      "  \"componentInitSettingsProtocol\": {\n",
      "        \"cleanup_enabled\": {\n",
      "          \"type\": \"boolean\",\n",
      "          \"description\": \"To enable the Clean Up of sessions\",\n",
      "          \"default\": true\n",
      "        },\n",
      "    \"cleanup_check_interval\": {\n",
      "      \"type\": \"number\",\n",
      "      \"description\": \"interval(in seconds) with which check happens for cleanup\",\n",
      "      \"min\": 1,\n",
      "      \"max\": 10000\n",
      "    },\n",
      "    \"cleanup_session_timeout\": {\n",
      "      \"type\": \"number\",\n",
      "      \"description\": \"Session ID will be removed beyond this timeout this threshold in seconds\",\n",
      "      \"min\": 1,\n",
      "      \"max\": 18000\n",
      "    },\n",
      "    \"gen_params\": {\n",
      "      \"type\": \"object\",\n",
      "      \"description\": \"Parameters needed for inference/chat/generation\",\n",
      "      \"properties\": {\n",
      "        \"max_new_tokens\": {\n",
      "          \"type\": \"number\",\n",
      "          \"description\": \"Maximum number of tokens to generate\",\n",
      "          \"min\": 1,\n",
      "          \"max\": 40960\n",
      "        },\n",
      "        \"temperature\": {\n",
      "          \"type\": \"float\",\n",
      "          \"description\": \"Sampling temperature\",\n",
      "          \"min\": 0.0,\n",
      "          \"max\": 1.0\n",
      "        },\n",
      "        \"top_k\": {\n",
      "          \"type\": \"number\",\n",
      "          \"description\": \"Topk \",\n",
      "          \"min\": 1,\n",
      "          \"max\": 1000\n",
      "        },\n",
      "        \"top_p\": {\n",
      "          \"type\": \"float\",\n",
      "          \"description\": \"top_p\",\n",
      "          \"min\": 0.0,\n",
      "          \"max\": 1.0\n",
      "        },\n",
      "        \"repetition_penalty\": {\n",
      "          \"type\": \"float\",\n",
      "          \"description\": \"repetition_penalty\",\n",
      "          \"min\": 0.0,\n",
      "          \"max\": 10.0\n",
      "        },\n",
      "        \"do_sample\": {\n",
      "          \"type\": \"boolean\",\n",
      "          \"description\": \"do_sample\",\n",
      "          \"default\": true\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"tensor_parallel\": {\n",
      "      \"type\": \"boolean\",\n",
      "      \"description\": \"Enable tensor parallelism\",\n",
      "      \"default\": true\n",
      "    },\n",
      "    \"device\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Device to run model on\",\n",
      "      \"default\": \"cuda\"\n",
      "    },\n",
      "    \"quantization_type\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization\",\n",
      "      \"enum\": [\n",
      "        \"4bit\",\n",
      "        \"8bit\",\n",
      "        \"fp16\",\n",
      "        \"fp8\"\n",
      "      ],\n",
      "      \"default\": null\n",
      "    }\n",
      "  },\n",
      "  \"componentInputProtocol\": {\n",
      "    \"mode\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Operation mode: chat, generate, tokens, embed\",\n",
      "      \"default\": \"chat\"\n",
      "    },\n",
      "    \"messages\": {\n",
      "      \"type\": \"array\",\n",
      "      \"description\": \"Input message from the user (for chat mode)\"\n",
      "    },\n",
      "    \"prompt\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Input prompt (for generate/tokens mode)\"\n",
      "    },\n",
      "    \"text\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Input text (for embed mode)\"\n",
      "    },\n",
      "    \"session_id\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Unique identifier for the chat session\",\n",
      "      \"default\": \"default\"\n",
      "    },\n",
      "    \"system_message\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"System message for chat initialization\"\n",
      "    },\n",
      "    \"generation_kwargs\": {\n",
      "      \"type\": \"object\",\n",
      "      \"description\": \"Additional generation parameters\"\n",
      "    }\n",
      "  },\n",
      "  \"componentOutputProtocol\": {\n",
      "    \"reply\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Chat reply from the model (chat mode)\"\n",
      "    },\n",
      "    \"generated\": {\n",
      "      \"type\": \"string\",\n",
      "      \"description\": \"Generated text (generate mode)\"\n",
      "    },\n",
      "    \"tokens\": {\n",
      "      \"type\": \"array\",\n",
      "      \"description\": \"Generated token IDs (tokens mode)\"\n",
      "    },\n",
      "    \"embedding\": {\n",
      "      \"type\": \"array\",\n",
      "      \"description\": \"Text embedding vector (embed mode)\"\n",
      "    }\n",
      "  },\n",
      "  \"componentParameters\": {\n",
      "    \"temperature\": 0.2\n",
      "  },\n",
      "  \"componentInitSettings\": {\n",
      "    \"tensor_parallel\": true,\n",
      "    \"device\": \"cuda\",\n",
      "    \"quantization_type\": \"8bit\",\n",
      "    \"gen_params\": {\n",
      "      \"max_new_tokens\": 2048,\n",
      "      \"temperature\": 0.2,\n",
      "      \"top_k\": 50,\n",
      "      \"top_p\": 0.95,\n",
      "      \"repetition_penalty\": 1.1,\n",
      "      \"do_sample\": true\n",
      "    }\n",
      "  },\n",
      "  \"componentManagementCommandsTemplate\": {\n",
      "    \"reload_model\": {\n",
      "      \"description\": \"Reloads the model with optional new parameters\",\n",
      "      \"args\": {\n",
      "        \"gen_params\": {\n",
      "          \"type\": \"object\",\n",
      "          \"description\": \"Parameters needed for inference/chat/generation\",\n",
      "          \"properties\": {\n",
      "            \"max_new_tokens\": {\n",
      "              \"type\": \"number\",\n",
      "              \"description\": \"Maximum number of tokens to generate\",\n",
      "              \"min\": 1,\n",
      "              \"max\": 40960\n",
      "            },\n",
      "            \"temperature\": {\n",
      "              \"type\": \"float\",\n",
      "              \"description\": \"Sampling temperature\",\n",
      "              \"min\": 0.0,\n",
      "              \"max\": 1.0\n",
      "            },\n",
      "            \"top_k\": {\n",
      "              \"type\": \"number\",\n",
      "              \"description\": \"Topk \",\n",
      "              \"min\": 1,\n",
      "              \"max\": 1000\n",
      "            },\n",
      "            \"top_p\": {\n",
      "              \"type\": \"float\",\n",
      "              \"description\": \"top_p\",\n",
      "              \"min\": 0.0,\n",
      "              \"max\": 1.0\n",
      "            },\n",
      "            \"repetition_penalty\": {\n",
      "              \"type\": \"float\",\n",
      "              \"description\": \"repetition_penalty\",\n",
      "              \"min\": 0.0,\n",
      "              \"max\": 10.0\n",
      "            },\n",
      "            \"do_sample\": {\n",
      "              \"type\": \"boolean\",\n",
      "              \"description\": \"do_sample\",\n",
      "              \"default\": true\n",
      "            }\n",
      "          }\n",
      "        },\n",
      "        \"tensor_parallel\": {\n",
      "          \"type\": \"boolean\",\n",
      "          \"description\": \"Enable tensor parallelism\",\n",
      "          \"default\": true\n",
      "        },\n",
      "        \"device\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Device to run model on\",\n",
      "          \"default\": \"cuda\"\n",
      "        },\n",
      "        \"quantization_type\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization\",\n",
      "          \"enum\": [\n",
      "            \"4bit\",\n",
      "            \"8bit\",\n",
      "            \"fp16\",\n",
      "            \"fp8\"\n",
      "          ],\n",
      "          \"default\": null\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"set_gen_params\": {\n",
      "      \"description\": \"Updates generation configuration parameters\",\n",
      "      \"args\": {\n",
      "        \"gen_params\": {\n",
      "          \"type\": \"object\",\n",
      "          \"description\": \"New generation configuration\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"device_info\": {\n",
      "      \"description\": \"Returns device and model diagnostics\",\n",
      "      \"args\": {}\n",
      "    },\n",
      "    \"remove_chat_session\": {\n",
      "      \"description\": \"Removes a specific chat session\",\n",
      "      \"args\": {\n",
      "        \"session_id\": {\n",
      "          \"type\": \"string\",\n",
      "          \"description\": \"Session ID to remove\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    \"update_cleanup_config\": {\n",
      "              \"description\": \"Update Clean Up Config\",\n",
      "              \"args\": {\n",
      "                \"cleanup_config\": {\n",
      "                  \"type\": \"object\",\n",
      "                  \"description\": \"Send enabled: boolen, check_interval: in seconds for keep checcking, session_timeout: remove session beyond this in seconds\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "  },\n",
      "  \"tags\": [\n",
      "    \"llama-cpp-python\",\n",
      "    \"chat\",\n",
      "    \"multi-modal\",\n",
      "    \"gpu\",\n",
      "    \"quantization\",\n",
      "    \"local-models\"\n",
      "  ]\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat component.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ca26905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  9654  100    91  100  9563  13622  1398k --:--:-- --:--:-- --:--:-- 1571k\n",
      "{\n",
      "   \"error\" : true,\n",
      "   \"message\" : \"Component with URI model.gemma3-27b:1.0.0-stable already exists.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Register the asset with the AIOS Component Registry\n",
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/registerComponent -H \"Content-Type: application/json\" -d @./component.json | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47abb055-0921-441c-bd8c-67a1ac9b2151",
   "metadata": {},
   "source": [
    "## 2. Allocating a Block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30a77b2",
   "metadata": {},
   "source": [
    "Now that the asset is registered, we can create a running instance of it. In AIOS, a running instance of an asset is called a **Block**. A Block is the fundamental unit of execution in AIOS, responsible for serving inference requests or running any computational workload defined by an Asset. When you allocate a Block, the AIOS **Resource Allocator** finds a suitable cluster and schedules the Block for execution.\n",
    "\n",
    "For more details, you can refer to the official documentation:\n",
    "- [Block Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md)\n",
    "- [Block APIs/Services](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-services)\n",
    "\n",
    "\n",
    "To allocate a block, we must provide an `allocation.json` file. This file is a request to the Resource Allocator, specifying the desired configuration,specification for the block. Let's examine the key fields in our `allocation.json` for the `gemma3_llama_cpp` model. This file tells AIOS:\n",
    "\n",
    "- **`blockId`**: We are requesting a block named `gemma3-27b-block`.\n",
    "- **`blockComponentURI`**: The block should be an instance of the `model.gemma3-27b:1.0.0-stable` asset we registered earlier.\n",
    "- **`blockInitData`**: This section provides initial data to the block. Here we specify the `model_name` (`gemma-3-27b-it-UD-Q8_K_XL.gguf`) and the `clip_model_name` (`mmproj-F16.gguf`) for multi-modal capabilities.\n",
    "- **`initSettings`**: These are settings for the block's runtime. We have settings for `tensor_parallel`, `device` (`cuda`), `quantization_type` (`fp16`), and `gen_params` which control the model's generation behavior (e.g., `max_new_tokens`, `temperature`).\n",
    "- **`policyRulesSpec`**: This is a crucial section that defines the chain of policies governing the block's behavior:\n",
    "    - **`accessChecker`**: Like a policing system which is embedded in `AIOS ecosystem` ,can be restricted zones ,region related polcies\n",
    "    - **`clusterAllocator`**: Selects a specific cluster, in this case, `gcp-cluster-2`.\n",
    "    - **`resourceAllocator`**: Assigns the block to a specific node (`wc-gpu-node1`) and GPU (`0`).\n",
    "    - **`loadBalancer`**: Manages how requests are distributed, configured here to cache sessions.\n",
    "    - **`stabilityChecker`**: Defines a health check mechanism to ensure the block is running correctly.\n",
    "    - **`autoscaler`**: Enables autoscaling for the block, an example could be targeting token utilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf0ac3a3-619e-41f0-8ed8-bedb2f0e2109",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"head\": {\n",
      "        \"templateUri\": \"Parser/V1\",\n",
      "        \"parameters\": {}\n",
      "    },\n",
      "    \"body\": {\n",
      "        \"spec\": {\n",
      "            \"values\": {\n",
      "                \"mode\": \"allocate\",\n",
      "                \"blockId\": \"gemma3-27b-block2\",\n",
      "                \"blockComponentURI\": \"model.gemma3-27b:1.0.0-stable\",\n",
      "                \"minInstances\": 1,\n",
      "                \"maxInstances\": 3,\n",
      "                \"blockInitData\": {\n",
      "                    \"model_name\": \"gemma-3-27b-it-UD-Q8_K_XL/gemma-3-27b-it-UD-Q8_K_XL.gguf\",\n",
      "                    \"clip_model_name\": \"gemma-3-27b-it-UD-Q8_K_XL/mmproj-F16.gguf\"\n",
      "                },\n",
      "                \"initSettings\": {\n",
      "                    \"tensor_parallel\": true,\n",
      "                    \"device\": \"cuda\",\n",
      "                    \"quantization_type\": \"fp16\",\n",
      "                    \"cleanup_enabled\":  true,\n",
      "                    \"cleanup_check_interval\": 60,\n",
      "                    \"cleanup_session_timeout\": 120,\n",
      "                    \"gen_params\": {\n",
      "                        \"max_new_tokens\": 2048,\n",
      "                        \"temperature\": 0.2,\n",
      "                        \"top_k\": 50,\n",
      "                        \"top_p\": 0.95,\n",
      "                        \"repetition_penalty\": 1.1,\n",
      "                        \"do_sample\": true\n",
      "                    }\n",
      "                },\n",
      "                \"policyRulesSpec\": [\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"clusterAllocator\",\n",
      "                            \"policyRuleURI\": \"cluster-selector:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"filter\": {\n",
      "                                    \"clusterQuery\": {\n",
      "                                        \"variable\": \"id\",\n",
      "                                        \"operator\": \"==\",\n",
      "                                        \"value\": \"gcp-cluster-2\"\n",
      "                                    }\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"max_candidates\": 2\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"resourceAllocator\",\n",
      "                            \"policyRuleURI\": \"allocator:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"allocation_data\": {\n",
      "                                    \"node_id\": \"wc-gpu-node2\",\n",
      "                                    \"gpus\": [0,1]\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"selection_mode\": \"balanced\"\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"loadBalancer\",\n",
      "                            \"policyRuleURI\": \"token-based-lb:1.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"output_token_weight\": 0.75,\n",
      "                                \"input_token_weight\":0.25\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"session_cache_size\": 2000\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"stabilityChecker\",\n",
      "                            \"policyRuleURI\": \"health_checker:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"unhealthy_threshold\": 2\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"check_interval_sec\": 10\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"autoscaler\",\n",
      "                            \"policyRuleURI\": \"tokenautoscaler:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"input_tokens_up_threshold\": 1500,\n",
      "                                \"output_tokens_up_threshold\": 1300,\n",
      "                                \"input_tokens_down_threshold\": 500,\n",
      "                                \"output_tokens_down_threshold\":300,\n",
      "                                \"averaging_period\": \"average_1m\"\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"scale_up_cooldown\": 45,\n",
      "                                \"scale_down_cooldown\": 90\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat allocation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5753721d-0069-4a6a-b7be-ef123eaba82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"result\": {\n",
      "    \"data\": {\n",
      "      \"message\": \"task scheduled in background\",\n",
      "      \"task_id\": \"67bd2c5c-d2a0-4498-bf33-93d3f2d3f675\"\n",
      "    },\n",
      "    \"success\": true\n",
      "  },\n",
      "  \"success\": true,\n",
      "  \"task_id\": \"\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Now, let's allocate the block using `curl`.\n",
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297df77f",
   "metadata": {},
   "source": [
    "## 3. Checking Block Status and Metrics\n",
    "\n",
    "After allocating the block, it's important to check its status to ensure it's running correctly. AIOS provides a comprehensive **Metrics System** that exposes endpoints for health status, and resource consumption, giving you a complete picture of your block's operational state. The Metrics System is designed to be extensible, allowing you to define and expose custom metrics for your applications.\n",
    "\n",
    "For more information on the metrics system, see the documentation:\n",
    "- [Metrics System Overview-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#metrics-system)\n",
    "- [Custom Metrics-covered in later tutorial](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/metrics-system/metrics-system.md#custom-metrics)\n",
    "\n",
    "Let's check the health status and metrics of our block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4ce922",
   "metadata": {},
   "source": [
    "### Check Block Health\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f79afd2d",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"block_id\": \"gemma3-27b-block2\",\n",
      "  \"healthy\": true,\n",
      "  \"instances\": [\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"executor\",\n",
      "      \"reason\": \"executor instance\"\n",
      "    },\n",
      "    {\n",
      "      \"healthy\": true,\n",
      "      \"instanceId\": \"in-0kcz\",\n",
      "      \"lastMetrics\": \"22.805049657821655s ago\"\n",
      "    }\n",
      "  ],\n",
      "  \"success\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Health - This command verifies the health of the running service within the block.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/health/gemma3-27b-block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82315c00-1c9a-4c42-a149-9f6659a02615",
   "metadata": {},
   "source": [
    "### Check Block Metrics(Before Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2348a6-abf3-4f44-aa5c-900f4866050d",
   "metadata": {},
   "source": [
    "#### Core Metric Categories\n",
    "\n",
    "##### üîß **Runtime Metrics**\n",
    "- **CPU Usage**: Real-time CPU utilization percentage for the block container\n",
    "- **Memory Usage**: Current memory consumption and peak memory usage\n",
    "- **GPU Utilization**: GPU usage percentage and VRAM consumption (for GPU-enabled blocks)\n",
    "\n",
    "##### üìä **Performance Metrics**\n",
    "- **Request Latency**: Average,  response times for inference requests\n",
    "- **Throughput**: Requests per second (RPS) and tokens per second (TPS)\n",
    "- **Queue Length**: Number of pending requests in the processing queue\n",
    "\n",
    "##### üîÑ **Operational Metrics**\n",
    "- **Request Count**: Total number of requests processed over time\n",
    "- **Error Rate**: Percentage of failed requests and error types\n",
    "\n",
    "##### üè• **Health Metrics**\n",
    "- **Uptime**: Total running time since last restart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ba47b0-3da3-4d0d-aa78-78473edd1c61",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  3105  100  3105    0     0   101k      0 --:--:-- --:--:-- --:--:--  101k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"gemma3-27b-block2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"gemma3-27b-block2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__gemma3-27b-block2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754029886.04923,\n",
      "                  \"tasks_processed_total\" : 0\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 610.118766069412,\n",
      "               \"uptime_hours\" : 0.169477435019281,\n",
      "               \"uptime_minutes\" : 10.1686461011569\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"gemma3-27b-block2\",\n",
      "               \"end_to_end_count_total\" : 0,\n",
      "               \"end_to_end_fps\" : 0,\n",
      "               \"end_to_end_latency\" : 0,\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 0.11,\n",
      "                     \"load1m\" : 0.31,\n",
      "                     \"load5m\" : 0.15,\n",
      "                     \"percent\" : 0.1\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 23498.31,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 73.68,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 58421.69,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 23466.31,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 73.53,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 58453.69,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 0.11,\n",
      "                     \"totalMem\" : 342406.93,\n",
      "                     \"usedMem\" : 366.12\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-0kcz\",\n",
      "               \"llm_active_sessions\" : 0,\n",
      "               \"llm_cpu_utilization\" : 0,\n",
      "               \"llm_gpu_utilization\" : 0,\n",
      "               \"llm_inference_duration_seconds_bucket\" : 0,\n",
      "               \"llm_inference_duration_seconds_count\" : 0,\n",
      "               \"llm_inference_duration_seconds_sum\" : 0,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_memory_usage_bytes\" : 0,\n",
      "               \"llm_prompt_tokens_total\" : 0,\n",
      "               \"llm_prompts_total\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 0,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 0,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 0,\n",
      "               \"llm_tokens_generated_total\" : 0,\n",
      "               \"llm_tokens_per_second\" : 0,\n",
      "               \"nodeId\" : \"wc-gpu-node2\",\n",
      "               \"nodeKey\" : \"in-0kcz__gemma3-27b-block2\",\n",
      "               \"on_data_count_total\" : 0,\n",
      "               \"on_data_fps\" : 0,\n",
      "               \"on_data_latency\" : 0,\n",
      "               \"on_preprocess_count_total\" : 0,\n",
      "               \"on_preprocess_fps\" : 0,\n",
      "               \"on_preprocess_latency\" : 0,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"timestamp\" : 1754030488.42884,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics before launching the app\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/gemma3-27b-block2 | json_pp #show case the metrics we are getting,hardware,online, our custom "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa4dce7",
   "metadata": {},
   "source": [
    "## 4. Performing Inference\n",
    "\n",
    "Now that the block is running, let's perform an inference task. We'll use a Python script to send a request to the block via gRPC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc6b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install the necessary libraries for our gRPC client and import them.\n",
    "# We also need to add the `inference_client` directory to our Python path to import the generated gRPC files.\n",
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0602a53",
   "metadata": {},
   "source": [
    "Now, let's define a function to send an inference request to our block. This function will connect to the gRPC server, construct a request packet, and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b573aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config, image_url=None):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "    if image_url:\n",
    "        data = {\n",
    "            \"mode\": \"chat\",\n",
    "            \"gen_params\": generation_config,\n",
    "            \"messages\": [{\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": message},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n",
    "                ]\n",
    "            }]\n",
    "        }\n",
    "    else:\n",
    "        data = {\n",
    "            \"mode\": \"chat\",\n",
    "            \"message\": message,\n",
    "            \"gen_params\": generation_config\n",
    "        }\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id=block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        data=json.dumps(data),\n",
    "        ts=time.time()\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        \n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(\"Data:\")\n",
    "            print(json.dumps(response_data, indent=2))\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "             print(f\"Data: {response.data}\")\n",
    "\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a54e5d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: 16.481139421463013s\n",
      "Session ID: session_notebook_multimodal-3\n",
      "Sequence No: 1\n",
      "Data:\n",
      "{\n",
      "  \"reply\": \"Here's an objective scene report based on the provided images:\\n\\n**Scene Description:**\\n\\nThe images depict a street scene with several individuals and vehicles. The road appears wet, possibly due to rain. \\n\\n*   **People:** There are at least four people visible. A woman in a green and red outfit is walking with a scooter. A man in a dark blue shirt is walking alongside her. Another person is visible near an auto-rickshaw (tuk-tuk). A fourth person is lying on the ground.\\n*   **Vehicles:** An auto-rickshaw is present. Several cars are visible in the background.\\n*   **Sequence:** The images appear to be a sequence, showing a progression of events.\\n\\n**Possible Snatching Activity:**\\n\\nBased on the sequence of images, it *appears* there may be a snatching or assault taking place. Here's why:\\n\\n*   **Sudden Fall:** The woman with the scooter and the man walking with her are upright in the first image. In the subsequent images, the woman is on the ground.\\n*   **Man's Position:** The man in the blue shirt is near the woman as she falls, and his posture could be interpreted as involved in the incident.\\n*   **Auto-Rickshaw:** The auto-rickshaw is positioned nearby, and it's possible it's related to the event (either as a getaway vehicle or as a witness).\\n\\n**However, it's important to note:** This is an interpretation based on a limited set of images. It's impossible to definitively confirm a snatching or assault without more information. The woman could have slipped, or there could be another explanation for her fall.\\n\\n**Further Investigation Needed:**\\n\\nTo determine what happened, further investigation would be required, such as:\\n\\n*   Video footage of the incident.\\n*   Witness statements.\\n*   Information about any reported crimes in the area.\\n\\n**Disclaimer:** I am an AI and cannot provide definitive conclusions about real-world events. This analysis is based solely on the visual information provided.\"\n",
      "}\n",
      "Timestamp: 1754031076.646736\n"
     ]
    }
   ],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 512\n",
    "}\n",
    "\n",
    "run_inference(\n",
    "    block_id=\"gemma3-27b-block2\",\n",
    "    session_id=\"session_notebook_multimodal-3\",\n",
    "    seq_no=1,\n",
    "    message=\"Analyze the following image and generate your objective scene report.Suggest is there any snatching like activity\",\n",
    "    generation_config=generation_config,\n",
    "    image_url=\"https://akm-img-a-in.tosshub.com/indiatoday/images/story/202311/chain-snatching-caught-on-camera-in-bengaluru-293151697-16x9_0.jpg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3535f442-73e2-4352-bf66-500320ae6c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968d807e-9229-4471-aad4-2b974dbd50b3",
   "metadata": {},
   "source": [
    "## 4.a Block Metrics After Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a8067e-7e3c-4eeb-9b9b-6af55f9998bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  5776  100  5776    0     0   243k      0 --:--:-- --:--:-- --:--:--  245k\n",
      "{\n",
      "   \"data\" : [\n",
      "      {\n",
      "         \"blockId\" : \"gemma3-27b-block2\",\n",
      "         \"instances\" : [\n",
      "            {\n",
      "               \"blockId\" : \"gemma3-27b-block2\",\n",
      "               \"instanceId\" : \"executor\",\n",
      "               \"latency\" : {\n",
      "                  \"latency\" : 0.000175237655639648\n",
      "               },\n",
      "               \"nodeKey\" : \"executor__gemma3-27b-block2\",\n",
      "               \"tasks_processed\" : {\n",
      "                  \"tasks_processed_created\" : 1754029886.04923,\n",
      "                  \"tasks_processed_total\" : 2\n",
      "               },\n",
      "               \"type\" : \"app\",\n",
      "               \"uptime\" : 1330.24674320221,\n",
      "               \"uptime_hours\" : 0.369512984222836,\n",
      "               \"uptime_minutes\" : 22.1707790533702\n",
      "            },\n",
      "            {\n",
      "               \"blockId\" : \"gemma3-27b-block2\",\n",
      "               \"end_to_end_count_total\" : 4,\n",
      "               \"end_to_end_fps\" : 0.0608751034141375,\n",
      "               \"end_to_end_latency\" : 16.4270768165588,\n",
      "               \"fps\" : {\n",
      "                  \"average_15m\" : 0.0702198058433361,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0.0702198058433361,\n",
      "                  \"current\" : 0.0608751034141375\n",
      "               },\n",
      "               \"hardware\" : {\n",
      "                  \"cpu\" : {\n",
      "                     \"load15m\" : 0.09,\n",
      "                     \"load1m\" : 0.1,\n",
      "                     \"load5m\" : 0.11,\n",
      "                     \"percent\" : 0.1\n",
      "                  },\n",
      "                  \"gpus\" : [\n",
      "                     {\n",
      "                        \"freeMem\" : 23346.31,\n",
      "                        \"id\" : 0,\n",
      "                        \"powerUtilization\" : 73.6,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 58573.69,\n",
      "                        \"utilization\" : 0\n",
      "                     },\n",
      "                     {\n",
      "                        \"freeMem\" : 23368.31,\n",
      "                        \"id\" : 1,\n",
      "                        \"powerUtilization\" : 73.86,\n",
      "                        \"totalMem\" : 81920,\n",
      "                        \"usedMem\" : 58551.69,\n",
      "                        \"utilization\" : 0\n",
      "                     }\n",
      "                  ],\n",
      "                  \"memory\" : {\n",
      "                     \"averageUtil\" : 0.28,\n",
      "                     \"totalMem\" : 342406.93,\n",
      "                     \"usedMem\" : 969.8\n",
      "                  }\n",
      "               },\n",
      "               \"instanceId\" : \"in-0kcz\",\n",
      "               \"latency\" : {\n",
      "                  \"average_15m\" : 14.4977474212646,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 14.4977474212646,\n",
      "                  \"current\" : 16.4270768165588\n",
      "               },\n",
      "               \"llm_active_sessions\" : 0,\n",
      "               \"llm_active_sessions_rolling\" : {\n",
      "                  \"average_15m\" : 1,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 1,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"llm_cpu_utilization\" : 13.2,\n",
      "               \"llm_cpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 6.6,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 6.6,\n",
      "                  \"current\" : 13.2\n",
      "               },\n",
      "               \"llm_gpu_utilization\" : 45,\n",
      "               \"llm_gpu_utilization_rolling\" : {\n",
      "                  \"average_15m\" : 44,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 44,\n",
      "                  \"current\" : 45\n",
      "               },\n",
      "               \"llm_inference_duration_rolling\" : {\n",
      "                  \"average_15m\" : 11.3480725,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 11.3480725,\n",
      "                  \"current\" : 14.24719\n",
      "               },\n",
      "               \"llm_inference_duration_seconds_bucket\" : 2,\n",
      "               \"llm_inference_duration_seconds_count\" : 2,\n",
      "               \"llm_inference_duration_seconds_sum\" : 22.696145,\n",
      "               \"llm_inference_errors_total\" : 0,\n",
      "               \"llm_input_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 292,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 292,\n",
      "                  \"current\" : 296\n",
      "               },\n",
      "               \"llm_memory_usage_bytes\" : 4091539456,\n",
      "               \"llm_memory_usage_rolling\" : {\n",
      "                  \"average_15m\" : 4076105728,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 4076105728,\n",
      "                  \"current\" : 4091539456\n",
      "               },\n",
      "               \"llm_output_tokens_per_minute_rolling\" : {\n",
      "                  \"average_15m\" : 343,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 343,\n",
      "                  \"current\" : 431\n",
      "               },\n",
      "               \"llm_prompt_tokens_total\" : 584,\n",
      "               \"llm_prompts_total\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_bucket\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_count\" : 2,\n",
      "               \"llm_time_per_output_token_seconds_sum\" : 0.0789236286589904,\n",
      "               \"llm_time_to_first_token_seconds_bucket\" : 2,\n",
      "               \"llm_time_to_first_token_seconds_count\" : 2,\n",
      "               \"llm_time_to_first_token_seconds_sum\" : 2.32616353034973,\n",
      "               \"llm_tokens_generated_total\" : 686,\n",
      "               \"llm_tokens_per_second\" : 26.6849466225791,\n",
      "               \"llm_tpot_rolling\" : {\n",
      "                  \"average_15m\" : 0.0394618143294952,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0.0394618143294952,\n",
      "                  \"current\" : 0.0374757778893491\n",
      "               },\n",
      "               \"llm_tps_rolling\" : {\n",
      "                  \"average_15m\" : 25.406557042485,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 25.406557042485,\n",
      "                  \"current\" : 26.6849466225791\n",
      "               },\n",
      "               \"llm_ttft_rolling\" : {\n",
      "                  \"average_15m\" : 1.16308176517487,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 1.16308176517487,\n",
      "                  \"current\" : 0.807368516921997\n",
      "               },\n",
      "               \"nodeId\" : \"wc-gpu-node2\",\n",
      "               \"nodeKey\" : \"in-0kcz__gemma3-27b-block2\",\n",
      "               \"on_data_count_total\" : 2,\n",
      "               \"on_data_fps\" : 0.0608850747588108,\n",
      "               \"on_data_latency\" : 16.4243865013123,\n",
      "               \"on_preprocess_count_total\" : 2,\n",
      "               \"on_preprocess_fps\" : 10180.3495145631,\n",
      "               \"on_preprocess_latency\" : 9.82284545898438e-05,\n",
      "               \"queue_length\" : {\n",
      "                  \"average_15m\" : 0,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 0,\n",
      "                  \"current\" : 0\n",
      "               },\n",
      "               \"tasks_processed\" : {\n",
      "                  \"average_15m\" : 1,\n",
      "                  \"average_1m\" : 0,\n",
      "                  \"average_5m\" : 1,\n",
      "                  \"current\" : 1\n",
      "               },\n",
      "               \"timestamp\" : 1754031208.90909,\n",
      "               \"type\" : \"app\"\n",
      "            }\n",
      "         ]\n",
      "      }\n",
      "   ],\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Check Block Metrics before launching the app\n",
    "!curl -X GET http://MANAGEMENTMASTER:30201/block/gemma3-27b-block2 | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd83c7-fa51-434c-9e45-ef0bc69539fe",
   "metadata": {},
   "source": [
    "## 5. Interactive Chat with Streamlit\n",
    "\n",
    "To provide an interactive way to test the model, we will launch a Streamlit application. The following cell will import the necessary function and launch the app, providing a public URL for you to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42570b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install streamlit pyngrok nest_asyncio websockets > /dev/null\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory of 'utils' to the path to find the inference_client\n",
    "sys.path.append(os.path.abspath('../..'))\n",
    "sys.path.append(os.path.abspath('../utils/streamlit_app'))\n",
    "\n",
    "\n",
    "from utils import run_streamlit_direct\n",
    "\n",
    "# Define the block_id and grpc_server_address\n",
    "BLOCK_ID = \"gemma3-27b-block2\"\n",
    "GRPC_SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "streamlit_url = run_streamlit_direct(BLOCK_ID, GRPC_SERVER_ADDRESS,port=8501)\n",
    "print(f\"Streamlit App URL: {streamlit_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a908068",
   "metadata": {},
   "source": [
    "You can now open the URL above in your browser to interact with the model. Once you are done, you can proceed to the cleanup steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856193c6",
   "metadata": {},
   "source": [
    "## 6. Cleaning Up\n",
    "\n",
    "After you are finished with the block, it is important to deallocate it to free up resources. You can also deregister the asset if you no longer need it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764da5ca",
   "metadata": {},
   "source": [
    "### Deallocate the Block\n",
    "This command will stop the running block and release all associated resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca4b4ab-f00d-4c30-8c14-fdfb2d89417d",
   "metadata": {},
   "source": [
    "`K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52cf3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   100  100    67  100    33     12      6  0:00:05  0:00:05 --:--:--    16\n",
      "{\n",
      "   \"data\" : {\n",
      "      \"data\" : \"Action performed\",\n",
      "      \"success\" : true\n",
      "   },\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#Show the Block in K8s first and then show after deletion! \n",
    "!curl -X POST http://MANAGEMENTMASTER:30600/controller/removeBlock/gcp-cluster-2 -H \"Content-Type: application/json\" -d '{\"block_id\": \"gemma3-27b-block2\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42160754",
   "metadata": {},
   "source": [
    "### Deregister the Asset\n",
    "If you no longer need the asset in the registry, you can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f62d230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   104  100    64  100    40   7246   4528 --:--:-- --:--:-- --:--:-- 13000\n",
      "{\n",
      "   \"error\" : false,\n",
      "   \"payload\" : {\n",
      "      \"acknowledged\" : true,\n",
      "      \"deletedCount\" : 1\n",
      "   }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30112/api/unregisterComponent -H \"Content-Type: application/json\" -d '{\"uri\": \"model.gemma3-27b:1.0.0-stable\"}' | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c905c913-8120-4ec3-aca2-a0667fe71f54",
   "metadata": {},
   "source": [
    "## More models sample notebooks can be found [here](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/tree/main/video_tutorial_series/02_more_models_llama_cpp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
