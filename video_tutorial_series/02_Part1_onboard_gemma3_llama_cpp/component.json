{
  "componentId": {
    "name": "gemma3-27b",
    "version": "1.0.0",
    "releaseTag": "stable"
  },
  "componentType": "model",
  "containerRegistryInfo": {
    "containerImage": "MANAGEMENTMASTER:31280/gemma3-27b:v1",
    "containerRegistryId": "MANAGEMENTMASTER:31280/gemma3-27b:v1",
    "containerImageMetadata": {
      "author": "llm-team",
      "description": "AIOS block for chat and multi-modal inference using llama-cpp-python with Gemma-3-27B and CLIP support"
    },
    "componentMode": "aios"
  },
  "componentMetadata": {
    "usecase": "chat-completion, multi-modal",
    "framework": "llama-cpp-python",
    "hardware": "gpu",
    "supports_local_models": true,
    "model_path_resolution": "automatic",
    "supports_quantization": true,
    "quantization_methods": [
      "4bit",
      "8bit",
      "fp16",
      "fp8"
    ],
    "provider": {
      "name": "AIOS-Internal",
      "modelIdentifier": "gemma3-27b-v1"
    },
    "architecture": {
      "contextLength": 8192,
      "parameterCountB": 27
    },
    "capabilities": {
      "supportsStreaming": true
    },
    "dataUsagePolicy": {
      "usedForTraining": false,
      "policyStatement": "User data is not used for training or improving this model."
    },
    "biasAndFairness": {
      "assessmentSummary": "Model has undergone internal safety evaluations. Like all LLMs, it may reflect biases present in its training data.",
      "knownBiases": [
        "potential for generating stereotypical content",
        "English-language cultural bias"
      ]
    },
    "knownLimitations": "The model's knowledge cutoff is mid-2024. It may not be aware of events after that date and can sometimes produce plausible but incorrect information.",
    "trainingDetails": {
      "trainingData": {
        "datasetName": "Mixed Web and Synthetic Data",
        "datasetSize": "Not Disclosed",
        "dataPreprocessing": "Extensive filtering for safety, quality, and removal of personal information."
      },
      "trainingProcedure": {
        "trainingFramework": "JAX",
        "hyperparameters": {
          "learningRate": "Not Disclosed",
          "batchSize": "Not Disclosed",
          "optimizer": "Not Disclosed"
        }
      }
    },
    "evaluation": {
      "benchmarks": {
        "MMLU": {
          "metric": "5-shot accuracy",
          "value": 82.3
        },
        "HellaSwag": {
          "metric": "10-shot",
          "value": 90.1
        },
        "ARC-Challenge": {
          "metric": "25-shot",
          "value": 95.2
        }
      },
      "evaluationData": {
        "datasetSummary": "Evaluated on a broad range of academic benchmarks covering reasoning, math, and coding."
      }
    }
  },
  "componentInitData": {
    "model_name": "gemma-3-27b-it-UD-Q8_K_XL/gemma-3-27b-it-UD-Q8_K_XL.gguf",
    "clip_model_name": "gemma-3-27b-it-UD-Q8_K_XL/mmproj-F16.gguf"
  },
  "componentInitParametersProtocol": {
    "temperature": {
      "type": "number",
      "description": "Sampling temperature",
      "min": 0.0,
      "max": 1.0
    }
  },
  "componentInitSettingsProtocol": {
        "cleanup_enabled": {
          "type": "boolean",
          "description": "To enable the Clean Up of sessions",
          "default": true
        },
    "cleanup_check_interval": {
      "type": "number",
      "description": "interval(in seconds) with which check happens for cleanup",
      "min": 1,
      "max": 10000
    },
    "cleanup_session_timeout": {
      "type": "number",
      "description": "Session ID will be removed beyond this timeout this threshold in seconds",
      "min": 1,
      "max": 18000
    },
    "gen_params": {
      "type": "object",
      "description": "Parameters needed for inference/chat/generation",
      "properties": {
        "max_new_tokens": {
          "type": "number",
          "description": "Maximum number of tokens to generate",
          "min": 1,
          "max": 40960
        },
        "temperature": {
          "type": "float",
          "description": "Sampling temperature",
          "min": 0.0,
          "max": 1.0
        },
        "top_k": {
          "type": "number",
          "description": "Topk ",
          "min": 1,
          "max": 1000
        },
        "top_p": {
          "type": "float",
          "description": "top_p",
          "min": 0.0,
          "max": 1.0
        },
        "repetition_penalty": {
          "type": "float",
          "description": "repetition_penalty",
          "min": 0.0,
          "max": 10.0
        },
        "do_sample": {
          "type": "boolean",
          "description": "do_sample",
          "default": true
        }
      }
    },
    "tensor_parallel": {
      "type": "boolean",
      "description": "Enable tensor parallelism",
      "default": true
    },
    "device": {
      "type": "string",
      "description": "Device to run model on",
      "default": "cuda"
    },
    "quantization_type": {
      "type": "string",
      "description": "Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization",
      "enum": [
        "4bit",
        "8bit",
        "fp16",
        "fp8"
      ],
      "default": null
    }
  },
  "componentInputProtocol": {
    "mode": {
      "type": "string",
      "description": "Operation mode: chat, generate, tokens, embed",
      "default": "chat"
    },
    "messages": {
      "type": "array",
      "description": "Input message from the user (for chat mode)"
    },
    "prompt": {
      "type": "string",
      "description": "Input prompt (for generate/tokens mode)"
    },
    "text": {
      "type": "string",
      "description": "Input text (for embed mode)"
    },
    "session_id": {
      "type": "string",
      "description": "Unique identifier for the chat session",
      "default": "default"
    },
    "system_message": {
      "type": "string",
      "description": "System message for chat initialization"
    },
    "generation_kwargs": {
      "type": "object",
      "description": "Additional generation parameters"
    }
  },
  "componentOutputProtocol": {
    "reply": {
      "type": "string",
      "description": "Chat reply from the model (chat mode)"
    },
    "generated": {
      "type": "string",
      "description": "Generated text (generate mode)"
    },
    "tokens": {
      "type": "array",
      "description": "Generated token IDs (tokens mode)"
    },
    "embedding": {
      "type": "array",
      "description": "Text embedding vector (embed mode)"
    }
  },
  "componentParameters": {
    "temperature": 0.2
  },
  "componentInitSettings": {
    "tensor_parallel": true,
    "device": "cuda",
    "quantization_type": "8bit",
    "gen_params": {
      "max_new_tokens": 2048,
      "temperature": 0.2,
      "top_k": 50,
      "top_p": 0.95,
      "repetition_penalty": 1.1,
      "do_sample": true
    }
  },
  "componentManagementCommandsTemplate": {
    "reload_model": {
      "description": "Reloads the model with optional new parameters",
      "args": {
        "gen_params": {
          "type": "object",
          "description": "Parameters needed for inference/chat/generation",
          "properties": {
            "max_new_tokens": {
              "type": "number",
              "description": "Maximum number of tokens to generate",
              "min": 1,
              "max": 40960
            },
            "temperature": {
              "type": "float",
              "description": "Sampling temperature",
              "min": 0.0,
              "max": 1.0
            },
            "top_k": {
              "type": "number",
              "description": "Topk ",
              "min": 1,
              "max": 1000
            },
            "top_p": {
              "type": "float",
              "description": "top_p",
              "min": 0.0,
              "max": 1.0
            },
            "repetition_penalty": {
              "type": "float",
              "description": "repetition_penalty",
              "min": 0.0,
              "max": 10.0
            },
            "do_sample": {
              "type": "boolean",
              "description": "do_sample",
              "default": true
            }
          }
        },
        "tensor_parallel": {
          "type": "boolean",
          "description": "Enable tensor parallelism",
          "default": true
        },
        "device": {
          "type": "string",
          "description": "Device to run model on",
          "default": "cuda"
        },
        "quantization_type": {
          "type": "string",
          "description": "Quantization type: '4bit', '8bit', 'fp16', 'fp8', or null for no quantization",
          "enum": [
            "4bit",
            "8bit",
            "fp16",
            "fp8"
          ],
          "default": null
        }
      }
    },
    "set_gen_params": {
      "description": "Updates generation configuration parameters",
      "args": {
        "gen_params": {
          "type": "object",
          "description": "New generation configuration"
        }
      }
    },
    "device_info": {
      "description": "Returns device and model diagnostics",
      "args": {}
    },
    "remove_chat_session": {
      "description": "Removes a specific chat session",
      "args": {
        "session_id": {
          "type": "string",
          "description": "Session ID to remove"
        }
      }
    },
    "update_cleanup_config": {
              "description": "Update Clean Up Config",
              "args": {
                "cleanup_config": {
                  "type": "object",
                  "description": "Send enabled: boolen, check_interval: in seconds for keep checcking, session_timeout: remove session beyond this in seconds"
                }
              }
            }
  },
  "tags": [
    "llama-cpp-python",
    "chat",
    "multi-modal",
    "gpu",
    "quantization",
    "local-models"
  ]
}