{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0641ab09",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Block Autoscaling in AIOS\n",
    "\n",
    "Welcome to this guide on the **Block Auto-Scaler** feature in AIOS. The auto-scaler automatically adjusts the number of block replicas based on real-time demand, ensuring optimal resource utilization and performance. This is a critical feature for building robust, cost-effective, and scalable AI applications.\n",
    "\n",
    "Below is a visual overview of the autoscaling process, where the number of replicas dynamically adjusts based on the incoming workload.\n",
    "\n",
    "<!--- ![Autoscaling Process](autoscaler.gif) -->\n",
    "<!-- <img src=\"autoscaler.gif\" width=\"400\" height=\"150\"> -->\n",
    "<!-- ![Custom Autoscaler](\"tokenautoscaler.png\") -->\n",
    "<img src=\"tokenautoscaler.png\" alt=\"Custom Autoscaler\" width=\"600\" height=\"400\">\n",
    "\n",
    "For a deep dive into the concepts, you can refer to the official documentation:\n",
    "- [Block Auto-Scaler Concepts](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/block/block.md#block-auto-scaler)\n",
    "\n",
    "The process involves:\n",
    "1.  **Building an Autoscaler Policy**: Defining the logic for scaling in a Python class.\n",
    "2.  **Testing the Policy**: Running unit tests to ensure the logic is correct.\n",
    "3.  **Deploying the Policy**: Uploading and registering the policy with AIOS.\n",
    "4.  **Triggering and Observing Scaling**: Simulating a workload and monitoring the system to see the autoscaler in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c60c6",
   "metadata": {},
   "source": [
    "## The AIOS Policy System\n",
    "\n",
    "Before we build our autoscaler, it's important to understand the **AIOS Policy System**. Policies are pluggable, versioned, and reusable modules that control the runtime behavior of Blocks. They allow you to customize how AIOS manages resources, routes traffic, and ensures stability without modifying the core application code.\n",
    "\n",
    "AIOS uses a chain of policies to manage a Block's lifecycle. Common policy types include:\n",
    "- **`clusterAllocator`**: Selects a cluster to run the block.\n",
    "- **`resourceAllocator`**: Assigns specific nodes and hardware.\n",
    "- **`loadBalancer`**: Distributes incoming requests among replicas.\n",
    "- **`stabilityChecker`**: Monitors the health of a block.\n",
    "- **`autoscaler`**: Automatically adjusts the number of block replicas.\n",
    "\n",
    "In this tutorial, we will focus on creating a custom `autoscaler` policy. For more information on the policy system, see the documentation:\n",
    "- [Policies System Overview](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/policies-system/policies-system.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4130d31d",
   "metadata": {},
   "source": [
    "## 1. Building a Token-Based Autoscaler Policy\n",
    "\n",
    "Our autoscaler will use a **token-based scaling** strategy. This is particularly effective for Large Language Models (LLMs), where the number of input and output tokens is a direct measure of the workload. The policy will monitor the rate of tokens processed by a block and make scaling decisions based on predefined thresholds.\n",
    "\n",
    "### The Logic of Token-Based Autoscaling\n",
    "The core idea is simple:\n",
    "- **If the rate of tokens per minute exceeds a certain \"high\" threshold**, it indicates that the current number of replicas is struggling to keep up with demand. To prevent performance degradation, the policy will trigger a **scale-up** event, adding more replicas to handle the load.\n",
    "- **If the token rate drops below a \"low\" threshold**, it means we have more resources than necessary. To save costs, the policy will trigger a **scale-down** event, removing idle replicas.\n",
    "- **A cooldown period** is used to prevent \"flapping\"—scaling up and down too frequently in response to short-lived spikes in traffic.\n",
    "\n",
    "This approach ensures that the number of running instances is always aligned with the actual workload, providing a balance between performance and cost.\n",
    "\n",
    "The policy is defined within a class named `AIOSv1PolicyRule`. We will build this class step-by-step, explaining each method. The class contains three main methods:\n",
    "- **`__init__(self, rule_id, settings, parameters)`**: Initializes the policy, setting up thresholds, cooldown periods, and other parameters from the policy registration.\n",
    "- **`eval(self, parameters, input_data, context)`**: The core evaluation logic. It retrieves metrics, calculates average token rates, and decides whether to scale up, scale down, or do nothing.\n",
    "- **`management(self, action, data)`**: Handles administrative commands, such as manual overrides or resets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0429916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "import inspect\n",
    "import textwrap\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d44e2",
   "metadata": {},
   "source": [
    "### The `AIOSv1PolicyRule` Class and `__init__` Method\n",
    "\n",
    "This is the main class for our policy. The `__init__` method initializes the policy's state. It sets up logging and retrieves parameters like scaling thresholds and cooldown periods from the policy's configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad5c8dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOSv1PolicyRule:\n",
    "    def __init__(self, rule_id: str, settings: Dict[str, Any], parameters: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Initializes the Tokens Autoscaler Policy.\n",
    "        \"\"\"\n",
    "        self.rule_id = rule_id\n",
    "        self.settings = settings\n",
    "        self.parameters = parameters\n",
    "        self.logger = logging.getLogger(f\"TokensAutoscalerPolicy-{self.rule_id}\")\n",
    "        self.logger.info(\"Initializing Tokens Autoscaler Policy\")\n",
    "        \n",
    "        # Scaling thresholds based on input and output tokens\n",
    "        self.input_up_threshold = self.parameters.get(\"input_tokens_up_threshold\", 500)\n",
    "        self.output_up_threshold = self.parameters.get(\"output_tokens_up_threshold\", 300)\n",
    "        self.input_down_threshold = self.parameters.get(\"input_tokens_down_threshold\", 100)\n",
    "        self.output_down_threshold = self.parameters.get(\"output_tokens_down_threshold\", 50)\n",
    "        \n",
    "        # Minimum number of replicas to maintain\n",
    "        self.min_replicas = self.parameters.get(\"min_replicas\", 1)\n",
    "        # The time window for averaging token metrics (e.g., 'average_1m', 'average_5m')\n",
    "        self.averaging_period = self.parameters.get(\"averaging_period\", \"average_1m\")\n",
    "        # Cooldown period in seconds to avoid flapping (scaling up and down too quickly)\n",
    "        self.cooldown_seconds = self.parameters.get(\"cooldown_seconds\", 120)\n",
    "        self.last_action_ts = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41b9a93",
   "metadata": {},
   "source": [
    "### The `_cooldown_ok` Method\n",
    "\n",
    "This is a simple helper method to prevent \"flapping\"—scaling up and down too rapidly. It checks if enough time has passed since the last scaling action before allowing another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f619342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOSv1PolicyRule(AIOSv1PolicyRule): \n",
    "    def _cooldown_ok(self, now: float) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the cooldown period has elapsed since the last action.\n",
    "        \"\"\"\n",
    "        if self.last_action_ts is None:\n",
    "            return True\n",
    "        return (now - self.last_action_ts) > self.cooldown_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b332177",
   "metadata": {},
   "source": [
    "### The `eval` Method\n",
    "\n",
    "This is the core logic of our autoscaler. It's called periodically by AIOS. The method fetches the latest token metrics, calculates the average input and output token rates across all block replicas, and compares them against the thresholds. Based on this comparison, it decides whether to scale up, scale down, or do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f0506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOSv1PolicyRule(AIOSv1PolicyRule):\n",
    "    def eval(self, parameters: Dict[str, Any], input_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluates the policy to determine if a scaling action is needed.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Evaluating policy with parameters: {self.parameters}\")\n",
    "    \n",
    "            metrics_collector = self.settings.get('get_metrics')\n",
    "            if not callable(metrics_collector):\n",
    "                self.logger.error(\"get_metrics function not found in settings\")\n",
    "                return {\"skip\": True, \"reason\": \"Metrics collector not configured.\"}\n",
    "    \n",
    "            metrics = metrics_collector()\n",
    "            self.logger.info(f\"Metrics received: {metrics}\")\n",
    "    \n",
    "            current_instances = input_data.get(\"current_instances\")\n",
    "            if not current_instances:\n",
    "                self.logger.warning(\"No current instances provided. Skipping evaluation.\")\n",
    "                return {\"skip\": True, \"reason\": \"No current instances provided.\"}\n",
    "    \n",
    "            self.logger.info(f\"Processing metrics for instances: {current_instances}\")\n",
    "            \n",
    "            block_metrics = metrics.get(\"block_metrics\", [])\n",
    "            now = time.time()\n",
    "    \n",
    "            instances_to_process = [\n",
    "                inst for inst in block_metrics\n",
    "                if inst.get('instanceId') in current_instances\n",
    "            ]\n",
    "    \n",
    "            if not instances_to_process:\n",
    "                self.logger.warning(\"No metrics found for the specified current_instances.\")\n",
    "                return {\"skip\": True, \"reason\": \"No metrics found for the specified instances.\"}\n",
    "    \n",
    "            total_input_tokens = 0\n",
    "            total_output_tokens = 0\n",
    "            \n",
    "            for instance in instances_to_process:\n",
    "                input_tokens_metrics = instance.get('llm_input_tokens_per_minute_rolling', {})\n",
    "                output_tokens_metrics = instance.get('llm_output_tokens_per_minute_rolling', {})\n",
    "                \n",
    "                total_input_tokens += input_tokens_metrics.get(self.averaging_period, 0)\n",
    "                total_output_tokens += output_tokens_metrics.get(self.averaging_period, 0)\n",
    "    \n",
    "            instance_count = len(instances_to_process)\n",
    "            avg_input_tokens = total_input_tokens / instance_count\n",
    "            avg_output_tokens = total_output_tokens / instance_count\n",
    "            \n",
    "            self.logger.info(f\"Average Input Tokens: {avg_input_tokens}, Average Output Tokens: {avg_output_tokens}\")\n",
    "    \n",
    "            if not self._cooldown_ok(now):\n",
    "                self.logger.info(f\"Cooldown active. Skipping evaluation. Last action at {self.last_action_ts}\")\n",
    "                return {\"skip\": True, \"reason\": \"Cooldown active.\"}\n",
    "    \n",
    "            # Upscale logic\n",
    "            if avg_input_tokens > self.input_up_threshold or avg_output_tokens > self.output_up_threshold:\n",
    "                self.last_action_ts = now\n",
    "                reason = f\"Token rate exceeded threshold (Input: {avg_input_tokens:.2f}, Output: {avg_output_tokens:.2f})\"\n",
    "                self.logger.info(f\"{reason}. Scaling up.\")\n",
    "                return {\n",
    "                    \"skip\": False,\n",
    "                    \"operation\": \"upscale\",\n",
    "                    \"instances_count\": 1,\n",
    "                    \"reason\": reason\n",
    "                }\n",
    "            \n",
    "            # Downscale logic\n",
    "            if avg_input_tokens < self.input_down_threshold and avg_output_tokens < self.output_down_threshold:\n",
    "                if instance_count > self.min_replicas:\n",
    "                    self.last_action_ts = now\n",
    "                    \n",
    "                    instances_to_process.sort(key=lambda x: \n",
    "                        x.get('llm_input_tokens_per_minute_rolling', {}).get(self.averaging_period, 0) +\n",
    "                        x.get('llm_output_tokens_per_minute_rolling', {}).get(self.averaging_period, 0)\n",
    "                    )\n",
    "                    \n",
    "                    instance_to_remove = instances_to_process[0].get('instanceId') if instances_to_process else None\n",
    "                    \n",
    "                    if instance_to_remove:\n",
    "                        reason = f\"Token rate below threshold (Input: {avg_input_tokens:.2f}, Output: {avg_output_tokens:.2f})\"\n",
    "                        self.logger.info(f\"{reason}. Scaling down. Removing instance {instance_to_remove}\")\n",
    "                        return {\n",
    "                            \"skip\": False,\n",
    "                            \"operation\": \"downscale\",\n",
    "                            \"instances_list\": [instance_to_remove],\n",
    "                            \"reason\": reason\n",
    "                        }\n",
    "    \n",
    "            self.logger.info(\"No scaling action required at this time.\")\n",
    "            return {\"skip\": True, \"reason\": \"No scaling action required.\"}\n",
    "    \n",
    "        except Exception as e:\n",
    "            self.logger.exception(f\"An unexpected error occurred during evaluation: {e}\")\n",
    "            return {\"skip\": True, \"reason\": f\"An error occurred: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adc3c2",
   "metadata": {},
   "source": [
    "### The `management` Method\n",
    "\n",
    "This method is for handling administrative commands. In our current policy, it's just a placeholder, but it could be extended to support actions like manually triggering a scaling event or resetting the cooldown timer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad35c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIOSv1PolicyRule(AIOSv1PolicyRule):\n",
    "    def management(self, action: str, data: dict) -> dict:\n",
    "        \"\"\"\n",
    "        Executes a custom management command.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if action == \"reset_cooldown\":\n",
    "                self.last_action_ts = None\n",
    "                self.logger.info(\"Cooldown timer reset\")\n",
    "                return {\"status\": \"ok\", \"message\": \"Cooldown timer reset successfully\"}\n",
    "        except Exception as e:\n",
    "            return {\"status\": \"error\", \"reason\": f\"Management action failed:{str(e)} \"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859deb1b",
   "metadata": {},
   "source": [
    "## 2. Testing the Policy\n",
    "\n",
    "Before deploying, it's crucial to test the policy logic. We can write a simple unit test to verify that the policy makes the correct scaling decisions under different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e8ea0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_no_action (__main__.TestTokenBasedAutoscaler.test_no_action) ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3275193/3605018975.py:93: DeprecationWarning: unittest.makeSuite() is deprecated and will be removed in Python 3.13. Please use unittest.TestLoader.loadTestsFromTestCase() instead.\n",
      "  suite.addTest(unittest.makeSuite(TestTokenBasedAutoscaler))\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Initializing Tokens Autoscaler Policy\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Evaluating policy with parameters: {'input_tokens_up_threshold': 500, 'output_tokens_up_threshold': 300, 'input_tokens_down_threshold': 100, 'output_tokens_down_threshold': 50, 'min_replicas': 1, 'averaging_period': 'average_1m', 'cooldown_seconds': 120}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Metrics received: {'block_metrics': [{'instanceId': 'instance-1', 'llm_input_tokens_per_minute_rolling': {'average_1m': 200}, 'llm_output_tokens_per_minute_rolling': {'average_1m': 100}}]}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Processing metrics for instances: ['instance-1']\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Average Input Tokens: 200.0, Average Output Tokens: 100.0\n",
      "INFO:TokensAutoscalerPolicy-test-rule:No scaling action required at this time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_scale_down (__main__.TestTokenBasedAutoscaler.test_scale_down) ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TokensAutoscalerPolicy-test-rule:Initializing Tokens Autoscaler Policy\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Evaluating policy with parameters: {'input_tokens_up_threshold': 500, 'output_tokens_up_threshold': 300, 'input_tokens_down_threshold': 100, 'output_tokens_down_threshold': 50, 'min_replicas': 1, 'averaging_period': 'average_1m', 'cooldown_seconds': 120}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Metrics received: {'block_metrics': [{'instanceId': 'instance-1', 'llm_input_tokens_per_minute_rolling': {'average_1m': 50}, 'llm_output_tokens_per_minute_rolling': {'average_1m': 20}}, {'instanceId': 'instance-2', 'llm_input_tokens_per_minute_rolling': {'average_1m': 40}, 'llm_output_tokens_per_minute_rolling': {'average_1m': 10}}]}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Processing metrics for instances: ['instance-1', 'instance-2']\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Average Input Tokens: 45.0, Average Output Tokens: 15.0\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Token rate below threshold (Input: 45.00, Output: 15.00). Scaling down. Removing instance instance-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "test_scale_up (__main__.TestTokenBasedAutoscaler.test_scale_up) ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:TokensAutoscalerPolicy-test-rule:Initializing Tokens Autoscaler Policy\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Evaluating policy with parameters: {'input_tokens_up_threshold': 500, 'output_tokens_up_threshold': 300, 'input_tokens_down_threshold': 100, 'output_tokens_down_threshold': 50, 'min_replicas': 1, 'averaging_period': 'average_1m', 'cooldown_seconds': 120}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Metrics received: {'block_metrics': [{'instanceId': 'instance-1', 'llm_input_tokens_per_minute_rolling': {'average_1m': 600}, 'llm_output_tokens_per_minute_rolling': {'average_1m': 200}}]}\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Processing metrics for instances: ['instance-1']\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Average Input Tokens: 600.0, Average Output Tokens: 200.0\n",
      "INFO:TokensAutoscalerPolicy-test-rule:Token rate exceeded threshold (Input: 600.00, Output: 200.00). Scaling up.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.011s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=3 errors=0 failures=0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "import time,sys\n",
    "from unittest.mock import MagicMock\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout  # Ensure logs go to the notebook output\n",
    ")\n",
    "\n",
    "\n",
    "class TestTokenBasedAutoscaler(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.rule_id = \"test-rule\"\n",
    "        self.settings = {}\n",
    "        self.parameters = {\n",
    "            \"input_tokens_up_threshold\": 500,\n",
    "            \"output_tokens_up_threshold\": 300,\n",
    "            \"input_tokens_down_threshold\": 100,\n",
    "            \"output_tokens_down_threshold\": 50,\n",
    "            \"min_replicas\": 1,\n",
    "            \"averaging_period\": \"average_1m\",\n",
    "            \"cooldown_seconds\": 120\n",
    "        }\n",
    "\n",
    "    def test_scale_up(self):\n",
    "        # Mock metrics collector to return high token rates\n",
    "        mock_metrics_collector = MagicMock(return_value={\n",
    "            \"block_metrics\": [\n",
    "                {\"instanceId\": \"instance-1\", \n",
    "                 \"llm_input_tokens_per_minute_rolling\": {\"average_1m\": 600},\n",
    "                 \"llm_output_tokens_per_minute_rolling\": {\"average_1m\": 200}\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        self.settings[\"get_metrics\"] = mock_metrics_collector\n",
    "        \n",
    "        policy = AIOSv1PolicyRule(self.rule_id, self.settings, self.parameters)\n",
    "        input_data = {\"current_instances\": [\"instance-1\"]}\n",
    "        \n",
    "        result = policy.eval({}, input_data, {})\n",
    "        self.assertFalse(result.get(\"skip\"))\n",
    "        self.assertEqual(result[\"operation\"], \"upscale\")\n",
    "        self.assertEqual(result[\"instances_count\"], 1)\n",
    "\n",
    "    def test_scale_down(self):\n",
    "        # Mock metrics collector for low token rates\n",
    "        mock_metrics_collector = MagicMock(return_value={\n",
    "            \"block_metrics\": [\n",
    "                {\"instanceId\": \"instance-1\", \n",
    "                 \"llm_input_tokens_per_minute_rolling\": {\"average_1m\": 50},\n",
    "                 \"llm_output_tokens_per_minute_rolling\": {\"average_1m\": 20}\n",
    "                },\n",
    "                {\"instanceId\": \"instance-2\", \n",
    "                 \"llm_input_tokens_per_minute_rolling\": {\"average_1m\": 40},\n",
    "                 \"llm_output_tokens_per_minute_rolling\": {\"average_1m\": 10}\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        self.settings[\"get_metrics\"] = mock_metrics_collector\n",
    "        \n",
    "        policy = AIOSv1PolicyRule(self.rule_id, self.settings, self.parameters)\n",
    "        policy.min_replicas = 1 # ensure we can scale down\n",
    "        input_data = {\"current_instances\": [\"instance-1\", \"instance-2\"]}\n",
    "        \n",
    "        result = policy.eval({}, input_data, {})\n",
    "        self.assertFalse(result.get(\"skip\"))\n",
    "        self.assertEqual(result[\"operation\"], \"downscale\")\n",
    "        self.assertEqual(result[\"instances_list\"], [\"instance-2\"])\n",
    "\n",
    "    def test_no_action(self):\n",
    "        # Mock metrics for moderate token rates\n",
    "        mock_metrics_collector = MagicMock(return_value={\n",
    "            \"block_metrics\": [\n",
    "                {\"instanceId\": \"instance-1\", \n",
    "                 \"llm_input_tokens_per_minute_rolling\": {\"average_1m\": 200},\n",
    "                 \"llm_output_tokens_per_minute_rolling\": {\"average_1m\": 100}\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "        self.settings[\"get_metrics\"] = mock_metrics_collector\n",
    "        \n",
    "        policy = AIOSv1PolicyRule(self.rule_id, self.settings, self.parameters)\n",
    "        input_data = {\"current_instances\": [\"instance-1\"]}\n",
    "        \n",
    "        result = policy.eval({}, input_data, {})\n",
    "        self.assertTrue(result[\"skip\"])\n",
    "        self.assertEqual(result[\"reason\"], \"No scaling action required.\")\n",
    "\n",
    "# Run the tests\n",
    "suite = unittest.TestSuite()\n",
    "suite.addTest(unittest.makeSuite(TestTokenBasedAutoscaler))\n",
    "runner = unittest.TextTestRunner(stream=sys.stdout, verbosity=2)\n",
    "runner.run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b9bcf2",
   "metadata": {},
   "source": [
    "## 3. Deploying the Policy\n",
    "\n",
    "With the policy tested, we can now deploy it to AIOS. This involves three steps: Assembling the policy,uploading the policy package (`tokenautoscaler.zip`) and then registering it with the AIOS Policy System."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff26a",
   "metadata": {},
   "source": [
    "### a. Assembling the Policy for Deployment\n",
    "\n",
    "Now that we have defined all the methods of our `AIOSv1PolicyRule` class, we need to assemble them into a single script and package it correctly for deployment. The policy must be contained within a `code` directory, which includes the `function.py` file and a `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96d9bca",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/function.py and code/requirements.txt created successfully.\n",
      "#!/usr/bin/env python3\n",
      "\"\"\"\n",
      "Input/Output Tokens Autoscaler Helper Policy\n",
      "\n",
      "Scales up/down based on rolling average input/output tokens per minute.\n",
      "\"\"\"\n",
      "import logging\n",
      "import time\n",
      "from typing import Dict, Any\n",
      "\n",
      "# Configure logger\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "\n",
      "class AIOSv1PolicyRule:\n",
      "    def __init__(self, rule_id, settings, parameters):\n",
      "        self.rule_id = rule_id\n",
      "        self.settings = settings\n",
      "        self.parameters = parameters\n",
      "        self.logger = logging.getLogger(f\"TokensAutoscalerPolicy-{self.rule_id}\")\n",
      "        self.logger.info(\"Initializing Tokens Autoscaler Policy\")\n",
      "        \n",
      "        self.input_up_threshold = self.parameters.get(\"input_tokens_up_threshold\", 500)\n",
      "        self.output_up_threshold = self.parameters.get(\"output_tokens_up_threshold\", 300)\n",
      "        self.input_down_threshold = self.parameters.get(\"input_tokens_down_threshold\", 100)\n",
      "        self.output_down_threshold = self.parameters.get(\"output_tokens_down_threshold\", 50)\n",
      "        \n",
      "        self.min_replicas = self.parameters.get(\"min_replicas\", 1)\n",
      "        self.averaging_period = self.parameters.get(\"averaging_period\", \"average_1m\")\n",
      "        self.cooldown_seconds = self.parameters.get(\"cooldown_seconds\", 120)\n",
      "        self.last_action_ts = None\n",
      "        \n",
      "    def _cooldown_ok(self, now):\n",
      "        if self.last_action_ts is None:\n",
      "            return True\n",
      "        return (now - self.last_action_ts) > self.cooldown_seconds\n",
      "\n",
      "    def eval(self, parameters: Dict[str, Any], input_data: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n",
      "        try:\n",
      "            self.logger.info(f\"Evaluating policy with parameters: {self.parameters}\")\n",
      "\n",
      "            metrics_collector = self.settings.get('get_metrics')\n",
      "            if not callable(metrics_collector):\n",
      "                self.logger.error(\"get_metrics function not found in settings\")\n",
      "                return {\"skip\": True, \"reason\": \"Metrics collector not configured.\"}\n",
      "\n",
      "            metrics = metrics_collector()\n",
      "            self.logger.info(f\"Metrics received: {metrics}\")\n",
      "\n",
      "            current_instances = input_data.get(\"current_instances\")\n",
      "            if not current_instances:\n",
      "                self.logger.warning(\"No current instances provided. Skipping evaluation.\")\n",
      "                return {\"skip\": True, \"reason\": \"No current instances provided.\"}\n",
      "\n",
      "            self.logger.info(f\"Processing metrics for instances: {current_instances}\")\n",
      "            \n",
      "            block_metrics = metrics.get(\"block_metrics\", [])\n",
      "            now = time.time()\n",
      "\n",
      "            # Filter metrics for current instances\n",
      "            instances_to_process = [\n",
      "                inst for inst in block_metrics\n",
      "                if inst.get('instanceId') in current_instances\n",
      "            ]\n",
      "\n",
      "            if not instances_to_process:\n",
      "                self.logger.warning(\"No metrics found for the specified current_instances.\")\n",
      "                return {\"skip\": True, \"reason\": \"No metrics found for the specified instances.\"}\n",
      "\n",
      "            total_input_tokens = 0\n",
      "            total_output_tokens = 0\n",
      "            \n",
      "            for instance in instances_to_process:\n",
      "                input_tokens_metrics = instance.get('llm_input_tokens_per_minute_rolling', {})\n",
      "                output_tokens_metrics = instance.get('llm_output_tokens_per_minute_rolling', {})\n",
      "                \n",
      "                total_input_tokens += input_tokens_metrics.get(self.averaging_period, 0)\n",
      "                total_output_tokens += output_tokens_metrics.get(self.averaging_period, 0)\n",
      "\n",
      "            instance_count = len(instances_to_process)\n",
      "            avg_input_tokens = total_input_tokens / instance_count\n",
      "            avg_output_tokens = total_output_tokens / instance_count\n",
      "            \n",
      "            self.logger.info(f\"Average Input Tokens: {avg_input_tokens}, Average Output Tokens: {avg_output_tokens}\")\n",
      "\n",
      "            if not self._cooldown_ok(now):\n",
      "                self.logger.info(f\"Cooldown active. Skipping evaluation. Last action at {self.last_action_ts}\")\n",
      "                return {\"skip\": True, \"reason\": \"Cooldown active.\"}\n",
      "\n",
      "            # Upscale logic\n",
      "            if avg_input_tokens > self.input_up_threshold or avg_output_tokens > self.output_up_threshold:\n",
      "                self.last_action_ts = now\n",
      "                reason = f\"Token rate exceeded threshold (Input: {avg_input_tokens:.2f}, Output: {avg_output_tokens:.2f})\"\n",
      "                self.logger.info(f\"{reason}. Scaling up.\")\n",
      "                return {\n",
      "                    \"skip\": False,\n",
      "                    \"operation\": \"upscale\",\n",
      "                    \"instances_count\": 1,\n",
      "                    \"reason\": reason\n",
      "                }\n",
      "            \n",
      "            # Downscale logic\n",
      "            if avg_input_tokens < self.input_down_threshold and avg_output_tokens < self.output_down_threshold:\n",
      "                if instance_count > self.min_replicas:\n",
      "                    self.last_action_ts = now\n",
      "                    \n",
      "                    # Find the instance with the lowest combined token rate to remove\n",
      "                    instances_to_process.sort(key=lambda x: \n",
      "                        x.get('llm_input_tokens_per_minute_rolling', {}).get(self.averaging_period, 0) +\n",
      "                        x.get('llm_output_tokens_per_minute_rolling', {}).get(self.averaging_period, 0)\n",
      "                    )\n",
      "                    \n",
      "                    instance_to_remove = instances_to_process[0].get('instanceId') if instances_to_process else None\n",
      "                    \n",
      "                    if instance_to_remove:\n",
      "                        reason = f\"Token rate below threshold (Input: {avg_input_tokens:.2f}, Output: {avg_output_tokens:.2f})\"\n",
      "                        self.logger.info(f\"{reason}. Scaling down. Removing instance {instance_to_remove}\")\n",
      "                        return {\n",
      "                            \"skip\": False,\n",
      "                            \"operation\": \"downscale\",\n",
      "                            \"instances_list\": [instance_to_remove],\n",
      "                            \"reason\": reason\n",
      "                        }\n",
      "\n",
      "            self.logger.info(\"No scaling action required at this time.\")\n",
      "            return {\"skip\": True, \"reason\": \"No scaling action required.\"}\n",
      "\n",
      "        except Exception as e:\n",
      "            self.logger.exception(f\"An unexpected error occurred during evaluation: {e}\")\n",
      "            return {\"skip\": True, \"reason\": f\"An error occurred: {e}\"}\n",
      "\n",
      "    def management(self, action: str, data: dict) -> dict:\n",
      "        self.logger.info(f\"Management action received: {action} with data: {data}\")\n",
      "        try:\n",
      "            if action == \"update_thresholds\":\n",
      "                updated = False\n",
      "                if \"input_up_threshold\" in data:\n",
      "                    self.input_up_threshold = data[\"input_up_threshold\"]\n",
      "                    updated = True\n",
      "                if \"output_up_threshold\" in data:\n",
      "                    self.output_up_threshold = data[\"output_up_threshold\"]\n",
      "                    updated = True\n",
      "                if \"input_down_threshold\" in data:\n",
      "                    self.input_down_threshold = data[\"input_down_threshold\"]\n",
      "                    updated = True\n",
      "                if \"output_down_threshold\" in data:\n",
      "                    self.output_down_threshold = data[\"output_down_threshold\"]\n",
      "                    updated = True\n",
      "                if \"min_replicas\" in data:\n",
      "                    self.min_replicas = max(1, data[\"min_replicas\"])\n",
      "                    updated = True\n",
      "                if \"cooldown_seconds\" in data:\n",
      "                    self.cooldown_seconds = max(0, data[\"cooldown_seconds\"])\n",
      "                    updated = True\n",
      "                \n",
      "                if updated:\n",
      "                    self.logger.info(f\"Thresholds updated: {data}\")\n",
      "                    return {\n",
      "                        \"status\": \"ok\",\n",
      "                        \"message\": \"Thresholds updated successfully\",\n",
      "                        \"current_config\": {\n",
      "                            \"input_up_threshold\": self.input_up_threshold,\n",
      "                            \"output_up_threshold\": self.output_up_threshold,\n",
      "                            \"input_down_threshold\": self.input_down_threshold,\n",
      "                            \"output_down_threshold\": self.output_down_threshold,\n",
      "                            \"min_replicas\": self.min_replicas,\n",
      "                            \"cooldown_seconds\": self.cooldown_seconds\n",
      "                        }\n",
      "                    }\n",
      "                else:\n",
      "                    return {\"status\": \"error\", \"reason\": \"No valid threshold parameters provided\"}\n",
      "            \n",
      "            elif action == \"reset_cooldown\":\n",
      "                self.last_action_ts = None\n",
      "                self.logger.info(\"Cooldown timer reset\")\n",
      "                return {\"status\": \"ok\", \"message\": \"Cooldown timer reset successfully\"}\n",
      "        except Exception as e:\n",
      "            self.logger.exception(f\"Error in management action '{action}': {e}\")\n",
      "            return {\"status\": \"not_implemented\", \"action\": action}\n"
     ]
    }
   ],
   "source": [
    "# Create a requirements.txt file\n",
    "with open(\"code/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"requests\") # No external dependencies for this policy\n",
    "\n",
    "print(\"code/function.py and code/requirements.txt created successfully.\")\n",
    "!cat code/function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd345d",
   "metadata": {},
   "source": [
    "Now, let's package our policy into a `tokenautoscaler.zip` file for deployment. The zip file must contain the `code` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e800cf29",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "!zip -r tokenautoscaler.zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1575dcbf",
   "metadata": {},
   "source": [
    "### b. Upload the Policy Package\n",
    "\n",
    "First, we upload the `.zip` file containing our `function.py` to a location accessible by AIOS. The following `curl` command sends the file to an upload server, which makes it available via a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b12242",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "!curl -X POST http://POLICYSTORESERVER:30186/upload -F \"file=@./tokenautoscaler.zip\" -F \"path=.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d90e3c",
   "metadata": {},
   "source": [
    "### c. Register the Policy\n",
    "\n",
    "Next, we register the policy with AIOS. This `curl` command sends a JSON payload to the policy registry endpoint. The payload contains metadata about our policy, including its name, version, and the URL where the code can be found (`\"code\": \"http://MANAGEMENTMASTER:32555/autoscaler.zip\"`). This tells AIOS how to find and execute our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0fd98f",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "!curl -X POST http://MANAGEMENTMASTER:30102/policy \\\\\n",
    "     -H \"Content-Type: application/json\" \\\\\n",
    "     -d '{ \\\\\n",
    "           \"name\": \"autoscaler\", \\\\\n",
    "           \"version\": \"2.0\", \\\\\n",
    "           \"release_tag\": \"stable\", \\\\\n",
    "           \"metadata\": {\"author\": \"admin\", \"category\": \"analytics\"}, \\\\\n",
    "           \"tags\": \"analytics,ai\", \\\\\n",
    "           \"code\": \"http://MANAGEMENTMASTER:32555/autoscaler.zip\", \\\\\n",
    "           \"code_type\": \"tar.xz\", \\\\\n",
    "           \"type\": \"policy\", \\\\\n",
    "           \"policy_input_schema\": {\"type\": \"object\", \"properties\": {\"input\": {\"type\": \"string\"}}}, \\\\\n",
    "           \"policy_output_schema\": {\"type\": \"object\", \"properties\": {\"output\": {\"type\": \"string\"}}}, \\\\\n",
    "           \"policy_settings_schema\": {}, \\\\ add some paramaters tat looks real as well,working,explain a bit as well\n",
    "           \"policy_parameters_schema\": {}, \\\\\n",
    "           \"policy_settings\": {}, \\\\\n",
    "           \"policy_parameters\": {}, \\\\\n",
    "           \"description\": \"A policy for token based autoscaling.\", \\\\\n",
    "           \"functionality_data\": {\"strategy\": \"ML-based\"}, \\\\\n",
    "           \"resource_estimates\": {} \\\\\n",
    "         }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e932a0",
   "metadata": {},
   "source": [
    "## 4. Triggering and Observing the Autoscaler in Action\n",
    "\n",
    "Now that our policy is deployed and registered, we can see it in action by generating a load on a block that is configured to use it. We will simulate both a high load to trigger a **scale-up** and a low load to trigger a **scale-down**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c45b733-f812-414f-ad06-c830b4afcefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first create the Block and show in K8s Dashboard\n",
    "!curl -X POST -d @./allocation.json -H \"Content-Type: application/json\" http://MANAGEMENTMASTER:30501/api/createBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243c4a7d",
   "metadata": {},
   "source": [
    "### a. Triggering a Scale-Up Event\n",
    "\n",
    "The following command executes a Python script (`client_test_token_autoscaler.py`) that sends a high volume of inference requests. We'll simulate 20 concurrent requests, each with a high token count, to push the average token rate above our defined `input_tokens_up_threshold`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd6a05-7849-4fbe-98a7-871d385be4ec",
   "metadata": {},
   "source": [
    "### how to show the tokens per second increase in throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f75d0c3",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test with 1 concurrent requests.\n",
      "Each request will have 100 input tokens and ask for max 50 output tokens.\n",
      "Session session-66a0c5b6-380c-4c39-ad64-faeb6c3fac3c: Latency: 1.33s\n",
      "Test finished.\n"
     ]
    }
   ],
   "source": [
    "!python3 client_test_token_autoscaler.py  --input-tokens=1500  --max-output-tokens=50 --num-requests=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b9606-fe64-4d5d-84c1-39a90864bdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 client_test_token_loadbalancer.py  --max-output-tokens=100 --num-requests=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb845643-57d1-4c85-b21f-1944d71edf33",
   "metadata": {},
   "source": [
    "### b. Triggering a Scale-Down Event \n",
    "Once the above request is completed, scale down event will be triggerd after the cool down period"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cca1b9",
   "metadata": {},
   "source": [
    "### c. Observing the Results\n",
    "\n",
    "After triggering the policy, you can observe the scaling events in the Kubernetes dashboard and Grafana.\n",
    "\n",
    "**In Kubernetes:**\n",
    "- `K8s dashboard` available at https://CLUSTER1MASTER:32319/#/login to watch for new pods being created (scale-up) or terminated (scale-down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab6c07-3737-4ab3-9cfe-f74e78e6b1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
