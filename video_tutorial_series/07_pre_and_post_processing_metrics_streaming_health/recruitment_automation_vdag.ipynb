{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a77db03",
   "metadata": {},
   "source": [
    "# Tutorial: Simple vDAG for workflow Automation with Swappable Policies\n",
    "\n",
    "In this tutorial we will explore a powerful architecture based on **swappable policies** and a **generic Large Language Model (LLM) model** to create workflows easily, and we will go on to show how we can create our custom policies as tools for creating different workflows using the AIOSv1 policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0577f301-3438-4c9b-84bf-3335e1d2c7a7",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "1. **Task**: Automate the process of screening candidate resumes against a job description using an LLM-powered workflow.\n",
    "2. **Policies**: Discover how pre-processing policies prepare data for an LLM and post-processing policies orchestrate actions based on the LLM's output.\n",
    "3. **Policies and Inference Code Overview**: Examine the Python code for the resume-parsing policy, the action-dispatching policy, and the model inference code.\n",
    "4. **vDAG Spec**: Define the entire vDAG, attaching the specific pre- and post-processing policies to a generic block, all within a single JSON specification.\n",
    "5. **Running this workflow as vDAG**: Deploy the vDAG and trigger the complete, multi-step recruitment workflow with a single API call.\n",
    "6. **Conclusion**: Master the use of policies to build modular, scalable, and intelligent automation workflows on the AIOS platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42d3b3-18d7-4fd7-a2c4-d26a50639540",
   "metadata": {},
   "source": [
    "## 1. Task\n",
    "\n",
    "Our goal is to automate the initial screening of resumes, and send them a mail communication. The pipeline should:\n",
    "1.  Accept a `.zip` file containing multiple resumes in PDF format.\n",
    "2.  Extract the text from each resume.\n",
    "3.  Use an LLM to analyze the resumes against a job description and decide on next steps (e.g., background check, send rejection email).\n",
    "4.  Execute these tool calling steps automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5e3cd-e0af-42da-84d4-b0c3f80ce9b5",
   "metadata": {},
   "source": [
    "## 2. The Architecture: Swappable Policies\n",
    "\n",
    "\n",
    "### What are Policies in AIOSv1?\n",
    "A policy is a dynamically loadable, executable Python code that is used in various places and use cases across the AIOS system. Since policies are dynamic, they allow developers to implement custom functionalities throughout the AIOS system. \n",
    "\n",
    "> 📖 **Further Reading**: [AIOSv1 Policies System Overview](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/policies-system/policies-system.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c7eb1-4f0a-43f7-9901-e49c3c6175b0",
   "metadata": {},
   "source": [
    "The pipeline is designed to be highly modular. Instead of a single, monolithic block, we use a central, generic LLM block that is customized at runtime by `preprocessingPolicyRule` and `postprocessingPolicyRule` policies in a vDAG\n",
    "\n",
    "A key concept in this architecture is that the tools are policies which are **stateless**, or \"fire and die.\" As detailed in the `policies-system.md` documentation, policies of this Type 3 are loaded for a single execution to perform a specific task and are then terminated. They do not maintain any memory or state between requests, which makes the system robust, scalable, and easy to debug.\n",
    "\n",
    "**Flow Diagram:**\n",
    "```\n",
    "[Input: zip file of resumes] -> [vDAG Block level Preprocessing Policy] -> [Generic LLM Model] -> [vDAG Block level Postprocessing Policy] -> [Automated Actions]\n",
    "```\n",
    "\n",
    "*   **`Preprocessing Policy`**: Its only job is to handle the input data. In this case, it unzips the file, finds all PDFs, and extracts their text content. It then passes this data as `supplemental_data` to the next block.\n",
    "*   **`Generic LLM Model`**: This is a standard Llama_cpp_python model code. It's designed to be unaware of the specific task. It receives a prompt and, optionally, `supplemental_data`. Its role is to generate a response based on the inputs.\n",
    "*   **`Postprocessing Policy`**: This policy takes the raw output from the LLM, parses it, and takes action. For our use case, it's responsible for parsing the JSON and executing the requested `tool_calls`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa890be0-8341-49af-a728-c9773ac5bb5e",
   "metadata": {},
   "source": [
    "Check this tutorial if you want to use the Author's given pre_processing and post_processing policy\n",
    "- [Tutorial: Building a Modular Recruitment Automation/Workflow Pipeline From Author's Policies](http://CLUSTER2NODE1:9999/notebooks/recruitment_automation_tutorial.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afdb97f-d035-4193-84d3-a2d106649282",
   "metadata": {},
   "source": [
    "## 3. Policies and Inference Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9e1327-9e3a-42d9-bb04-0c7657e3ec58",
   "metadata": {},
   "source": [
    "### Preprocessing Policy: Getting the Data Ready\n",
    "\n",
    "This policy handles the initial data extraction. It's a simple Python function that uses standard libraries to process the file.\n",
    "\n",
    "**File:** `policies/recruitment_automation/preprocessing_policy/code/function.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4ec26-2d29-44d3-9ce9-9d55619ae908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "def eval(data):\n",
    "    # Assumes 'file' is a key in the input data, containing the zip file bytes\n",
    "    zip_file_bytes = data.get('file')\n",
    "    if not zip_file_bytes:\n",
    "        return {\"error\": \"No file provided\"}\n",
    "\n",
    "    extracted_texts = []\n",
    "    with io.BytesIO(zip_file_bytes) as zip_stream:\n",
    "        with zipfile.ZipFile(zip_stream, 'r') as zip_ref:\n",
    "            for file_name in zip_ref.namelist():\n",
    "                if file_name.lower().endswith('.pdf'):\n",
    "                    with zip_ref.open(file_name) as pdf_file:\n",
    "                        pdf_bytes = pdf_file.read()\n",
    "                        with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "                            text = \"\"\n",
    "                            for page in doc:\n",
    "                                text += page.get_text()\n",
    "                            extracted_texts.append({\"file_name\": file_name, \"content\": text})\n",
    "\n",
    "    # The output of this policy becomes the 'supplemental_data' for the LLM block\n",
    "    return {\"candidates\": extracted_texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc590a0d-fcd4-405c-a740-7d789ea734b0",
   "metadata": {},
   "source": [
    "### Postprocessing Policy: Taking Action\n",
    "\n",
    "This policy is responsible for interpreting the LLM's response. A key lesson learned was that LLMs don't always produce perfect JSON. They might wrap it in markdown or add extra text. Therefore, we need a robust way to extract the JSON.\n",
    "\n",
    "**File:** `policies/recruitment_automation/postprocessing_policy/code/function.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2bcd8c-44c5-453a-8091-924c9731a9aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    # Use regex to find JSON wrapped in markdown-style code blocks\n",
    "    match = re.search(r\"```json\\n(.*?)\\n```\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        # Fallback for cases where there's no markdown wrapping\n",
    "        json_str = response_text\n",
    "    \n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle cases where the extracted string is still not valid JSON\n",
    "        return {\"error\": \"Failed to decode JSON from LLM response\", \"response\": response_text}\n",
    "        \n",
    "    def submit_and_monitor_job(self, job_name: str, policy_rule_uri: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Submits a job to an AIOS executor and monitors it until completion.\n",
    "        \"\"\"\n",
    "        # 1. Submit the job\n",
    "        \n",
    "        submit_endpoint = f\"{self.aios_url}/jobs/submit/executor-001\"\n",
    "        \n",
    "        submit_payload = {\n",
    "            \"name\": job_name,\n",
    "            \"policy_rule_uri\": policy_rule_uri,\n",
    "            \"inputs\": params,\n",
    "            \"policy_rule_parameters\": {},\n",
    "            \"node_selector\": {}\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Submitting job '{job_name}' for policy '{policy_rule_uri}' with params: {_snippet(params)}\")\n",
    "        \n",
    "        try:\n",
    "            headers = {'Content-Type': 'application/json'}\n",
    "            response = requests.post(submit_endpoint, json=submit_payload, headers=headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            job_info = response.json()\n",
    "            \n",
    "            job_id = job_info.get(\"job_id\")\n",
    "            if not job_id:\n",
    "                raise ValueError(f\"Job submission did not return a job_id. Response: {_snippet(job_info)}\")\n",
    "            logger.info(f\"Job '{job_id}' submitted for policy '{policy_rule_uri}'.\")\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if e.response and 500 <= e.response.status_code < 600:\n",
    "                logger.error(f\"Server error when submitting job. Status: {e.response.status_code}. Response: {e.response.text}\")\n",
    "            raise\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Failed to submit job for policy '{policy_rule_uri}': {e}\")\n",
    "            raise\n",
    "\n",
    "        # 2. Poll for job completion\n",
    "        status_endpoint = f\"{self.aios_url}/jobs/{job_id}\"\n",
    "        start_time = time.monotonic()\n",
    "        \n",
    "        while True:\n",
    "            if time.monotonic() - start_time > self.job_timeout:\n",
    "                raise TimeoutError(f\"Job '{job_id}' timed out after {self.job_timeout} seconds.\")\n",
    "\n",
    "            try:\n",
    "                response = requests.get(status_endpoint, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    resp_json = response.json()\n",
    "                    \n",
    "                    # Check for success at the top level\n",
    "                    if resp_json.get(\"success\"):\n",
    "                        data = resp_json.get(\"data\", {})\n",
    "                        status = data.get(\"job_status\")\n",
    "                        \n",
    "                        if status == \"completed\":\n",
    "                            logger.info(f\"Job '{job_id}' completed successfully. Retrieving result from final poll.\")\n",
    "                            result_data = data.get(\"job_output_data\")\n",
    "                            if result_data is None:\n",
    "                                logger.warning(f\"Job result for '{job_id}' did not contain 'job_output_data'. Full response: {_snippet(resp_json)}\")\n",
    "                                return resp_json # Fallback to the full body\n",
    "                            \n",
    "                            logger.info(f\"Result for job '{job_id}': {_snippet(result_data)}\")\n",
    "                            return result_data\n",
    "                        elif status == \"failed\":\n",
    "                            # The detailed result might be in 'job_output_data'\n",
    "                            details = data.get(\"job_output_data\", \"No details provided.\")\n",
    "                            raise RuntimeError(f\"Job '{job_id}' failed. Details: {details}\")\n",
    "                        elif status in [\"queued\", \"running\"]:\n",
    "                            logger.debug(f\"Job '{job_id}' is '{status}'. Polling again.\")\n",
    "                            time.sleep(self.job_poll_interval)\n",
    "                        else:\n",
    "                            logger.warning(f\"Job '{job_id}' has unknown status: '{status}'. Retrying...\")\n",
    "                            time.sleep(self.job_poll_interval)\n",
    "                    else:\n",
    "                        # Handle cases where 'success' is false or missing\n",
    "                        error_message = resp_json.get(\"message\", \"Unknown error during polling.\")\n",
    "                        logger.warning(f\"Polling for job '{job_id}' was not successful: {error_message}. Retrying...\")\n",
    "                        time.sleep(self.job_poll_interval)\n",
    "                else:\n",
    "                    logger.warning(f\"Polling for job '{job_id}' returned status {response.status_code}. Retrying...\")\n",
    "                    time.sleep(self.job_poll_interval)\n",
    "                            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logger.warning(f\"Error polling job status for '{job_id}': {e}. Retrying...\")\n",
    "                time.sleep(self.job_poll_interval)\n",
    "                \n",
    "def eval(data):\n",
    "    # 'data' is the raw output from the LLM block\n",
    "    llm_response_text = data.get('response', '')\n",
    "    \n",
    "    # Attempt to parse the response directly\n",
    "    try:\n",
    "        parsed_json = json.loads(llm_response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If direct parsing fails, use our robust extraction function\n",
    "        parsed_json = extract_json_from_response(llm_response_text)\n",
    "\n",
    "    if 'error' in parsed_json:\n",
    "        return parsed_json\n",
    "\n",
    "    # The core logic: check for the 'tool_calls' key\n",
    "    if 'tool_calls' not in parsed_json:\n",
    "        return {\"error\": \"'tool_calls' key not found in the LLM response.\", \"response_data\": parsed_json}\n",
    "\n",
    "    # In a real system, you would execute the tool calls here caliing submit_and_monitor_job\n",
    "    return {\"executed_tool_calls\": parsed_json['tool_calls']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f74a47-5377-4be3-bebc-9cfa04fee5d5",
   "metadata": {},
   "source": [
    "### Generic Tool Policies: The \"Fire and Die\" Policies\n",
    "\n",
    "It's important to note that the `postprocessing_policy` itself doesn't contain the logic for every possible action. Instead, it acts as a dispatcher. When the LLM requests a `background-check` or a `send-email`, the postprocessing policy calls *other* generic, stateless policies to do the actual work.\n",
    "\n",
    "These tool policies, such as `background_check_policy` and `send_email_policy`, re loaded, executed with the inputs provided by the `postprocessing_policy`, and then terminated. This keeps the entire system modular, as new tools can be added without changing the core pipeline, and each tool is a self-contained, stateless unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac99669-7dce-413e-ae39-111e6733d665",
   "metadata": {},
   "source": [
    "## Making the LLM Context-Aware\n",
    "\n",
    "We modify the `on_data` method in the LLM block's code to explicitly format the supplemental data and prepend it to the user's prompt. This ensures the LLM has the necessary context to make an informed decision.\n",
    "\n",
    "**File:** Refer `main.py` from [02_Part2_onboard_custom_llama_cpp\n",
    "](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/blob/main/video_tutorial_series/02_Part2_onboard_custom_llama_cpp/02-Model-Integration-Setup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a448d21-e495-453c-b03c-cdddcbb74b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simplified representation of the key logic in main_more_context.py\n",
    "\n",
    "class LlamaCppBlock:\n",
    "    # ... (other class methods)\n",
    "\n",
    "    def on_data(self, input_data):\n",
    "        message = input_data.get(\"message\", \"\")\n",
    "        supplemental_context = \"\"\n",
    "\n",
    "        # Check for our specific supplemental data from the preprocessing policy\n",
    "        if \"supplemental_data\" in input_data and \"candidates\" in input_data[\"supplemental_data\"]:\n",
    "            candidates = input_data[\"supplemental_data\"][\"candidates\"]\n",
    "            \n",
    "            # **THE CRITICAL FIX**: Format the resume data into a clear context string\n",
    "            context_parts = [\"\\n\\n--- START OF SUPPLEMENTAL DATA ---\"]\n",
    "            for i, candidate in enumerate(candidates):\n",
    "                context_parts.append(f\"\\n--- Resume {i+1}: {candidate.get('file_name', 'Unknown')} ---\")\n",
    "                context_parts.append(candidate.get('content', 'No content'))\n",
    "            context_parts.append(\"\\n--- END OF SUPPLEMENTAL DATA ---\")\n",
    "            supplemental_context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            # Clean up the input data so it's not processed further\n",
    "            del input_data[\"supplemental_data\"]\n",
    "\n",
    "        # Prepend the context to the user's original message\n",
    "        final_message = f\"{supplemental_context}\\n\\nUSER REQUEST:\\n{message}\"\n",
    "\n",
    "        # ... (rest of the logic to send 'final_message' to the LLM)\n",
    "        # self.llm.create_chat_completion(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef40701c-f2f1-42f4-8eb1-dac35b0b36bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"head\": {\n",
      "        \"templateUri\": \"Parser/V1\",\n",
      "        \"parameters\": {}\n",
      "    },\n",
      "    \"body\": {\n",
      "        \"spec\": {\n",
      "            \"values\": {\n",
      "                \"mode\": \"allocate\",\n",
      "                \"blockId\": \"llama4-scout-17b-block\",\n",
      "                \"blockComponentURI\": \"model.llama4-scout-17b:1.0.0-stable\",\n",
      "                \"minInstances\": 1,\n",
      "                \"maxInstances\": 3,\n",
      "                \"blockInitData\": {\n",
      "                    \"model_name\": \"Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00001-of-00003.gguf\",\n",
      "                    \"system_message\": \"You are an expert recruitment assistant. Your task is to analyze the provided resume context and create a JSON-based execution plan for a multi-stage recruitment pipeline. Your output MUST be a valid JSON object with a single key, \\\"tool_calls\\\". This key must contain a list of jobs to be executed in sequence.\\n\\nIMPORTANT: Group ALL candidates into a SINGLE background check job. Do not create separate background check jobs for each candidate.\\n\\nEach job in the list is a JSON object and MUST have the following keys:\\n- \\\"name\\\": A descriptive name for the job (e.g., \\\"background-check-batch-1\\\").\\n- \\\"policy_rule_uri\\\": The specific URI for the policy to be executed.\\n- \\\"inputs\\\": A JSON object containing the parameters for that policy.\\n\\nHere are the available policies and their details:\\n\\n1.  **Background Check Policy**\\n    -   `policy_rule_uri`: `generic_background_check:1.0.0-stable`\\n    -   `inputs`: A JSON object with a \\\"candidates\\\" key, which is an ARRAY containing ALL candidate objects with \\\"id\\\", \\\"name\\\", and \\\"email\\\".\\n\\n2.  **Send Email Policy**\\n    -   `policy_rule_uri`: `generic_send_email:1.0.0-stable`\\n    -   `inputs`: A JSON object with \\\"subject\\\" and \\\"body\\\" keys. The \\\"to\\\" address will be handled automatically by the pipeline for candidates who pass the background check.\\n\\n**Correct Output Format (Note: ALL candidates in ONE background check):**\\n\\n{\\n  \\\"tool_calls\\\": [\\n    {\\n      \\\"name\\\": \\\"background-check-batch\\\",\\n      \\\"policy_rule_uri\\\": \\\"generic_background_check:1.0.0-stable\\\",\\n      \\\"inputs\\\": {\\n        \\\"candidates\\\": [\\n          {\\n            \\\"id\\\": \\\"1\\\",\\n            \\\"name\\\": \\\"John Doe\\\",\\n            \\\"email\\\": \\\"john.doe@example.com\\\"\\n          },\\n          {\\n            \\\"id\\\": \\\"2\\\",\\n            \\\"name\\\": \\\"Jane Smith\\\",\\n            \\\"email\\\": \\\"jane.smith@example.com\\\"\\n          }\\n        ]\\n      }\\n    },\\n    {\\n      \\\"name\\\": \\\"send-follow-up-email\\\",\\n      \\\"policy_rule_uri\\\": \\\"generic_send_email:1.0.0-stable\\\",\\n      \\\"inputs\\\": {\\n        \\\"subject\\\": \\\"Next Steps in Your Application\\\",\\n        \\\"body\\\": \\\"Thank you for your application. We will be in touch with the next steps shortly.\\\"\\n      }\\n    }\\n  ]\\n}\\n\\nAlways combine ALL candidates into a SINGLE background check job. Do not create individual background check jobs per candidate.\"\n",
      "\n",
      "                },\n",
      "                \"initSettings\": {\n",
      "                    \"tensor_parallel\": true,\n",
      "                    \"device\": \"cuda\",\n",
      "                    \"quantization_type\": \"int8\",\n",
      "                    \"cleanup_enabled\":  true,\n",
      "                    \"cleanup_check_interval\": 60,\n",
      "                    \"cleanup_session_timeout\": 1800,\n",
      "                    \"generation_config\": {\n",
      "                        \"max_new_tokens\": 2048,\n",
      "                        \"temperature\": 0.6,\n",
      "                        \"top_k\": 50,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"repetition_penalty\": 1.1,\n",
      "                        \"do_sample\": true\n",
      "                    }\n",
      "                },\n",
      "                \"policyRulesSpec\": [\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"clusterAllocator\",\n",
      "                            \"policyRuleURI\": \"cluster-selector:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"filter\": {\n",
      "                                    \"clusterQuery\": {\n",
      "                                        \"variable\": \"id\",\n",
      "                                        \"operator\": \"==\",\n",
      "                                        \"value\": \"gcp-cluster-2\"\n",
      "                                    }\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"max_candidates\": 2\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"resourceAllocator\",\n",
      "                            \"policyRuleURI\": \"allocator:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"allocation_data\": {\n",
      "                                    \"node_id\": \"wc-gpu-node2\",\n",
      "                                    \"gpus\": [0,1]\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"selection_mode\": \"balanced\"\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"loadBalancer\",\n",
      "                            \"policyRuleURI\": \"load_balancer:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"cache_sessions\": true\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"session_cache_size\": 2000\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"stabilityChecker\",\n",
      "                            \"policyRuleURI\": \"health_checker:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"unhealthy_threshold\": 2\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"check_interval_sec\": 10\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"autoscaler\",\n",
      "                            \"policyRuleURI\": \"autoscaler:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"target_gpu_utilization\": 0.8\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"scale_up_cooldown\": 45,\n",
      "                                \"scale_down_cooldown\": 90\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat allocation-llama4scout_recruiter_vdag.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64601a",
   "metadata": {},
   "source": [
    "# 4. vDAG Creation Spec\n",
    "\n",
    "The entire vDAG is defined by a single allocation JSON file. This file acts as the blueprint, telling the system which model to load, what initial instructions to give it (the system prompt), and which policies to attach for pre-processing and post-processing. This approach makes the pipeline modular, as you can change the model or policies simply by modifying this file.\n",
    "\n",
    "Below is a simplified version of the allocation spec, highlighting the key components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b06232f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recruitment_vdag_spec = {\n",
    "    \"parser_version\": \"Parser/V1\",\n",
    "    \"body\": {\n",
    "        \"spec\": {\n",
    "            \"values\": {\n",
    "                \"vdagName\": \"recruitment-automation-vdag\",\n",
    "                \"vdagVersion\": {\"version\": \"1.0.0\", \"release-tag\": \"stable\"},\n",
    "                \"discoveryTags\": [\"recruitment\", \"single-node-vdag\"],\n",
    "                \"controller\": {},\n",
    "                \"nodes\": [\n",
    "                    {\n",
    "                        \"spec\": {\n",
    "                            \"values\": {\n",
    "                                \"nodeLabel\": \"recruitment-node\",\n",
    "                                \"nodeType\": \"block\",\n",
    "                                \"manualBlockId\": \"llama4-scout-17b-block\",\n",
    "                                \"preprocessingPolicyRule\": {\"policyRuleURI\": \"recruitment_preprocessing:1.0.0-stable\"},\n",
    "                                \"postprocessingPolicyRule\": {\"policyRuleURI\": \"recruitment_postprocessing:1.0.0-stable\"},\n",
    "                                \"modelParameters\": {}\n",
    "                            },\n",
    "                            \"IOMap\": [\n",
    "                                {\n",
    "                                    \"inputs\": [{\"name\": \"input_0\", \"reference\": \"input_0\"}],\n",
    "                                    \"outputs\": [{\"name\": \"output_0\", \"reference\": \"output_0\"}]\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"graph\": {\n",
    "                    \"input\": [{\n",
    "                        \"nodeLabel\": \"recruitment-node\",\n",
    "                        \"inputNames\": [\"input_0\"]\n",
    "                    }],\n",
    "                    \"connections\": [],\n",
    "                    \"output\": [{\n",
    "                        \"nodeLabel\": \"recruitment-node\",\n",
    "                        \"outputNames\": [\"output_0\"]\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33dc312-6100-442f-8da8-9a1ae0188ada",
   "metadata": {},
   "source": [
    "# 5. Running this workflow as vDAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa427d",
   "metadata": {},
   "source": [
    "## 1. Inference Flow\n",
    "\n",
    "When an inference request is made, the vDAG Controller orchestrates the following steps:\n",
    "\n",
    "1.  **Request Reception**: The vDAG Controller receives a request containing a job description and a zip file of resumes.\n",
    "2.  **Block Selection**: The controller identifies that the request is for the `llama4-scout-17b-block`.\n",
    "3.  **vDAG Block Pre-processing**: The attached `recruitment_preprocessing` policy is executed. It opens the zip file, extracts the text from all PDF resumes, and formats it as supplemental data.\n",
    "4.  **Author's Block Pre_processing**: In case of any block level preprocessing present added by the author, that policy is executed.Here it is empty.\n",
    "5.  **LLM Invocation**: The extracted resume text is passed along with the original job description to the `llama4-scout` model. The model analyzes the information based on its system prompt.\n",
    "6.  **Author's Block Post_processing**: In case of any block level postprocessing present added by the author, that policy is executed.Here it is empty.\n",
    "7.  **vDAG Block Post-processing**: The model's raw output (a JSON string) is sent to the `recruitment_postprocessing` policy. This policy parses the JSON, validates it, and extracts the `tool_calls`.\n",
    "8.  **Response**: The final, structured output is returned to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d8ef5-cb3e-46c0-a04f-9a18d9722c9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Pre-processing Policy: `recruitment_preprocessing:1.0.0-stable`\n",
    "This policy is responsible for handling the input files.\n",
    "\n",
    "### Post-processing Policy: `recruitment_postprocessing:1.0.0-stable`\n",
    "This policy parses the LLM's output and perform tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac6d45d-2525-4a07-a616-939cacb04c12",
   "metadata": {},
   "source": [
    "### a. Create the vDAG with the AIOS createvDAG Endpoint\n",
    "This command registers the vDAG definition with the AIOS system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f467fb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vDAG Creation Response Status: 200\n",
      "Response Body: {'result': {'task_id': '80e95f71-a6e2-4837-aabc-15a97a528f77', 'vdagURI': 'recruitment-automation-vdag:1.0.0-stable'}, 'success': True, 'task_id': ''}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "# NOTE: Adjust the IP address to match your cluster's endpoint.\n",
    "create_vdag_url = \"http://MANAGEMENTMASTER:30501/api/createvDAG\" \n",
    "response = requests.post(create_vdag_url, json=recruitment_vdag_spec)\n",
    "print(f\"vDAG Creation Response Status: {response.status_code}\")\n",
    "print('Response Body:', response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725735af",
   "metadata": {},
   "source": [
    "### b. Verify the vDAG is Registered\n",
    "Use this command to confirm that the vDAG exists in the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9703257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  1208  100  1208    0     0   217k      0 --:--:-- --:--:-- --:--:--  235k\n",
      "{\n",
      "   \"data\" : {\n",
      "      \"assignment_info\" : {\n",
      "         \"recruitment-node\" : \"llama4-scout-17b-block\"\n",
      "      },\n",
      "      \"compiled_graph_data\" : {\n",
      "         \"head\" : \"llama4-scout-17b-block\",\n",
      "         \"rev_mapping\" : {\n",
      "            \"llama4-scout-17b-block\" : \"recruitment-node\"\n",
      "         },\n",
      "         \"t2_graph\" : {\n",
      "            \"llama4-scout-17b-block\" : []\n",
      "         },\n",
      "         \"t3_graph\" : {\n",
      "            \"llama4-scout-17b-block\" : {\n",
      "               \"outputs\" : []\n",
      "            }\n",
      "         },\n",
      "         \"tail\" : [\n",
      "            \"llama4-scout-17b-block\"\n",
      "         ]\n",
      "      },\n",
      "      \"controller\" : {\n",
      "         \"initParameters\" : {},\n",
      "         \"initSettings\" : {},\n",
      "         \"inputSources\" : [],\n",
      "         \"policies\" : []\n",
      "      },\n",
      "      \"discoveryTags\" : [\n",
      "         \"recruitment\",\n",
      "         \"single-node-vdag\"\n",
      "      ],\n",
      "      \"graph\" : {\n",
      "         \"connections\" : [],\n",
      "         \"input\" : [\n",
      "            {\n",
      "               \"inputNames\" : [\n",
      "                  \"input_0\"\n",
      "               ],\n",
      "               \"nodeLabel\" : \"recruitment-node\"\n",
      "            }\n",
      "         ],\n",
      "         \"output\" : [\n",
      "            {\n",
      "               \"nodeLabel\" : \"recruitment-node\",\n",
      "               \"outputNames\" : [\n",
      "                  \"output_0\"\n",
      "               ]\n",
      "            }\n",
      "         ]\n",
      "      },\n",
      "      \"metadata\" : {},\n",
      "      \"nodes\" : [\n",
      "         {\n",
      "            \"IOMap\" : [],\n",
      "            \"assignmentPolicyRule\" : {},\n",
      "            \"inputProtocol\" : {},\n",
      "            \"manualBlockId\" : \"llama4-scout-17b-block\",\n",
      "            \"modelParameters\" : {},\n",
      "            \"nodeLabel\" : \"recruitment-node\",\n",
      "            \"nodeType\" : \"block\",\n",
      "            \"outputProtocol\" : {},\n",
      "            \"postprocessingPolicyRule\" : {\n",
      "               \"policyRuleURI\" : \"recruitment_postprocessing:1.0.0-stable\"\n",
      "            },\n",
      "            \"preprocessingPolicyRule\" : {\n",
      "               \"policyRuleURI\" : \"recruitment_preprocessing:1.0.0-stable\"\n",
      "            },\n",
      "            \"vdagURI\" : \"\"\n",
      "         }\n",
      "      ],\n",
      "      \"status\" : \"assigned\",\n",
      "      \"vdagURI\" : \"recruitment-automation-vdag:1.0.0-stable\",\n",
      "      \"vdag_name\" : \"recruitment-automation-vdag\",\n",
      "      \"vdag_version\" : {\n",
      "         \"release-tag\" : \"stable\",\n",
      "         \"version\" : \"1.0.0\"\n",
      "      }\n",
      "   },\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Adjust the IP address and vDAG name if you changed it.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30103/vdag/recruitment-automation-vdag:1.0.0-stable | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b1206",
   "metadata": {},
   "source": [
    "### c. Create a vDAG Controller\n",
    "This deploys the vDAG, creating a running instance with an API endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb3e290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   344  100    58  100   286    567   2799 --:--:-- --:--:-- --:--:--  3372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\":\"Controller created successfully\",\"success\":true}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# NOTE: Adjust the IP address, cluster name, and vdag_controller_id if needed.\n",
    "curl -X POST http://MANAGEMENTMASTER:30600/vdag-controller/gcp-cluster-2 \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"action\": \"create_controller\",\n",
    "    \"payload\": {\n",
    "      \"vdag_controller_id\": \"recruitment-automation-vdag-controller\", \n",
    "      \"vdag_uri\": \"recruitment-automation-vdag:1.0.0-stable\",\n",
    "      \"config\": {\n",
    "        \"policy_execution_mode\": \"local\",\n",
    "        \"replicas\": 1\n",
    "      }\n",
    "    }\n",
    "  }'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d64964",
   "metadata": {},
   "source": [
    "### d. Get the Controller Details\n",
    "Check the status of your deployed vDAG controller. The `service_urls` will give you the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c065aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   440  100   440    0     0  88852      0 --:--:-- --:--:-- --:--:--  107k\n",
      "{\n",
      "   \"data\" : {\n",
      "      \"cluster_id\" : \"gcp-cluster-2\",\n",
      "      \"config\" : {\n",
      "         \"api_url\" : \"http://CLUSTER1MASTER:31871\",\n",
      "         \"policy_execution_mode\" : \"local\",\n",
      "         \"replicas\" : 1,\n",
      "         \"rest_url\" : \"http://CLUSTER1MASTER:31533\",\n",
      "         \"rpc_url\" : \"CLUSTER1MASTER:32511\"\n",
      "      },\n",
      "      \"metadata\" : {},\n",
      "      \"public_url\" : \"CLUSTER1MASTER:32511\",\n",
      "      \"search_tags\" : [\n",
      "         \"recruitment\",\n",
      "         \"single-node-vdag\"\n",
      "      ],\n",
      "      \"vdag_controller_id\" : \"recruitment-automation-vdag-controller\",\n",
      "      \"vdag_uri\" : \"recruitment-automation-vdag:1.0.0-stable\"\n",
      "   },\n",
      "   \"success\" : true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Adjust the IP address and controller ID if needed.\n",
    "!curl -X GET http://MANAGEMENTMASTER:30103/vdag-controller/recruitment-automation-vdag-controller | json_pp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d898a",
   "metadata": {},
   "source": [
    "## 6. Inference\n",
    "\n",
    "To run the pipeline, we send an inference request to the vDAG controller's API endpoint. The request must contain the `block_id`, the `message` (job description), and the zipped resumes encoded in Base64.\n",
    "\n",
    "The following Python code automates this process: it reads the zip file, encodes it, constructs the JSON payload, and sends the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d99570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting recruitment pipeline at 2025-09-02 08:33:17\n",
      "======================================================================\n",
      "✅ Successfully read resume_compressed.zip (250601 bytes)\n",
      "Inference request successful. The raw JSON output is stored in 'output'.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "zip_file_path =  'resume_compressed.zip'\n",
    "# NOTE: This URL should match the vDAG controller's service URL from the step above.\n",
    "INFERENCE_URL = \"http://CLUSTER1MASTER:31871/v1/infer\" \n",
    "message = \"\"\"We're hiring for a Senior Computer Vision Engineer. Requirements include:\n",
    "    - 6+ years of hands-on experience in computer vision and deep learning.\n",
    "    - Production-level experience with frameworks like PyTorch or TensorFlow, and libraries like OpenCV.\n",
    "    - Master's degree or Ph.D. in Computer Science or a related field.\n",
    "\n",
    "Please analyze the provided resumes and create a recruitment plan that:\n",
    "1. Initiates background checks for qualified candidates\n",
    "2. Sends appropriate follow-up communications\"\"\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "print(f\"🚀 Starting recruitment pipeline at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    with open(zip_file_path, \"rb\") as f:\n",
    "        zip_binary_data = f.read()\n",
    "    base64_encoded_data = base64.b64encode(zip_binary_data).decode('utf-8')\n",
    "    file_size = len(zip_binary_data)\n",
    "    print(f\"✅ Successfully read {zip_file_path} ({file_size} bytes)\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: The file was not found at '{zip_file_path}'.\")\n",
    "    raise\n",
    "\n",
    "# The payload now targets the vDAG controller, not a specific block_id.\n",
    "payload = {\n",
    "    \"model\": \"llama4-scout-17b-block\",\n",
    "    \"session_id\": \"test_135\",\n",
    "    \"seq_no\": 1,\n",
    "    \"data\": {\n",
    "        \"mode\": \"chat\",\n",
    "        \"message\": message   },\n",
    "    \"files\": [\n",
    "      {\n",
    "        \"metadata\": {\"filename\": zip_file_path, \"size\": file_size},\n",
    "        \"file_data\": base64_encoded_data #zip_binary_data #\n",
    "      }],\n",
    "    \"graph\": {},\n",
    "    \"selection_query\": {}\n",
    "}\n",
    "\n",
    "\n",
    "output = None\n",
    "try:\n",
    "    response = requests.post(INFERENCE_URL, json=payload, timeout=600) # Added a long timeout for long debates\n",
    "    response.raise_for_status()\n",
    "    output = response.json()\n",
    "    print(\"Inference request successful. The raw JSON output is stored in 'output'.\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Failed to get response from inference service: {e}\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Failed to parse JSON from response. Raw text:\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae219bbd-b263-4043-af76-ca9eb7143fd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'background_check_processed': True, 'message': 'Recruitment pipeline processing finished.', 'processed_tool_calls': 2}, 'seq_no': 1, 'session_id': 'test_134', 'ts': 1756735787.09144}\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "# print(payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
