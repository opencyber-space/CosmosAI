import time
import logging
from typing import Optional, Dict, Any
import os

try:
    import psutil
except ImportError:
    psutil = None
    logging.warning("psutil not found. CPU and memory metrics will not be available.")

try:
    import pynvml
except ImportError:
    pynvml = None
    logging.warning("pynvml not found. GPU metrics will not be available.")




# Setup logging for metrics
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class LLMMetricsUpdated:
    """
    Enhanced LLM metrics system with rolling averages and native timing integration
    for autoscaling and load balancing decisions.
    
    Integrates with AIOSMetrics infrastructure to provide:
    - Rolling averages for resource utilization, performance, and latency metrics
    - Native llama_cpp timing integration for accurate performance measurement
    - LLM-specific metrics for single-inference (batch_size=1) systems
    """
    
    def __init__(self, metrics, block_id=None):
        self.metrics = metrics
        self.block_id = block_id
        self._model_reference = None  # Reference to llama_cpp model for native timings
        self._use_native_timings = False  # Flag to use native timings when available
        self._register_llm_metrics()
        import pynvml,psutil

        # Initialize psutil for process-specific monitoring
        self.process = None
        if psutil:
            try:
                self.process = psutil.Process(os.getpid())
            except Exception as e:
                logger.error(f"Failed to initialize psutil.Process: {e}")
                self.process = None
        print(pynvml)
        # Initialize pynvml for GPU monitoring
        if pynvml:
            try:
                pynvml.nvmlInit()
            except pynvml.NVMLError as e:
                logger.error(f"Failed to initialize pynvml: {e}")
                pynvml = None # Disable pynvml if init fails

    def __del__(self):
        """Ensure pynvml is shutdown cleanly."""
        if pynvml:
            try:
                pynvml.nvmlShutdown()
            except Exception as e:
                logger.error(f"Error shutting down pynvml: {e}")

    def set_model_reference(self, model, use_native_timings=True):
        """
        Set reference to llama_cpp model for accessing native performance timings.
        
        Args:
            model: The llama_cpp model instance
            use_native_timings: Whether to use native timings when available
        """
        self._model_reference = model
        self._use_native_timings = use_native_timings
        logger.info(f"ðŸ“Š Model reference set for native timings (enabled: {use_native_timings})")
        
    def _get_native_timings(self) -> Optional[Dict[str, Any]]:
        """
        Extract native performance timings from llama_cpp context.
        
        Returns:
            Dict with timing data or None if not available
        """
        if not (self._model_reference and self._use_native_timings):
            return None
            
        try:
            if hasattr(self._model_reference, '_ctx') and hasattr(self._model_reference._ctx, 'get_timings'):
                timings = self._model_reference._ctx.get_timings()
                logger.info(f"ðŸ”§ Native timings retrieved: {timings}")
                return timings
        except Exception as e:
            logger.warning(f"âš ï¸ Could not retrieve native timings: {e}")
            
        return None

    def _register_llm_metrics(self):
        """Register all LLM Prometheus metrics (same as original)"""
        self.metrics.register_counter("llm_prompts_total", "Total number of prompts processed")
        self.metrics.register_counter("llm_tokens_generated_total", "Total number of tokens generated by the LLM")
        self.metrics.register_counter("llm_prompt_tokens_total", "Total number of prompt tokens received")
        self.metrics.register_gauge("llm_active_sessions", "Current number of active LLM chat sessions")
        self.metrics.register_histogram("llm_inference_duration_seconds", "Duration of LLM inference in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10])
        self.metrics.register_histogram("llm_time_to_first_token_seconds", "Time to First Token (TTFT) in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5, 1, 2])
        self.metrics.register_histogram("llm_time_per_output_token_seconds", "Time Per Output Token (TPOT) in seconds",
                                        buckets=[0.01, 0.05, 0.1, 0.2, 0.5])
        self.metrics.register_gauge("llm_tokens_per_second", "Number of tokens generated per second")
        self.metrics.register_gauge("llm_cpu_utilization", "CPU utilization percentage during inference")
        self.metrics.register_gauge("llm_gpu_utilization", "GPU utilization percentage during inference")
        self.metrics.register_gauge("llm_memory_usage_bytes", "Memory usage in bytes during inference")
        self.metrics.register_counter("llm_inference_errors_total", "Total number of inference errors")

    # ==================== LOGGING METHODS (Original API) ====================
    
    def log_prompt(self, prompt_token_count: int):
        """Log a prompt processing event"""
        print(f"[DEBUG] log_prompt called with {prompt_token_count} tokens")
        logger.info(f"ðŸ“Š log_prompt called with {prompt_token_count} tokens")
        try:
            self.metrics.increment_counter("llm_prompts_total")
            print("[DEBUG] llm_prompts_total incremented")
            logger.info("âœ… llm_prompts_total incremented")
            # Fix: Use the correct API for incrementing by a specific value
            counter = self.metrics.metrics.get("llm_prompt_tokens_total")
            if counter:
                counter.inc(prompt_token_count)
                print(f"[DEBUG] llm_prompt_tokens_total incremented by {prompt_token_count}")
                logger.info(f"âœ… llm_prompt_tokens_total incremented by {prompt_token_count}")
            else:
                print("[DEBUG] llm_prompt_tokens_total counter not found, registering...")
                logger.warning("âš ï¸ llm_prompt_tokens_total counter not found, registering...")
                # Counter doesn't exist, register it first
                self.metrics.register_counter("llm_prompt_tokens_total", "Total number of prompt tokens received")
                counter = self.metrics.metrics.get("llm_prompt_tokens_total")
                if counter:
                    counter.inc(prompt_token_count)
                    print(f"[DEBUG] llm_prompt_tokens_total registered and incremented by {prompt_token_count}")
                    logger.info(f"âœ… llm_prompt_tokens_total registered and incremented by {prompt_token_count}")
                else:
                    print("[DEBUG] Failed to register llm_prompt_tokens_total counter")
                    logger.error("âŒ Failed to register llm_prompt_tokens_total counter")
            # Add rolling metric for input tokens per minute
            print(f"[DEBUG] observe_rolling for llm_input_tokens_per_minute_rolling: {prompt_token_count}")
            self.metrics.observe_rolling("llm_input_tokens_per_minute_rolling", prompt_token_count)
        except Exception as e:
            print(f"[DEBUG] ERROR in log_prompt: {e}")
            logger.error(f"âŒ ERROR in log_prompt: {e}")
            import traceback
            print(traceback.format_exc())
            logger.error(f"Traceback: {traceback.format_exc()}")

    def log_response(self, generated_token_count: int):
        print(f"[DEBUG] log_response called with {generated_token_count} tokens")
        logger.info(f"ðŸ“Š log_response called with {generated_token_count} tokens")
        try:
            # Fix: Use the correct API for incrementing by a specific value
            counter = self.metrics.metrics.get("llm_tokens_generated_total")
            if counter:
                counter.inc(generated_token_count)
                print(f"[DEBUG] llm_tokens_generated_total incremented by {generated_token_count}")
                logger.info(f"âœ… llm_tokens_generated_total incremented by {generated_token_count}")
            else:
                print("[DEBUG] llm_tokens_generated_total counter not found, registering...")
                logger.warning("âš ï¸ llm_tokens_generated_total counter not found, registering...")
                # Counter doesn't exist, register it first
                self.metrics.register_counter("llm_tokens_generated_total", "Total number of tokens generated by the LLM")
                counter = self.metrics.metrics.get("llm_tokens_generated_total")
                if counter:
                    counter.inc(generated_token_count)
                    print(f"[DEBUG] llm_tokens_generated_total registered and incremented by {generated_token_count}")
                    logger.info(f"âœ… llm_tokens_generated_total registered and incremented by {generated_token_count}")
                else:
                    print("[DEBUG] Failed to register llm_tokens_generated_total counter")
                    logger.error("âŒ Failed to register llm_tokens_generated_total counter")
            # Add rolling metric for output tokens per minute
            print(f"[DEBUG] observe_rolling for llm_output_tokens_per_minute_rolling: {generated_token_count}")
            self.metrics.observe_rolling("llm_output_tokens_per_minute_rolling", generated_token_count)
        except Exception as e:
            print(f"[DEBUG] ERROR in log_response: {e}")
            logger.error(f"âŒ ERROR in log_response: {e}")
            import traceback
            print(traceback.format_exc())
            logger.error(f"Traceback: {traceback.format_exc()}")

    def observe_inference_time(self, start_time: float):
        """
        Observe inference duration and update rolling metrics.
        
        Will use native llama_cpp timings if available and enabled,
        otherwise falls back to calculated duration.
        """
        calculated_elapsed = time.time() - start_time
        logger.info(f"ðŸ“Š observe_inference_time called with calculated elapsed: {calculated_elapsed:.4f}s")
        
        # Try to get native timings first
        native_timings = self._get_native_timings()
        final_elapsed = calculated_elapsed  # Default to calculated
        
        if native_timings and self._use_native_timings:
            # Use native timing if available
            # llama_cpp provides timings in different formats, let's try to extract the total time
            if 'eval' in native_timings:
                # eval timing is usually the generation/inference time
                native_elapsed = native_timings['eval'].get('t_eval_ms', 0) / 1000.0
                if native_elapsed > 0:
                    final_elapsed = native_elapsed
                    logger.info(f"ðŸ”§ Using native inference timing: {final_elapsed:.4f}s (vs calculated: {calculated_elapsed:.4f}s)")
            elif 't_load_ms' in native_timings:
                # Fallback to total time if eval not available
                total_ms = native_timings.get('t_load_ms', 0) + native_timings.get('t_eval_ms', 0)
                if total_ms > 0:
                    final_elapsed = total_ms / 1000.0
                    logger.info(f"ðŸ”§ Using native total timing: {final_elapsed:.4f}s")
        
        try:
            self.metrics.observe_histogram("llm_inference_duration_seconds", final_elapsed)
            logger.info(f"âœ… llm_inference_duration_seconds histogram updated with {final_elapsed:.4f}s")
            
            # Add to rolling metrics for autoscaling (Tier-1 Essential)
            self.metrics.observe_rolling("llm_inference_duration_rolling", final_elapsed)
            logger.info(f"âœ… llm_inference_duration_rolling updated with {final_elapsed:.4f}s")
        except Exception as e:
            logger.error(f"âŒ ERROR in observe_inference_time: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    def observe_time_to_first_token(self, start_time: float):
        """
        Observe TTFT and update rolling metrics.
        
        Will use native llama_cpp timings if available and enabled.
        """
        calculated_ttft = time.time() - start_time
        final_ttft = calculated_ttft  # Default to calculated
        
        # Try to get native timings
        # native_timings = self._get_native_timings()
        # if native_timings and self._use_native_timings:
        #     # Look for prompt evaluation time as TTFT
        #     if 'prompt' in native_timings:
        #         native_ttft = native_timings['prompt'].get('t_prompt_ms', 0) / 1000.0
        #         if native_ttft > 0:
        #             final_ttft = native_ttft
        #             logger.info(f"ðŸ”§ Using native TTFT: {final_ttft:.4f}s")
        
        self.metrics.observe_histogram("llm_time_to_first_token_seconds", final_ttft)
        
        # Add to rolling metrics for autoscaling (Performance)
        self.metrics.observe_rolling("llm_ttft_rolling", final_ttft)

    def _get_process_resource_utilization(self) -> Dict[str, float]:
        """
        Get CPU, GPU, and memory utilization for the current process.
        """
        utilization = {
            "cpu_util": 0.0,
            "gpu_util": 0.0,
            "mem_bytes": 0.0,
        }

        # Get CPU and Memory usage from psutil
        if self.process:
            try:
                # Get CPU utilization as a percentage. interval=None compares to last call.
                utilization["cpu_util"] = self.process.cpu_percent(interval=None)
                # Get memory usage in bytes
                utilization["mem_bytes"] = self.process.memory_info().rss
            except Exception as e:
                logger.warning(f"Could not get CPU/Memory usage: {e}")

        # Get GPU usage from pynvml
        if pynvml:
            try:
                # Assuming one GPU, get handle for device 0
                handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                # Get utilization rates
                gpu_util_info = pynvml.nvmlDeviceGetUtilizationRates(handle)
                utilization["gpu_util"] = float(gpu_util_info.gpu)
            except pynvml.NVMLError as e:
                # This can happen if the GPU is not available or nvidia-smi is not running
                logger.warning(f"Could not get GPU usage from pynvml: {e}")
            except Exception as e:
                logger.error(f"An unexpected error occurred while fetching GPU metrics: {e}")

        return utilization

    def observe_resource_utilization(self, cpu_util: float, gpu_util: float, mem_bytes: float):
        """
        Observe and set resource utilization gauges.
        
        Args:
            cpu_util: CPU utilization percentage
            gpu_util: GPU utilization percentage
            mem_bytes: Memory usage in bytes
        """
        try:
            self.metrics.set_gauge("llm_cpu_utilization", cpu_util)
            self.metrics.set_gauge("llm_gpu_utilization", gpu_util)
            self.metrics.set_gauge("llm_memory_usage_bytes", mem_bytes)
            logger.info(f"âœ… Resource utilization gauges updated: CPU {cpu_util}%, GPU {gpu_util}%, Mem {mem_bytes}B")
        except Exception as e:
            logger.error(f"âŒ ERROR in observe_resource_utilization: {e}")

    def observe_time_per_output_token(self, start_time: float, token_count: int):
        """
        Observe TPOT and update rolling metrics.
        
        Will use native llama_cpp timings if available and enabled.
        """
        logger.info(f"ðŸ“Š observe_time_per_output_token called with token_count: {token_count}")
        try:
            if token_count > 0:
                calculated_tpot = (time.time() - start_time) / token_count
                final_tpot = calculated_tpot  # Default to calculated
                
                # Try to get native timings
                native_timings = self._get_native_timings()
                if native_timings and self._use_native_timings:
                    # Calculate TPOT from native eval timing
                    if 'eval' in native_timings:
                        eval_time_ms = native_timings['eval'].get('t_eval_ms', 0)
                        eval_tokens = native_timings['eval'].get('n_eval', token_count)
                        if eval_time_ms > 0 and eval_tokens > 0:
                            native_tpot = (eval_time_ms / 1000.0) / eval_tokens
                            final_tpot = native_tpot
                            logger.info(f"ðŸ”§ Using native TPOT: {final_tpot:.4f}s per token")
                
                logger.info(f"ðŸ“Š Final TPOT: {final_tpot:.4f}s per token")
                self.metrics.observe_histogram("llm_time_per_output_token_seconds", final_tpot)
                logger.info(f"âœ… llm_time_per_output_token_seconds histogram updated")
                
                # Add to rolling metrics for autoscaling (Performance)
                self.metrics.observe_rolling("llm_tpot_rolling", final_tpot)
                logger.info(f"âœ… llm_tpot_rolling updated with {final_tpot:.4f}s")
            else:
                logger.warning("âš ï¸ Token count is 0, skipping TPOT calculation")
        except Exception as e:
            logger.error(f"âŒ ERROR in observe_time_per_output_token: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    def update_tokens_per_second(self, tokens_generated: int, duration_seconds: float):
        """
        Update tokens per second and rolling metrics.
        
        Will use native llama_cpp timings if available and enabled.
        """
        logger.info(f"ðŸ“Š update_tokens_per_second called with tokens: {tokens_generated}, duration: {duration_seconds:.4f}s")
        try:
            final_duration = duration_seconds
            final_tokens = tokens_generated
            
            # Try to get native timings for more accurate TPS
            native_timings = self._get_native_timings()
            if native_timings and self._use_native_timings:
                if 'eval' in native_timings:
                    eval_time_ms = native_timings['eval'].get('t_eval_ms', 0)
                    eval_tokens = native_timings['eval'].get('n_eval', 0)
                    if eval_time_ms > 0 and eval_tokens > 0:
                        final_duration = eval_time_ms / 1000.0
                        final_tokens = eval_tokens
                        logger.info(f"ðŸ”§ Using native timing data: {final_tokens} tokens in {final_duration:.4f}s")
            
            if final_duration > 0:
                tps = final_tokens / final_duration
                logger.info(f"ðŸ“Š Calculated TPS: {tps:.2f} tokens/sec")
                self.metrics.set_gauge("llm_tokens_per_second", tps)
                logger.info(f"âœ… llm_tokens_per_second gauge set to {tps:.2f}")
                
                # Add to rolling metrics for autoscaling (Performance)
                self.metrics.observe_rolling("llm_tps_rolling", tps)
                logger.info(f"âœ… llm_tps_rolling updated with {tps:.2f}")
            else:
                logger.warning("âš ï¸ Duration is 0, skipping TPS calculation")
        except Exception as e:
            logger.error(f"âŒ ERROR in update_tokens_per_second: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")

    def increment_inference_errors(self):
        """Increment inference errors counter"""
        self.metrics.increment_counter("llm_inference_errors_total")

    def increase_active_sessions(self):
        """Increase active sessions and update rolling metrics"""
        self._add_to_gauge("llm_active_sessions", 1)
        
        # Update rolling metrics for autoscaling (Tier-1 Essential)
        current_sessions = self._get_gauge_value("llm_active_sessions")
        self.metrics.observe_rolling("llm_active_sessions_rolling", current_sessions)

    def decrease_active_sessions(self):
        """Decrease active sessions and update rolling metrics"""
        self._add_to_gauge("llm_active_sessions", -1)
        
        # Update rolling metrics for autoscaling (Tier-1 Essential)
        current_sessions = self._get_gauge_value("llm_active_sessions")
        self.metrics.observe_rolling("llm_active_sessions_rolling", current_sessions)

    # ==================== ROLLING METRICS UPDATE ====================

    def update_rolling_metrics(self):
        """
        Manual update of rolling metrics for LLM-specific gauge values.
        Called periodically by the main application.
        
        NOTE: This method now actively polls for process-specific resource utilization.
        """
        print("[DEBUG] update_rolling_metrics called")
        try:
            # Get real resource utilization for the current process
            resource_util = self._get_process_resource_utilization()
            self.observe_resource_utilization(
                cpu_util=resource_util["cpu_util"],
                gpu_util=resource_util["gpu_util"],
                mem_bytes=resource_util["mem_bytes"]
            )
            
            # Update LLM-specific rolling metrics that aren't provided by infrastructure
            
            # LLM Active Sessions (Tier-1 Essential for autoscaling)
            current_sessions = self._get_gauge_value("llm_active_sessions")
            if current_sessions is not None:
                self.metrics.observe_rolling("llm_active_sessions_rolling", current_sessions)
            
            # LLM Tokens Per Second (Performance metric)
            current_tps = self._get_gauge_value("llm_tokens_per_second")
            if current_tps is not None:
                self.metrics.observe_rolling("llm_tps_rolling", current_tps)
            
            # LLM-specific resource utilization (only if explicitly set by library.py)
            # These are different from hardware-level metrics in block_metrics.json
            llm_cpu_util = self._get_gauge_value("llm_cpu_utilization")
            if llm_cpu_util is not None:
                self.metrics.observe_rolling("llm_cpu_utilization_rolling", llm_cpu_util)
            
            llm_gpu_util = self._get_gauge_value("llm_gpu_utilization")  
            if llm_gpu_util is not None:
                self.metrics.observe_rolling("llm_gpu_utilization_rolling", llm_gpu_util)
            
            llm_memory_usage = self._get_gauge_value("llm_memory_usage_bytes")
            if llm_memory_usage is not None:
                self.metrics.observe_rolling("llm_memory_usage_rolling", llm_memory_usage)
            
            # Add rolling averages for input/output tokens per minute
            input_tokens_rolling = self.metrics.rolling_metrics.get("llm_input_tokens_per_minute_rolling")
            if input_tokens_rolling:
                print(f"[DEBUG] input_tokens_rolling.current(): {input_tokens_rolling.current()}")
                self.metrics.observe_rolling("llm_input_tokens_per_minute_rolling", input_tokens_rolling.current())
            output_tokens_rolling = self.metrics.rolling_metrics.get("llm_output_tokens_per_minute_rolling")
            if output_tokens_rolling:
                print(f"[DEBUG] output_tokens_rolling.current(): {output_tokens_rolling.current()}")
                self.metrics.observe_rolling("llm_output_tokens_per_minute_rolling", output_tokens_rolling.current())
        except Exception as e:
            print(f"[DEBUG] ERROR in update_rolling_metrics: {e}")
            # Use logging instead of print in production
            pass

    # ==================== HELPER METHODS ====================
    
    def _add_to_gauge(self, name: str, delta: int):
        """Helper to add/subtract from gauge value"""
        gauge = self.metrics.metrics.get(name)
        if gauge:
            current = gauge._value.get()
            gauge.set(current + delta)

    def _get_gauge_value(self, name: str) -> Optional[float]:
        """Helper to get current gauge value"""
        gauge = self.metrics.metrics.get(name)
        if gauge:
            return gauge._value.get()
        return None
