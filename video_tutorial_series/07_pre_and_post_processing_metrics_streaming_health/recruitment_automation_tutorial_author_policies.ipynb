{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4b42360",
   "metadata": {},
   "source": [
    "# Tutorial: Building a Workflow From Author's Policies\n",
    "\n",
    "In this tutorial we will explore a powerful workflow based on a block's **pre_processing** and **post_processing** policies as provided by the authors of the models and a **generic Large Language Model (LLM) model** playing the role of the Author's Model. We will then go on to show how we can create our custom generic policies as tools for calling different tools decided by the LLM using the AIOSv1 policies ecosystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb8de12-4959-4748-8b4f-d1c78b45aaec",
   "metadata": {},
   "source": [
    "## Tutorial Overview\n",
    "1. **Task Explanation**: Automate the screening of candidate resumes against a job description to identify and process qualified applicants.\n",
    "2. **The Architecture**: Block Policies: Utilize a single, generic LLM block customized at runtime with author's pre-processing and post-processing policies.\n",
    "3. **Block Allocation Json**: Define the entire pipeline, including the model and its attached policies, within a single JSON configuration file.\n",
    "4. **Policies and Inference Code Overview**: Review the Python code for the resume-parsing policy, the tools-orchestrating policy, and the model inference script.\n",
    "5. **Running the Pipeline**: Making an Inference Request: Trigger the entire automated recruitment workflow with a single API request to the deployed block's endpoint.\n",
    "6. **Conclusion**: Understand how policies enable the creation of modular, scalable, and complex real-world workflows on the AIOS platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e9712",
   "metadata": {},
   "source": [
    "## 1. Task\n",
    "\n",
    "Our goal is to automate the initial screening of resumes, and send them a mail communication. The pipeline should:\n",
    "1.  Accept a `.zip` file containing multiple resumes in PDF format.\n",
    "2.  Extract the text from each resume.\n",
    "3.  Use an LLM to analyze the resumes against a job description and decide on next steps (e.g., background check, send communication email).\n",
    "4.  Execute these tool calling steps automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e77398",
   "metadata": {},
   "source": [
    "## 2. The Architecture: Block Policies\n",
    "\n",
    "### What are Policies in AIOSv1?\n",
    "A policy is a dynamically loadable, executable Python code that is used in various places and use cases across the AIOS system. Since policies are dynamic, they allow developers to implement custom functionalities throughout the AIOS system. \n",
    "\n",
    "> ðŸ“– **Further Reading**: [AIOSv1 Policies System Overview](https://github.com/OpenCyberspace/OpenOS.AI-Documentation/blob/main/policies-system/policies-system.md)\n",
    "\n",
    "The pipeline is designed to be highly modular. We use a central, generic LLM block  with the author's runtime policies implemented as `pre_processing` and `post_processing` policies.\n",
    "\n",
    "We also show,a key concept in this architecture, that tool calling is implemented as policies that are **stateless**, or \"fire and die.\" As detailed in the `policies-system.md` documentation, policies of this type Type 3 are loaded for a single execution to perform a specific task and are then terminated. They do not maintain any memory or state between requests, which makes the system robust, scalable, and easy to debug.\n",
    "\n",
    "**Blocks Flow Diagram:** \n",
    "```\n",
    "[Input: zip file of resumes] -> [Block Level Preprocessing Policy] -> [Generic LLM Model] -> [Block Level Postprocessing Policy] -> [Automated Actions]\n",
    "```\n",
    "\n",
    "*   **`Preprocessing Policy`**: Its only job is to handle the input data. In this case, it unzips the file, finds all PDFs, and extracts their text content. It then passes this data as `supplemental_data` to the next block.\n",
    "*   **`Generic LLM Model`**: This is a standard Llama_cpp_python model code. It's designed to be unaware of the specific task. It receives a prompt and, optionally, `supplemental_data`. Its role is to generate a response based on the inputs.\n",
    "*   **`Postprocessing Policy`**: This policy takes the raw output from the LLM, parses it, and takes action. For our use case, it's responsible for parsing the JSON and executing the requested `tool_calls`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8934b581-1759-4cb7-8151-33ec371f9c8a",
   "metadata": {},
   "source": [
    "If you are interested in custom policies for Author's models, refer below tutorial, \n",
    "- [Tutorial: Simple vDAG for workflow Automation with Swappable policies](http://CLUSTER2NODE1:9999/notebooks/recruitment_automation_vdag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8204df5",
   "metadata": {},
   "source": [
    "## 3. Block Allocation Json `allocation-llama4scout_recruiter.json`\n",
    "\n",
    "This JSON file is the master configuration for our `llama4-scout` block. It defines everything from the model to be used, to the initial system prompt, and the policies that govern its behavior. It's the central piece that orchestrates the entire pipeline. Let's look at the key parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0da41e",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "{\n",
    "    \"body\": {\n",
    "        \"spec\": {\n",
    "            \"values\": {\n",
    "                \"blockComponentURI\": \"model.llama4-scout-17b:1.0.0-stable\",\n",
    "                \"blockInitData\": {\n",
    "                    \"model_name\": \"Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL/...\",\n",
    "                    \"system_message\": \"You are an expert recruitment assistant. Your task is to analyze the provided resume context and create a JSON-based execution plan...Your output MUST be a valid JSON object with a single key, \\\\\"tool_calls\\\\\"...\"\n",
    "                },\n",
    "                \"policyRulesSpec\": [\n",
    "                    {\n",
    "                        \"values\": {\n",
    "                            \"name\": \"pre_processing\",\n",
    "                            \"policyRuleURI\": \"recruitment_preprocessing:1.0.0-stable\"\n",
    "                        }\n",
    "                    },                    \n",
    "                    {\n",
    "                        \"values\": {\n",
    "                            \"name\": \"post_processing\",\n",
    "                            \"policyRuleURI\": \"recruitment_postprocessing:1.0.0-stable\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca3b3644-9b0e-43aa-bd48-8c0ad9a3adde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"head\": {\n",
      "        \"templateUri\": \"Parser/V1\",\n",
      "        \"parameters\": {}\n",
      "    },\n",
      "    \"body\": {\n",
      "        \"spec\": {\n",
      "            \"values\": {\n",
      "                \"mode\": \"allocate\",\n",
      "                \"blockId\": \"llama4-scout-17b-block\",\n",
      "                \"blockComponentURI\": \"model.llama4-scout-17b:1.0.0-stable\",\n",
      "                \"minInstances\": 1,\n",
      "                \"maxInstances\": 3,\n",
      "                \"blockInitData\": {\n",
      "                    \"model_name\": \"Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL/Llama-4-Scout-17B-16E-Instruct-UD-Q8_K_XL-00001-of-00003.gguf\",\n",
      "                    \"system_message\": \"You are an expert recruitment assistant. Your task is to analyze the provided resume context and create a JSON-based execution plan for a multi-stage recruitment pipeline. Your output MUST be a valid JSON object with a single key, \\\"tool_calls\\\". This key must contain a list of jobs to be executed in sequence.\\n\\nIMPORTANT: Group ALL candidates into a SINGLE background check job. Do not create separate background check jobs for each candidate.\\n\\nEach job in the list is a JSON object and MUST have the following keys:\\n- \\\"name\\\": A descriptive name for the job (e.g., \\\"background-check-batch-1\\\").\\n- \\\"policy_rule_uri\\\": The specific URI for the policy to be executed.\\n- \\\"inputs\\\": A JSON object containing the parameters for that policy.\\n\\nHere are the available policies and their details:\\n\\n1.  **Background Check Policy**\\n    -   `policy_rule_uri`: `generic_background_check:1.0.0-stable`\\n    -   `inputs`: A JSON object with a \\\"candidates\\\" key, which is an ARRAY containing ALL candidate objects with \\\"id\\\", \\\"name\\\", and \\\"email\\\".\\n\\n2.  **Send Email Policy**\\n    -   `policy_rule_uri`: `generic_send_email:1.0.0-stable`\\n    -   `inputs`: A JSON object with \\\"subject\\\" and \\\"body\\\" keys. The \\\"to\\\" address will be handled automatically by the pipeline for candidates who pass the background check.\\n\\n**Correct Output Format (Note: ALL candidates in ONE background check):**\\n\\n{\\n  \\\"tool_calls\\\": [\\n    {\\n      \\\"name\\\": \\\"background-check-batch\\\",\\n      \\\"policy_rule_uri\\\": \\\"generic_background_check:1.0.0-stable\\\",\\n      \\\"inputs\\\": {\\n        \\\"candidates\\\": [\\n          {\\n            \\\"id\\\": \\\"1\\\",\\n            \\\"name\\\": \\\"John Doe\\\",\\n            \\\"email\\\": \\\"john.doe@example.com\\\"\\n          },\\n          {\\n            \\\"id\\\": \\\"2\\\",\\n            \\\"name\\\": \\\"Jane Smith\\\",\\n            \\\"email\\\": \\\"jane.smith@example.com\\\"\\n          }\\n        ]\\n      }\\n    },\\n    {\\n      \\\"name\\\": \\\"send-follow-up-email\\\",\\n      \\\"policy_rule_uri\\\": \\\"generic_send_email:1.0.0-stable\\\",\\n      \\\"inputs\\\": {\\n        \\\"subject\\\": \\\"Next Steps in Your Application\\\",\\n        \\\"body\\\": \\\"Thank you for your application. We will be in touch with the next steps shortly.\\\"\\n      }\\n    }\\n  ]\\n}\\n\\nAlways combine ALL candidates into a SINGLE background check job. Do not create individual background check jobs per candidate.\"\n",
      "\n",
      "                },\n",
      "                \"initSettings\": {\n",
      "                    \"tensor_parallel\": true,\n",
      "                    \"device\": \"cuda\",\n",
      "                    \"quantization_type\": \"int8\",\n",
      "                    \"cleanup_enabled\":  true,\n",
      "                    \"cleanup_check_interval\": 60,\n",
      "                    \"cleanup_session_timeout\": 1800,\n",
      "                    \"generation_config\": {\n",
      "                        \"max_new_tokens\": 2048,\n",
      "                        \"temperature\": 0.6,\n",
      "                        \"top_k\": 50,\n",
      "                        \"top_p\": 0.9,\n",
      "                        \"repetition_penalty\": 1.1,\n",
      "                        \"do_sample\": true\n",
      "                    }\n",
      "                },\n",
      "                \"policyRulesSpec\": [\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"pre_processing\",\n",
      "                            \"policyRuleURI\": \"recruitment_preprocessing:1.0.0-stable\",\n",
      "                            \"parameters\": {},\n",
      "                            \"settings\": {}\n",
      "                        }\n",
      "                    },                    \n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"post_processing\",\n",
      "                            \"policyRuleURI\": \"recruitment_postprocessing:1.0.0-stable\",\n",
      "                            \"parameters\": {},\n",
      "                            \"settings\": {}\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"clusterAllocator\",\n",
      "                            \"policyRuleURI\": \"cluster-selector:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"filter\": {\n",
      "                                    \"clusterQuery\": {\n",
      "                                        \"variable\": \"id\",\n",
      "                                        \"operator\": \"==\",\n",
      "                                        \"value\": \"gcp-cluster-2\"\n",
      "                                    }\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"max_candidates\": 2\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"resourceAllocator\",\n",
      "                            \"policyRuleURI\": \"allocator:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"allocation_data\": {\n",
      "                                    \"node_id\": \"wc-gpu-node2\",\n",
      "                                    \"gpus\": [0,1]\n",
      "                                }\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"selection_mode\": \"balanced\"\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"loadBalancer\",\n",
      "                            \"policyRuleURI\": \"load_balancer:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"cache_sessions\": true\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"session_cache_size\": 2000\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"stabilityChecker\",\n",
      "                            \"policyRuleURI\": \"health_checker:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"unhealthy_threshold\": 2\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"check_interval_sec\": 10\n",
      "                            }\n",
      "                        }\n",
      "                    },\n",
      "                    {\n",
      "                        \"values\": {\n",
      "                            \"name\": \"autoscaler\",\n",
      "                            \"policyRuleURI\": \"autoscaler:2.0-stable\",\n",
      "                            \"parameters\": {\n",
      "                                \"target_gpu_utilization\": 0.8\n",
      "                            },\n",
      "                            \"settings\": {\n",
      "                                \"scale_up_cooldown\": 45,\n",
      "                                \"scale_down_cooldown\": 90\n",
      "                            }\n",
      "                        }\n",
      "                    }\n",
      "                ]\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat allocation-llama4scout_recruiter_block.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e4ed200-7b5b-4d94-9b2f-272b7a197245",
   "metadata": {},
   "source": [
    "# 4. Policies and Inference Code Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b93eda",
   "metadata": {},
   "source": [
    "### Preprocessing Policy: Getting the Data Ready\n",
    "\n",
    "This policy handles the initial data extraction. It's a simple Python function that uses standard libraries to process the file.\n",
    "\n",
    "**File:** `policies/recruitment_automation/preprocessing_policy/code/function.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505dda9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import fitz  # PyMuPDF\n",
    "import io\n",
    "\n",
    "def eval(data):\n",
    "    # Assumes 'file' is a key in the input data, containing the zip file bytes\n",
    "    zip_file_bytes = data.get('file')\n",
    "    if not zip_file_bytes:\n",
    "        return {\"error\": \"No file provided\"}\n",
    "\n",
    "    extracted_texts = []\n",
    "    with io.BytesIO(zip_file_bytes) as zip_stream:\n",
    "        with zipfile.ZipFile(zip_stream, 'r') as zip_ref:\n",
    "            for file_name in zip_ref.namelist():\n",
    "                if file_name.lower().endswith('.pdf'):\n",
    "                    with zip_ref.open(file_name) as pdf_file:\n",
    "                        pdf_bytes = pdf_file.read()\n",
    "                        with fitz.open(stream=pdf_bytes, filetype=\"pdf\") as doc:\n",
    "                            text = \"\"\n",
    "                            for page in doc:\n",
    "                                text += page.get_text()\n",
    "                            extracted_texts.append({\"file_name\": file_name, \"content\": text})\n",
    "\n",
    "    # The output of this policy becomes the 'supplemental_data' for the LLM block\n",
    "    return {\"candidates\": extracted_texts}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6b69e",
   "metadata": {},
   "source": [
    "### Postprocessing Policy: Taking Action\n",
    "\n",
    "This policy is responsible for interpreting the LLM's response. A key lesson learned was that LLMs don't always produce perfect JSON. They might wrap it in markdown or add extra text. Therefore, we need a robust way to extract the JSON.\n",
    "\n",
    "**File:** `policies/recruitment_automation/postprocessing_policy/code/function.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71a90968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    # Use regex to find JSON wrapped in markdown-style code blocks\n",
    "    match = re.search(r\"```json\\n(.*?)\\n```\", response_text, re.DOTALL)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        # Fallback for cases where there's no markdown wrapping\n",
    "        json_str = response_text\n",
    "    \n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        # Handle cases where the extracted string is still not valid JSON\n",
    "        return {\"error\": \"Failed to decode JSON from LLM response\", \"response\": response_text}\n",
    "        \n",
    "def submit_and_monitor_job(self, job_name: str, policy_rule_uri: str, params: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Submits a job to an AIOS executor and monitors it until completion.\n",
    "    \"\"\"\n",
    "    # 1. Submit the job\n",
    "    \n",
    "    submit_endpoint = f\"{self.aios_url}/jobs/submit/executor-001\"\n",
    "    \n",
    "    submit_payload = {\n",
    "        \"name\": job_name,\n",
    "        \"policy_rule_uri\": policy_rule_uri,\n",
    "        \"inputs\": params,\n",
    "        \"policy_rule_parameters\": {},\n",
    "        \"node_selector\": {}\n",
    "    }\n",
    "    \n",
    "    logger.info(f\"Submitting job '{job_name}' for policy '{policy_rule_uri}' with params: {_snippet(params)}\")\n",
    "    \n",
    "    try:\n",
    "        headers = {'Content-Type': 'application/json'}\n",
    "        response = requests.post(submit_endpoint, json=submit_payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        job_info = response.json()\n",
    "        \n",
    "        job_id = job_info.get(\"job_id\")\n",
    "        if not job_id:\n",
    "            raise ValueError(f\"Job submission did not return a job_id. Response: {_snippet(job_info)}\")\n",
    "        logger.info(f\"Job '{job_id}' submitted for policy '{policy_rule_uri}'.\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if e.response and 500 <= e.response.status_code < 600:\n",
    "            logger.error(f\"Server error when submitting job. Status: {e.response.status_code}. Response: {e.response.text}\")\n",
    "        raise\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to submit job for policy '{policy_rule_uri}': {e}\")\n",
    "        raise\n",
    "\n",
    "    # 2. Poll for job completion\n",
    "    status_endpoint = f\"{self.aios_url}/jobs/{job_id}\"\n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    while True:\n",
    "        if time.monotonic() - start_time > self.job_timeout:\n",
    "            raise TimeoutError(f\"Job '{job_id}' timed out after {self.job_timeout} seconds.\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(status_endpoint, timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                resp_json = response.json()\n",
    "                \n",
    "                # Check for success at the top level\n",
    "                if resp_json.get(\"success\"):\n",
    "                    data = resp_json.get(\"data\", {})\n",
    "                    status = data.get(\"job_status\")\n",
    "                    \n",
    "                    if status == \"completed\":\n",
    "                        logger.info(f\"Job '{job_id}' completed successfully. Retrieving result from final poll.\")\n",
    "                        result_data = data.get(\"job_output_data\")\n",
    "                        if result_data is None:\n",
    "                            logger.warning(f\"Job result for '{job_id}' did not contain 'job_output_data'. Full response: {_snippet(resp_json)}\")\n",
    "                            return resp_json # Fallback to the full body\n",
    "                        \n",
    "                        logger.info(f\"Result for job '{job_id}': {_snippet(result_data)}\")\n",
    "                        return result_data\n",
    "                    elif status == \"failed\":\n",
    "                        # The detailed result might be in 'job_output_data'\n",
    "                        details = data.get(\"job_output_data\", \"No details provided.\")\n",
    "                        raise RuntimeError(f\"Job '{job_id}' failed. Details: {details}\")\n",
    "                    elif status in [\"queued\", \"running\"]:\n",
    "                        logger.debug(f\"Job '{job_id}' is '{status}'. Polling again.\")\n",
    "                        time.sleep(self.job_poll_interval)\n",
    "                    else:\n",
    "                        logger.warning(f\"Job '{job_id}' has unknown status: '{status}'. Retrying...\")\n",
    "                        time.sleep(self.job_poll_interval)\n",
    "                else:\n",
    "                    # Handle cases where 'success' is false or missing\n",
    "                    error_message = resp_json.get(\"message\", \"Unknown error during polling.\")\n",
    "                    logger.warning(f\"Polling for job '{job_id}' was not successful: {error_message}. Retrying...\")\n",
    "                    time.sleep(self.job_poll_interval)\n",
    "            else:\n",
    "                logger.warning(f\"Polling for job '{job_id}' returned status {response.status_code}. Retrying...\")\n",
    "                time.sleep(self.job_poll_interval)\n",
    "                        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.warning(f\"Error polling job status for '{job_id}': {e}. Retrying...\")\n",
    "            time.sleep(self.job_poll_interval)\n",
    "                \n",
    "def eval(data):\n",
    "    # 'data' is the raw output from the LLM block\n",
    "    llm_response_text = data.get('response', '')\n",
    "    \n",
    "    # Attempt to parse the response directly\n",
    "    try:\n",
    "        parsed_json = json.loads(llm_response_text)\n",
    "    except json.JSONDecodeError:\n",
    "        # If direct parsing fails, use our robust extraction function\n",
    "        parsed_json = extract_json_from_response(llm_response_text)\n",
    "\n",
    "    if 'error' in parsed_json:\n",
    "        return parsed_json\n",
    "\n",
    "    # The core logic: check for the 'tool_calls' key\n",
    "    if 'tool_calls' not in parsed_json:\n",
    "        return {\"error\": \"'tool_calls' key not found in the LLM response.\", \"response_data\": parsed_json}\n",
    "\n",
    "    # In a real system, you would execute the tool calls here caliing submit_and_monitor_job\n",
    "    return {\"executed_tool_calls\": parsed_json['tool_calls']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336aa478",
   "metadata": {},
   "source": [
    "### Generic Tool Policies: The \"Fire and Die\" Policies\n",
    "\n",
    "It's important to note that the `postprocessing_policy` itself doesn't contain the logic for every possible action. Instead, it acts as a dispatcher. When the LLM requests a `background-check` or a `send-email`, the postprocessing policy calls *other* generic, stateless policies to do the actual work.\n",
    "\n",
    "These tool policies, such as `background_check_policy` and `send_email_policy`, are loaded, executed with the inputs provided by the `postprocessing_policy`, and then terminated. This keeps the entire system modular, as new tools can be added without changing the core pipeline, and each tool is a self-contained, stateless unit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357be61",
   "metadata": {},
   "source": [
    "## Making the LLM Context-Aware\n",
    "\n",
    "We modify the `on_data` method in the LLM block's code to explicitly format the supplemental data and prepend it to the user's prompt. This ensures the LLM has the necessary context to make an informed decision.\n",
    "\n",
    "**File:** Refer `main.py` from [02_Part2_onboard_custom_llama_cpp\n",
    "](https://github.com/OpenCyberspace/AIOS_AI_Blueprints/blob/main/video_tutorial_series/02_Part2_onboard_custom_llama_cpp/02-Model-Integration-Setup.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simplified representation of the key logic in main_more_context.py\n",
    "\n",
    "class LlamaCppBlock:\n",
    "    # ... (other class methods)\n",
    "\n",
    "    def on_data(self, input_data):\n",
    "        message = input_data.get(\"message\", \"\")\n",
    "        supplemental_context = \"\"\n",
    "\n",
    "        # Check for our specific supplemental data from the preprocessing policy\n",
    "        if \"supplemental_data\" in input_data and \"candidates\" in input_data[\"supplemental_data\"]:\n",
    "            candidates = input_data[\"supplemental_data\"][\"candidates\"]\n",
    "            \n",
    "            # **THE CRITICAL FIX**: Format the resume data into a clear context string\n",
    "            context_parts = [\"\\n\\n--- START OF SUPPLEMENTAL DATA ---\"]\n",
    "            for i, candidate in enumerate(candidates):\n",
    "                context_parts.append(f\"\\n--- Resume {i+1}: {candidate.get('file_name', 'Unknown')} ---\")\n",
    "                context_parts.append(candidate.get('content', 'No content'))\n",
    "            context_parts.append(\"\\n--- END OF SUPPLEMENTAL DATA ---\")\n",
    "            supplemental_context = \"\\n\".join(context_parts)\n",
    "            \n",
    "            # Clean up the input data so it's not processed further\n",
    "            del input_data[\"supplemental_data\"]\n",
    "\n",
    "        # Prepend the context to the user's original message\n",
    "        final_message = f\"{supplemental_context}\\n\\nUSER REQUEST:\\n{message}\"\n",
    "\n",
    "        # ... (rest of the logic to send 'final_message' to the LLM)\n",
    "        # self.llm.create_chat_completion(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38d5f2",
   "metadata": {},
   "source": [
    "## 5. Running the Pipeline: Making an Inference Request\n",
    "\n",
    "Now that all the components are in place, we can run the entire pipeline by sending an inference request to the `llama4-scout` block. This request includes the job description in the `message` and the zipped resumes as a base64-encoded file.\n",
    "\n",
    "The block will automatically trigger the `pre_processing policy` to handle the file, pass the extracted text to the LLM, and then use the `post_processing policy` to execute the generated tool calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98312377",
   "metadata": {},
   "source": [
    "Instead of manually creating the `curl` command, we can use Python to build it for us. The following cell will:\n",
    "1.  Locate the `resume_compressed.zip` file.\n",
    "2.  Read its content in binary format.\n",
    "3.  Encode the content into a Base64 string.\n",
    "4.  Construct the complete `request payload` for inference\n",
    "5.  Perform the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49784d89-4d98-443d-97c2-38d5acc901d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: grpcio in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: grpcio-tools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (1.74.0)\n",
      "Requirement already satisfied: protobuf in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (6.31.1)\n",
      "Requirement already satisfied: setuptools in /home/srikanth_g_cognitif_ai/venvs/hface/lib/python3.12/site-packages (from grpcio-tools) (80.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install grpcio grpcio-tools protobuf\n",
    "\n",
    "import sys\n",
    "sys.path.append('../utils/inference_client')\n",
    "\n",
    "import grpc\n",
    "import json\n",
    "import time\n",
    "\n",
    "import service_pb2\n",
    "import service_pb2_grpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d27a55b-d3c7-4013-806b-28bfcb18a5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(block_id, session_id, seq_no, message, generation_config):\n",
    "    SERVER_ADDRESS = \"CLUSTER1MASTER:31500\"\n",
    "    \n",
    "    # Connect to the gRPC server\n",
    "    channel = grpc.insecure_channel(SERVER_ADDRESS)\n",
    "    stub = service_pb2_grpc.BlockInferenceServiceStub(channel)\n",
    "\n",
    "\n",
    "    zip_file_path = \"resume_compressed.zip\"\n",
    "    with open(zip_file_path, \"rb\") as f:\n",
    "        zip_data = f.read()\n",
    "    \n",
    "    file_info = service_pb2.FileInfo(\n",
    "        metadata=json.dumps({\"filename\": zip_file_path, \"size\": len(zip_data)}),\n",
    "        file_data=zip_data\n",
    "    )\n",
    "\n",
    "    # Create the BlockInferencePacket request\n",
    "    request = service_pb2.BlockInferencePacket(\n",
    "        block_id= block_id,\n",
    "        session_id=session_id,\n",
    "        seq_no=seq_no,\n",
    "        frame_ptr=b\"\",  # Emptbytes for now\n",
    "        data=json.dumps({\n",
    "         \"mode\": \"chat\",\n",
    "         \"message\": message,\n",
    "         \"gen_params\":generation_config\n",
    "       }),\n",
    "        query_parameters=\"\",\n",
    "        ts=1234567890.0,\n",
    "        files=[file_info],  # Attach the file\n",
    "        output_ptr=b''\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        st = time.time()\n",
    "        # Make the gRPC call\n",
    "        response = stub.infer(request)\n",
    "        et = time.time()\n",
    "\n",
    "        print(\"\\n=== Response Received ===\")\n",
    "        print(f\"Latency: {et - st}s\")\n",
    "        print(f\"Session ID: {response.session_id}\")\n",
    "        print(f\"Sequence No: {response.seq_no}\")\n",
    "        print(f\"Data: {response.data}\")\n",
    "        print(f\"Timestamp: {response.ts}\")\n",
    "        print(f\"Output Ptr: {response.output_ptr}\")\n",
    "        print(f\"Files Received: {len(response.files)}\")\n",
    "\n",
    "        # Parse JSON response data\n",
    "        try:\n",
    "            response_data = json.loads(response.data)\n",
    "            print(f\"Parsed Response: {response_data}\")\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Response data is not a valid JSON string.\")\n",
    "\n",
    "    except grpc.RpcError as e:\n",
    "        print(f\"gRPC Error: {e.code()} - {e.details()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1773a5bb-2fee-423c-8cf5-ec80e609d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_config = {\n",
    "    \"temperature\": 0.1,\n",
    "    # \"min_p\": 0.01,\n",
    "    # \"top_k\": 64,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 4096 # Set a limit for the response length\n",
    "}\n",
    "\n",
    "# Create the BlockInferencePacket request\n",
    "\n",
    "recruitment_prompt = \"\"\"We're hiring for a Senior Computer Vision Engineer. Requirements include:\n",
    "- 6+ years of hands-on experience in computer vision and deep learning.\n",
    "- Production-level experience with frameworks like PyTorch or TensorFlow, and libraries like OpenCV.\n",
    "- Master's degree or Ph.D. in Computer Science or a related field.\n",
    "\n",
    "Please analyze the provided resumes and create a recruitment plan that:\n",
    "1. Initiates background checks for qualified candidates\n",
    "2. Sends appropriate follow-up communications\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ce7ff1-c20d-4828-9a05-3385e64c6d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Response Received ===\n",
      "Latency: 56.35062766075134s\n",
      "Session ID: session_notebook_7\n",
      "Sequence No: 1\n",
      "Data: {\n",
      "  \"message\": \"Recruitment pipeline processing finished.\",\n",
      "  \"background_check_processed\": true,\n",
      "  \"processed_tool_calls\": 2\n",
      "}\n",
      "Timestamp: 1234567890.0\n",
      "Output Ptr: {\"outputs\": [{\"host\": \"inference-server.inference-server.svc.cluster.local\", \"port\": 6379, \"queue_name\": \"instance-2__session_notebook_7__1\"}]}\n",
      "Files Received: 1\n",
      "Parsed Response: {'message': 'Recruitment pipeline processing finished.', 'background_check_processed': True, 'processed_tool_calls': 2}\n"
     ]
    }
   ],
   "source": [
    "run_inference(\n",
    "    block_id=\"llama4-scout-17b-block\",\n",
    "    session_id=\"session_notebook_7\",\n",
    "    seq_no=1,\n",
    "    message=recruitment_prompt,\n",
    "    generation_config=generation_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0f834",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "By implementing this fix, the pipeline now works as expected:\n",
    "1.  The `pre_processing policy` correctly extracts text from all 5 resumes.\n",
    "2.  The `generic_llama_cpp` block now correctly formats this text and includes it in the prompt sent to the LLM.\n",
    "3.  The LLM, now fully aware of the resume contents, generates a valid JSON response containing the required `tool_calls`.\n",
    "4.  The `post_processing policy` successfully parses this JSON and can proceed with the automated actions.\n",
    "\n",
    "This modular, policy-based architecture proves to be both powerful and flexible. The generic core remains unchanged, while specific behaviors can be easily defined and swapped out, making the system adaptable to new and varied tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3e2772-a066-4681-9298-f59181f61ff7",
   "metadata": {},
   "source": [
    "Checkout this Next Tutorial On runnning these policies as custom Swapable policies as part of vDAG\n",
    "- [Tutorial: Simple vDAG for workflow Automation with Swappable Policies](http://CLUSTER2NODE1:9999/notebooks/07_pre_and_post_processing_metrics_streaming_health/recruitment_automation_vdag.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ecf19-5ff4-48e9-8df7-d45e0300ecfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
