{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4f34218",
   "metadata": {},
   "source": [
    "# A Developer's Guide to Ad-hoc Inference in AIOS\n",
    "\n",
    "Welcome to this guide on the **Ad-hoc Inference Server**, a powerful feature that combines the dynamic block selection of the Router with direct inference execution. This allows a client to send a single, unified request that both finds the best block for a task and immediately runs inference on it.\n",
    "\n",
    "Below is a visual overview of the ad-hoc inference process, where a single request triggers both routing and execution.\n",
    "\n",
    "<img src=\"adhoc_inference_serving.png\" alt=\"Adhoc inference serving\" width=\"800\" height=\"800\">\n",
    "\n",
    "For more details, refer to the official documentation:\n",
    "- [Ad-hoc Inference Server Concepts](https://docs.aigr.id/adhoc-inference-server/adhoc-inference-server/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdad696",
   "metadata": {},
   "source": [
    "## The Ad-hoc Server in the AIOS Ecosystem\n",
    "\n",
    "The Ad-hoc Inference Server streamlines the process of dynamic inference. While the **Router** is excellent for discovering and selecting blocks, it only returns the *ID* of the best block. The client would then need to make a second call to that specific block to run the actual inference.\n",
    "\n",
    "The Ad-hoc Inference Server eliminates this two-step process. It acts as a higher-level service that:\n",
    "1.  Receives a single request containing both a `selection_query` (for the Router) and an inference `data` payload.\n",
    "2.  Internally calls the Router to find the best block based on the `selection_query`.\n",
    "3.  Automatically forwards the `data` payload to the selected block.\n",
    "4.  Returns the final inference result to the client.\n",
    "\n",
    "This provides a seamless experience for applications that need to dynamically select and execute models in a single, atomic operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdd91f1-6d3b-496d-acad-dd06ae4d2773",
   "metadata": {},
   "source": [
    "## 1. Deploying the Policy\n",
    "With the policy tested, we can now deploy it to AIOS. This involves three steps: Assembling the policy,uploading the policy package (tokenautoscaler.zip) and then registering it with the AIOS Policy System."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11937b84-f58a-4af0-88ff-2a68754d4199",
   "metadata": {},
   "source": [
    "### a. Assembling the Policy for Deployment\n",
    "\n",
    "Now that we have defined all the methods of our `AIOSv1PolicyRule` class, we need to assemble them into a single script and package it correctly for deployment. The policy must be contained within a `code` directory, which includes the `function.py` file and a `requirements.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710592bd-93c0-4063-8eb9-8f04ed6764ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a requirements.txt file\n",
    "with open(\"code/requirements.txt\", \"w\") as f:\n",
    "    f.write(\"requests\") # No external dependencies for this policy\n",
    "\n",
    "print(\"code/function.py and code/requirements.txt created successfully.\")\n",
    "!cat code/function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39558b7-e31f-402e-bdb5-dc7d3c00282a",
   "metadata": {},
   "source": [
    "Now, let's package our policy into a `attributebasedrouter.zip` file for deployment. The zip file must contain the `code` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0521d7-8017-41a9-8b1c-a98ae2a191d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r attributebasedrouter.zip code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca5230c-5fee-4b96-8d5f-7053937b3b2f",
   "metadata": {},
   "source": [
    "### b. Upload the Policy Package\n",
    "\n",
    "First, we upload the `.zip` file containing our `function.py` to a location accessible by AIOS. The following `curl` command sends the file to an upload server, which makes it available via a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dad633-5095-4660-96a2-2ed30b142be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST http://POLICYSTORESERVER:30186/upload -F \"file=@./attributebasedrouter.zip\" -F \"path=.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bea15e-912d-4b56-8017-903ff5cf182d",
   "metadata": {},
   "source": [
    "### c. Register the Policy\n",
    "\n",
    "Next, we register the policy with AIOS. This `curl` command sends a JSON payload to the policy registry endpoint. The payload contains metadata about our policy, including its name, version, and the URL where the code can be found (`\"code\": \"http://MANAGEMENTMASTER:32555/attributebasedrouter.zip\"`). This tells AIOS how to find and execute our policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a041b7d0-3250-4699-ad3a-b4699dee20e3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "policy_registration_payload = {\n",
    "    \"name\": \"attributebasedrouter\",\n",
    "    \"version\": \"2.0\",\n",
    "    \"release_tag\": \"stable\",\n",
    "    \"metadata\": {\"author\": \"admin\", \"category\": \"analytics\"},\n",
    "    \"tags\": \"analytics,ai\",\n",
    "    \"code\": \"http://MANAGEMENTMASTER:32555/attributebasedrouter.zip\",\n",
    "    \"code_type\": \"tar.xz\",\n",
    "    \"type\": \"policy\",\n",
    "    \"policy_input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"input\": {\"type\": \"string\"}\n",
    "        }\n",
    "    },\n",
    "    \"policy_output_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"output\": {\"type\": \"string\"}\n",
    "        }\n",
    "    },\n",
    "    \"policy_settings_schema\": {\n",
    "        \"METRICS_BASE_URL\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Base URL for fetching block metrics\",\n",
    "            \"pattern\": \"^https?://.*\"\n",
    "        },\n",
    "        \"APP_MAP\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Mapping of application types to use cases\",\n",
    "            \"additionalProperties\": {\"type\": \"string\"}\n",
    "        }\n",
    "    },\n",
    "    \"policy_parameters_schema\": {\n",
    "        \"METRICS_BASE_URL\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Override for metrics base URL\",\n",
    "            \"pattern\": \"^https?://.*\"\n",
    "        },\n",
    "        \"selection_query\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Query for selecting and optimizing block candidates\",\n",
    "            \"properties\": {\n",
    "                \"application_type\": {\"type\": \"string\"},\n",
    "                \"optimization_goals\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"name\": {\"type\": \"string\"},\n",
    "                            \"weight\": {\"type\": \"number\"}\n",
    "                        },\n",
    "                        \"required\": [\"name\"]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"policy_settings\": {\n",
    "        \"METRICS_BASE_URL\": \"http://metrics-service:30201\",\n",
    "        \"APP_MAP\": {\n",
    "            \"RAG\": \"chat-completion\",\n",
    "            \"Code\": \"code-generation\",\n",
    "            \"Vision\": \"multi-modal\",\n",
    "            \"Chat\": \"chat-completion\"\n",
    "        }\n",
    "    },\n",
    "    \"policy_parameters\": {\n",
    "        \"METRICS_BASE_URL\": \"http://metrics-service:30201\"\n",
    "    },\n",
    "    \"description\": \"A policy for dynamic block selection based on metrics and application type.\",\n",
    "    \"functionality_data\": {\"strategy\": \"ML-based\"},\n",
    "    \"resource_estimates\": {}\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa2958-0319-420a-aefc-e3be034cb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\n",
    "    \"http://MANAGEMENTMASTER:30102/policy\",\n",
    "    json=policy_registration_payload,\n",
    "    headers={\"Content-Type\": \"application/json\"}\n",
    ")\n",
    "\n",
    "print(\"Status Code:\", response.status_code)\n",
    "print(\"Response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c416dd",
   "metadata": {},
   "source": [
    "## 2. The Ad-hoc Inference Request Structure\n",
    "\n",
    "The `curl` command below sends a request to the `/v1/infer` endpoint. This request is a hybrid; it contains both an inference payload (the `data` section) and a block selection query (the `selection_query` section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484df036",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"model\": \"\",\n",
    "    \"session_id\": \"session-1257\",\n",
    "    \"seq_no\": 11,\n",
    "    \"data\": {\n",
    "        \"mode\": \"chat\",\n",
    "        \"gen_params\": {\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.95,\n",
    "            \"max_tokens\": 256\n",
    "        },\n",
    "        \"message\": \"Explain about the Deepseek LLM Model\"\n",
    "    },\n",
    "    \"graph\": {},\n",
    "    \"selection_query\": {\n",
    "        \"header\": {\n",
    "            \"templateUri\": \"Parser/V1\"\n",
    "        },\n",
    "        \"body\": {\n",
    "            \"values\": {\n",
    "                \"matchType\": \"block\",\n",
    "                \"rankingPolicyRule\": {\n",
    "                    \"values\": {\n",
    "                        \"policyRuleURI\": \"attributebasedrouter:2.0-stable\",\n",
    "                        \"parameters\": {\n",
    "                            \"filterRule\": {\n",
    "                                \"matchType\": \"block\",\n",
    "                                \"filter\": {\n",
    "                                    \"blockQuery\": {\n",
    "                                        \"logicalOperator\": \"AND\",\n",
    "                                        \"conditions\": [\n",
    "                                            {\n",
    "                                                \"variable\": \"component.componentMetadata.usecase\",\n",
    "                                                \"operator\": \"LIKE\",\n",
    "                                                \"value\": \"*chat-completion*\"\n",
    "                                            },\n",
    "                                            {\n",
    "                                                \"variable\": \"component.componentMetadata.capabilities.supportsStreaming\",\n",
    "                                                \"operator\": \"==\",\n",
    "                                                \"value\": True\n",
    "                                            },\n",
    "                                            {\n",
    "                                                \"logicalOperator\": \"OR\",\n",
    "                                                \"conditions\": [\n",
    "                                                    {\n",
    "                                                        \"variable\": \"component.componentMetadata.evaluation.benchmarks.MMLU.value\",\n",
    "                                                        \"operator\": \">=\",\n",
    "                                                        \"value\": 80\n",
    "                                                    },\n",
    "                                                    {\n",
    "                                                        \"variable\": \"component.componentMetadata.evaluation.benchmarks.ARC-Challenge.value\",\n",
    "                                                        \"operator\": \">=\",\n",
    "                                                        \"value\": 90\n",
    "                                                    }\n",
    "                                                ]\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    },\n",
    "                                    \"blockMetricsQuery\": {\n",
    "                                        \"logicalOperator\": \"OR\",\n",
    "                                        \"conditions\": [\n",
    "                                            {\n",
    "                                                \"aggOperator\": \"avg\",\n",
    "                                                \"target\": \"instances.uptime_hours\",\n",
    "                                                \"operator\": \">\",\n",
    "                                                \"value\": 10\n",
    "                                            },\n",
    "                                            {\n",
    "                                                \"aggOperator\": \"avg\",\n",
    "                                                \"target\": \"instances.llm_tokens_per_second\",\n",
    "                                                \"operator\": \">\",\n",
    "                                                \"value\": 10\n",
    "                                            }\n",
    "                                        ]\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                            \"user_query\": \"Find a reliable and high-performing chat model\",\n",
    "                            \"METRICS_BASE_URL\": \"http://MANAGEMENTMASTER:30201/block/\",\n",
    "                            \"selection_query\": {\n",
    "                                \"optimization_goals\": [\n",
    "                                    {\n",
    "                                        \"name\": \"Fast_Generator\",\n",
    "                                        \"weight\": 0.9\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"name\": \"Free_Block\",\n",
    "                                        \"weight\": 0.1\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f5de9",
   "metadata": {},
   "source": [
    "## 3. Understanding the Flow\n",
    "\n",
    "Hereâ€™s what happens when AIOS receives this request:\n",
    "\n",
    "1.  **Selection First**: The Ad-hoc Inference Server first takes the `selection_query` part and sends it to the **Router**. The Router uses the specified policy (`non-llm-matcher`) to find the best block. This involves:\n",
    "    - **Filtering**: Applying the `blockQuery` (for static metadata) and `blockMetricsQuery` (for real-time metrics) to find all suitable candidates.\n",
    "    - **Scoring & Ranking**: The `non-llm-matcher` policy then scores the candidates based on the `optimization_goals` to create a ranked list.\n",
    "    - **Selection**: The Router selects the top-ranked block.\n",
    "\n",
    "2.  **Inference Next**: Once the Router returns the ID of the selected block, the Ad-hoc Inference Server takes the `data` part of the original request and forwards it to that specific block for processing.\n",
    "\n",
    "### The Filters in This Request\n",
    "\n",
    "-   **`blockQuery` (Metadata)**: This time, we're looking for a multi-modal block that uses the `llama-cpp-python` framework and supports quantization.\n",
    "    -   `framework` is `llama-cpp-python`.\n",
    "    -   `supports_quantization` is `true`.\n",
    "    -   `tags` contains `multi-modal`.\n",
    "-   **`blockMetricsQuery` (Metrics)**: We want a block that is performing well.\n",
    "    -   Average `on_preprocess_fps` (frames per second) is greater than `8000`.\n",
    "    -   Maximum `llm_active_sessions` is at least `5`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0c1165",
   "metadata": {},
   "source": [
    "## 4. Running the Inference\n",
    "\n",
    "Executing the `requests.post` command will trigger the entire flow. The system will find the best multi-modal block and ask it to analyze the provided image, all in a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89963869",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://CLUSTER1MASTER:31504/v1/infer\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "try:\n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=30)\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Response Headers: {dict(response.headers)}\")\n",
    "    \n",
    "    if response.headers.get('content-type', '').startswith('application/json'):\n",
    "        print(f\"Response JSON: {json.dumps(response.json(), indent=2)}\")\n",
    "    else:\n",
    "        print(f\"Response Text: {response.text}\")\n",
    "        \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Request failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffad8ff",
   "metadata": {},
   "source": [
    "## 5. Observing in Logs\n",
    "\n",
    "To see the whole process, you can check the logs of two services:\n",
    "\n",
    "1.  **Router (Parser) Logs**: These logs will show the details of the selection process, including the list of candidate blocks, the results of the filters, the final ranked list, and the block that was ultimately selected. This can be see here in `Management Server` http://MANAGEMENTMASTER:32199/explore , under label conatiner with value executor-executor-001.\n",
    "2.   **Ad-hoc Inference Server Logs**: logs of the block in `K8s dashboard`https://CLUSTER1MASTER:32319/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072568c-740b-4aa9-83ee-ffc99a39f155",
   "metadata": {},
   "source": [
    "## 6. LLM Based Matchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5f7cae-942e-480b-92df-9b000ddd1e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any, Optional\n",
    "def eval(self, parameters: Dict, input_data: Dict, context: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Main evaluation function that processes input data and returns the selected block.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            merged_params = {**self.parameters, **parameters}\n",
    "            \n",
    "            candidates = input_data.get('candidates', [])\n",
    "            if not candidates:\n",
    "                return self._create_error_response(\"No candidates provided\")\n",
    "            \n",
    "            query_string = merged_params.get('query_string', '').strip()\n",
    "            if not query_string:\n",
    "                return self._create_error_response(\"Query string is required\")\n",
    "\n",
    "            llm_block_id = merged_params.get('llm_block_id')\n",
    "            llm_server_address = merged_params.get('llm_server_address')\n",
    "\n",
    "            if not llm_block_id or not llm_server_address:\n",
    "                return self._create_error_response(\"LLM block ID and server address are required\")\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following user query, select the best block from the candidates provided.\n",
    "            User Query: \"{query_string}\"\n",
    "            \n",
    "            Candidates:\n",
    "            {json.dumps(candidates, indent=2)}\n",
    "            \n",
    "            Respond with only the blockId of the best candidate.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Extract relevant fields for the prompt\n",
    "            summarized_candidates = [self._summarize_candidate(c) for c in candidates]\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "            Based on the following user query, select the best block from the candidates provided.\n",
    "            User Query: \"{query_string}\"\n",
    "            \n",
    "            Candidates:\n",
    "            {json.dumps(summarized_candidates, indent=2)}\n",
    "            \n",
    "            Respond with only the containerImage of the best candidate in a JSON object with the key \"containerImage\".\n",
    "            \"\"\"\n",
    "        \n",
    "            llm_response = self._call_llm_inference(prompt, llm_block_id, llm_server_address)\n",
    "            \n",
    "            if not llm_response or 'reply' not in llm_response:\n",
    "                return self._create_error_response(\"Invalid or empty response from LLM\")\n",
    "            \n",
    "            reply_text = llm_response['reply'].strip()\n",
    "            \n",
    "            selected_container_image = None\n",
    "            \n",
    "            # 1. Look for a JSON code block\n",
    "            json_match = re.search(r\"```json\\n(.*?)\\n```\", reply_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    json_data = json.loads(json_match.group(1))\n",
    "                    selected_container_image = json_data.get(\"containerImage\")\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Found JSON block but failed to decode: {json_match.group(1)}\")\n",
    "\n",
    "            # 2. If no JSON block found, try to parse the whole reply\n",
    "            if not selected_container_image:\n",
    "                try:\n",
    "                    json_reply = json.loads(reply_text)\n",
    "                    selected_container_image = json_reply.get(\"containerImage\")\n",
    "                except json.JSONDecodeError:\n",
    "                    pass # Not a JSON object\n",
    "\n",
    "            # 3. If still nothing, assume the last line is the container image\n",
    "            if not selected_container_image:\n",
    "                selected_container_image = reply_text.splitlines()[-1].strip()\n",
    "\n",
    "\n",
    "            if not selected_container_image:\n",
    "                 return self._create_error_response(f\"Could not extract containerImage from LLM response: {reply_text}\")\n",
    "\n",
    "            # Find the selected block in the candidates\n",
    "            selected_block = next((c for c in candidates if c.get(\"containerRegistryInfo\", {}).get(\"containerImage\") == selected_container_image), None)\n",
    "\n",
    "            if not selected_block:\n",
    "                return self._create_error_response(f\"LLM selected a containerImage not in the candidate list: {selected_container_image}\")\n",
    "\n",
    "            return {\n",
    "                \"allowed\": True,\n",
    "                \"input_data\": {\n",
    "                    \"selected_block\": selected_block,\n",
    "                    \"timestamp\": self.current_time\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during evaluation: {e}\", exc_info=True)\n",
    "            return self._create_error_response(f\"Error during evaluation: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a5112-eecf-4380-9abc-2fe4f09089b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
